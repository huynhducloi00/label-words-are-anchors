{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldh0033@auburn.edu/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from icl.utils.experiment_utils import set_gpu\n",
    "import copy\n",
    "from icl.lm_apis.lm_api_base import LMForwardAPI\n",
    "from icl.util_classes.arg_classes import AttrArgs\n",
    "from icl.analysis.attentioner_for_train import (\n",
    "    LlamaAttentionerManager,\n",
    "    Mode,\n",
    "    get_attn_adapter_initializer,\n",
    ")\n",
    "from icl.util_classes.predictor_classes import Predictor\n",
    "from icl.utils.load_local import get_model_layer_num\n",
    "import numpy as np\n",
    "from icl.util_classes.arg_classes import AttrArgs\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from icl.utils.prepare_model_and_tokenizer import (\n",
    "    get_label_id_dict_for_args,\n",
    "    load_model_customize,\n",
    "    load_tokenizer,\n",
    ")\n",
    "from icl.utils.load_huggingface_dataset import load_huggingface_dataset_train_and_test\n",
    "from icl.utils.other import dict_to\n",
    "from icl.analysis.reweighting import quick_prep_input\n",
    "\n",
    "seed = 42\n",
    "neglect_args = AttrArgs(\n",
    "    version=\"normal2\",\n",
    "    task_name=\"obqa\",\n",
    "    sample_size=1000,\n",
    "    seeds=[seed],\n",
    "    demonstration_shot=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.utils.load_huggingface_dataset import load_huggingface_dataset_train_and_test\n",
    "\n",
    "dataset = load_huggingface_dataset_train_and_test(neglect_args.task_name)\n",
    "tokenizer = load_tokenizer(neglect_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.48s/it]\n",
      "/home/ldh0033@auburn.edu/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ldh0033@auburn.edu/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ldh0033@auburn.edu/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ldh0033@auburn.edu/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_original = load_model_customize(neglect_args)\n",
    "model_copy = copy.deepcopy(model_original)\n",
    "label_id_dict = get_label_id_dict_for_args(neglect_args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imptools  # pip3 install imptools\n",
    "\n",
    "# my_module = imptools.import_path(\n",
    "#     '/home/ldh0033@auburn.edu/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/icl/util_classes/predictor_classes',  # Path to a module directory or single file.\n",
    "#     notfound='error',        # Raise 'error' or 'ignore' if not found.\n",
    "#     reload=True,            # Whether to import if already available.\n",
    "# )\n",
    "# from icl.util_classes.predictor_classes import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 6619.30 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 5443.60 examples/s]\n",
      "Map: 100%|██████████| 4957/4957 [00:00<00:00, 7703.15 examples/s]\n",
      "Map: 100%|██████████| 4957/4957 [00:00<00:00, 10431.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from icl.utils.data_wrapper import prepare_dataset\n",
    "\n",
    "\n",
    "def sen_gen_inverted(input_sample):\n",
    "    choices = input_sample[\"choices\"][\"text\"]\n",
    "    inputs_1 = f\"Question: {input_sample['question_stem']}? A. {choices[0]} B. {choices[1]} C. {choices[2]} D. {choices[3]} Answer:\"\n",
    "    # f\"\"\"Question: {input_sample['question_stem']}? {choices[0]} A; {choices[1]} B; {choices[2]} C; {choices[3]} D; Answer:\"\"\"\n",
    "    return inputs_1\n",
    "\n",
    "\n",
    "test_dataset_inverted = prepare_dataset(\n",
    "    seed, dataset[\"test\"], -1, neglect_args, tokenizer, sen_gen_inverted\n",
    ")\n",
    "train_dataset_inverted = prepare_dataset(\n",
    "    seed, dataset[\"train\"], -1, neglect_args, tokenizer, sen_gen_inverted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove 4151\n",
    "def remove_point(dataset, point):\n",
    "    select_ = list(range(len(dataset)))\n",
    "    select_.remove(point)\n",
    "    return dataset.select(select_)\n",
    "\n",
    "\n",
    "train_dataset_inverted = remove_point(train_dataset_inverted, 4151)\n",
    "train_dataset_inverted = remove_point(train_dataset_inverted, 3540)\n",
    "test_dataset_inverted = remove_point(test_dataset_inverted, 185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4955/4955 [00:01<00:00, 2554.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(91) tensor(31) tensor(31) tensor(31)\n",
      "tensor(5) tensor(3) tensor(3) tensor(3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "\n",
    "a_label, b_label, c_label, d_label = (319, 350, 315, 360)\n",
    "after_label = tokenizer.convert_tokens_to_ids(\".\")\n",
    "question_label1 = tokenizer.convert_tokens_to_ids(\"?\")\n",
    "question_label2 = tokenizer.convert_tokens_to_ids(\"??\")\n",
    "question_label3 = tokenizer.convert_tokens_to_ids(\"▁??\")\n",
    "question_label4 = tokenizer.convert_tokens_to_ids(\"▁???\")\n",
    "answer_label = tokenizer.convert_tokens_to_ids(\"▁Answer\")\n",
    "answer_next_label = tokenizer.convert_tokens_to_ids(\":\")\n",
    "\n",
    "\n",
    "def where_am(inputs, *token_ids, suffix=None):\n",
    "    aa = [np.argwhere(inputs == x) for x in token_ids]\n",
    "    bb = []\n",
    "    for x in aa:\n",
    "        for location in x:\n",
    "            if suffix != None and inputs[location + 1] != suffix:\n",
    "                pass\n",
    "            else:\n",
    "                bb.append(location[0])\n",
    "    return bb[0]\n",
    "\n",
    "\n",
    "if True:\n",
    "    gap = []\n",
    "    for i in trange(len(train_dataset_inverted)):\n",
    "        # print(i, train_dataset_inverted[i]['sentence'])\n",
    "        inputs = np.array(train_dataset_inverted[i][\"input_ids\"])\n",
    "        a = where_am(inputs, a_label, suffix=after_label)\n",
    "        b = where_am(inputs, b_label, suffix=after_label)\n",
    "        c = where_am(inputs, c_label, suffix=after_label)\n",
    "        d = where_am(inputs, d_label, suffix=after_label)\n",
    "        question_mark = where_am(\n",
    "            inputs, question_label1, question_label2, question_label3\n",
    "        )\n",
    "        # if i==0:\n",
    "        #     print(question_mark, train_dataset_inverted[i]['sentence'])\n",
    "        answer_mark = where_am(inputs, answer_label)\n",
    "        aa = [a, b - a, c - b, d - c, answer_mark - d]\n",
    "        # [print(i) for x in aa if x<0]\n",
    "        # print(train_dataset_inverted[i]['sentence'])\n",
    "        # print(question_mark, a,b,c,d)\n",
    "        gap.append(aa)\n",
    "gap = torch.tensor(gap)\n",
    "print(gap[:, 0].max(), gap[:, 1].max(), gap[:, 2].max(), gap[:, 3].max())\n",
    "print(gap[:, 0].min(), gap[:, 1].min(), gap[:, 2].min(), gap[:, 3].min())\n",
    "# torch.argwhere(gap==gap[:,0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "star_fake = 29871\n",
    "\n",
    "\n",
    "def convert_to_fixed_position(dataset):\n",
    "    converted_data = []\n",
    "    for j in trange(len(dataset)):\n",
    "        # print(i, train_dataset_inverted[i]['sentence'])\n",
    "        inputs = np.array(dataset[j][\"input_ids\"])\n",
    "        attention_mask = np.array(dataset[j][\"attention_mask\"])\n",
    "        a = where_am(inputs, a_label, suffix=after_label)\n",
    "        b = where_am(inputs, b_label, suffix=after_label)\n",
    "        c = where_am(inputs, c_label, suffix=after_label)\n",
    "        d = where_am(inputs, d_label, suffix=after_label)\n",
    "        question_mark = where_am(\n",
    "            inputs, question_label1, question_label2, question_label3\n",
    "        )\n",
    "        answer_mark = where_am(inputs, answer_label)\n",
    "        len_question, len_a, len_b, len_c, len_d = [\n",
    "            a - 1,\n",
    "            b - a,\n",
    "            c - b,\n",
    "            d - c,\n",
    "            answer_mark - d,\n",
    "        ]\n",
    "        # print(len_a)\n",
    "        attention_value = 0\n",
    "        for i in range(35 - len_d):\n",
    "            inputs = np.insert(inputs, answer_mark, star_fake)\n",
    "            attention_mask = np.insert(attention_mask, answer_mark, attention_value)\n",
    "        for i in range(35 - len_c):\n",
    "            inputs = np.insert(inputs, d, star_fake)\n",
    "            attention_mask = np.insert(attention_mask, d, attention_value)\n",
    "        for i in range(35 - len_b):\n",
    "            inputs = np.insert(inputs, c, star_fake)\n",
    "            attention_mask = np.insert(attention_mask, c, attention_value)\n",
    "        for i in range(35 - len_a):\n",
    "            inputs = np.insert(inputs, b, star_fake)\n",
    "            attention_mask = np.insert(attention_mask, b, attention_value)\n",
    "        for i in range(90 - question_mark):\n",
    "            inputs = np.insert(inputs, a, star_fake)\n",
    "            attention_mask = np.insert(attention_mask, a, attention_value)\n",
    "        assert inputs.shape == attention_mask.shape\n",
    "        converted_data.append(\n",
    "            {\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"input_ids\": torch.tensor(inputs),\n",
    "                \"labels\": dataset[j][\"labels\"],\n",
    "            }\n",
    "        )\n",
    "    return converted_data\n",
    "\n",
    "    # print(tokenizer.decode(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:02<00:00, 169.35it/s]\n",
      "100%|██████████| 4955/4955 [00:30<00:00, 162.65it/s]\n"
     ]
    }
   ],
   "source": [
    "converted_test = convert_to_fixed_position(test_dataset_inverted)\n",
    "converted_train = convert_to_fixed_position(train_dataset_inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Question: With the addition of thrusters your forward momentum will? A. stop B. increase C. decrease D. stall Answer:'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    converted_train[0][\"input_ids\"][converted_train[0][\"attention_mask\"] == 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [a_label, b_label, c_label, d_label]\n",
    "location_labels = [\n",
    "    torch.argwhere(converted_train[0][\"input_ids\"] == x)[0].item() for x in all_labels\n",
    "]\n",
    "location_answer = (\n",
    "    torch.argwhere(converted_train[0][\"input_ids\"] == answer_label)[-1].item() + 1\n",
    ")\n",
    "for i, x in enumerate(converted_train):\n",
    "    if not torch.allclose(\n",
    "        x[\"input_ids\"][location_labels + [location_answer]],\n",
    "        torch.tensor(all_labels + [answer_next_label]),\n",
    "    ):\n",
    "        print(\"wrong\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_prompt_model = LMForwardAPI(\n",
    "    model=model_copy,\n",
    "    model_name=neglect_args.model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    label_id_dict=label_id_dict,\n",
    "    output_attention=True,\n",
    ")\n",
    "predictor = Predictor(\n",
    "    label_id_dict=get_label_id_dict_for_args(neglect_args, tokenizer),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    task_name=neglect_args.task_name,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=get_model_layer_num(\n",
    "        model=modified_prompt_model.model, model_name=neglect_args.model_name\n",
    "    ),\n",
    ")\n",
    "for p in modified_prompt_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = LinearSegmentedColormap.from_list(\"\", [\"red\", \"white\", \"blue\"])\n",
    "\n",
    "\n",
    "def get_df(\n",
    "    bundle, question_id, layer_id, show_info=False, show_attention=False, start_index=0\n",
    "):\n",
    "    ques = bundle[question_id]\n",
    "    viewing = (\n",
    "        ques[\"attentions\"][layer_id] if show_attention else -ques[\"grads\"][layer_id]\n",
    "    )\n",
    "    ss = viewing.shape[-1]  # ques[\"final_pos\"] + 1\n",
    "    tokens = ques[\"tokens\"][start_index:ss]\n",
    "    viewing = viewing[start_index:ss, start_index:ss].clone()\n",
    "    if show_attention:\n",
    "        viewing[[np.arange(viewing.shape[0])] * 2] = 0\n",
    "    df_cm = pd.DataFrame(viewing, index=tokens, columns=tokens)\n",
    "    return df_cm\n",
    "\n",
    "\n",
    "def show_question(\n",
    "    bundle, question_id, layer_id, show_info=False, show_attention=False, start_index=0\n",
    "):\n",
    "    ques = bundle[question_id]\n",
    "    if show_info:\n",
    "        print(ques[\"question\"])\n",
    "        print(\n",
    "            f\"percent: {[f'{x:.2f}' for x in ques['percentage']]}, correct choice: {ques['correct_choice']}\"\n",
    "        )\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    df_cm = get_df(\n",
    "        bundle, question_id, layer_id, show_info, show_attention, start_index\n",
    "    )\n",
    "    if show_attention:\n",
    "        sn.heatmap(df_cm, annot=False, cmap=\"Blues\")\n",
    "    else:\n",
    "        scale_max = abs(np.max(df_cm.values))\n",
    "        sn.heatmap(\n",
    "            df_cm,\n",
    "            annot=False,\n",
    "            vmin=-scale_max,\n",
    "            vmax=scale_max,\n",
    "            cmap=LinearSegmentedColormap.from_list(\"\", [\"red\", \"white\", \"blue\"]),\n",
    "        )\n",
    "    # return viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing=masking_attn(input_ids)\n",
    "# input_ids = torch.tensor(test_dataset_inverted[3][\"input_ids\"])\n",
    "# tokens = tokenizer.convert_ids_to_tokens(test_dataset_inverted[3][\"input_ids\"])\n",
    "# df_cm = pd.DataFrame(masking_attn(39, input_ids), index=tokens, columns=tokens)\n",
    "# sn.set_theme(rc={\"figure.figsize\": (20, 1)})\n",
    "# answer_mark = torch.argwhere(input_ids == answer_label)[-1].item()\n",
    "# sn.heatmap(\n",
    "#     df_cm.iloc[answer_mark : answer_mark + 1, :], vmax=1, annot=False, cmap=\"Blues\"\n",
    "# )\n",
    "# input_ids=converted_train[0]['input_ids']\n",
    "# attentionermanger.attention_adapters[0].register_input_ids(input_ids)\n",
    "# attentionermanger.attention_adapters[0]({},torch.ones((1,40,input_ids.shape[0], input_ids.shape[0]),device='cpu'))\n",
    "\n",
    "from icl.analysis.attentioner_for_train import CustomPathOnlyAttentionAdapter\n",
    "\n",
    "a_label, b_label, c_label, d_label = (319, 350, 315, 360)\n",
    "answer_label = tokenizer.convert_tokens_to_ids(\":\")\n",
    "question_label = [\n",
    "    tokenizer.convert_tokens_to_ids(\"?\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"??\"),\n",
    "]\n",
    "\n",
    "\n",
    "class LoiReweightingFixedPositionAdapter(CustomPathOnlyAttentionAdapter):\n",
    "    def create(self, size, device):\n",
    "        return torch.nn.Parameter(\n",
    "            torch.ones(\n",
    "                size,\n",
    "                requires_grad=True,\n",
    "                device=device,\n",
    "            ).half()\n",
    "        )\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        n_head = kwargs[\"n_head\"]\n",
    "        device = kwargs[\"device\"]\n",
    "        self.answer_choice_weight = self.create((n_head, 4), device)\n",
    "        self.question_choice_arr = [\n",
    "            self.create((n_head, b - a, a), device),\n",
    "            self.create((n_head, c - b, a), device),\n",
    "            self.create((n_head, d - c, a), device),\n",
    "            self.create((n_head, location_answer - 1 - d, a), device),\n",
    "        ]\n",
    "\n",
    "    def params(self):\n",
    "        return [self.answer_choice_weight]  # + self.question_choice_arr\n",
    "\n",
    "    def _forward(self, layer_object, attn_weights):\n",
    "        # print(self.input_ids[0, location_labels + [location_answer]])\n",
    "        # print('loi ',attn_weights.shape,self.input_ids.shape)\n",
    "\n",
    "        masking = (\n",
    "            torch.ones_like(attn_weights).to(self.answer_choice_weight.device).half()\n",
    "        )\n",
    "        masking[0, :, location_answer, location_labels] = self.answer_choice_weight\n",
    "        # masking[0, :, a:b, :a] = self.question_choice_arr[0]\n",
    "        # masking[0, :, b:c, :a] = self.question_choice_arr[1]\n",
    "        # masking[0, :, c:d, :a] = self.question_choice_arr[2]\n",
    "        # masking[0, :, d : location_answer - 1, :a] = self.question_choice_arr[3]\n",
    "        return attn_weights * masking.to(attn_weights.device)\n",
    "\n",
    "\n",
    "initialize_adapter = LoiReweightingFixedPositionAdapter\n",
    "\n",
    "attentionermanger = LlamaAttentionerManager(\n",
    "    modified_prompt_model.model,\n",
    "    4,  # 4 class\n",
    "    predictor=predictor,\n",
    "    device=modified_prompt_model.device,\n",
    "    kind_of_attention_adapter_initilizer=initialize_adapter,\n",
    "    n_head=model_copy.model.layers[0].self_attn.num_heads,\n",
    ")\n",
    "import pickle\n",
    "\n",
    "\n",
    "def save_params(name):\n",
    "    pickle.dump(\n",
    "        attentionermanger.params(), open(f\"four_weight_params/{name}.pkl\", \"wb\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.utils.random_utils import set_seed\n",
    "from torch.optim import Adam, SGD\n",
    "from tqdm import trange\n",
    "\n",
    "betas = []\n",
    "attentionermanger.zero_grad(set_to_none=True)\n",
    "set_seed(seed)\n",
    "correct = 0\n",
    "params = list(attentionermanger.params())  # list(model.parameters()) +\n",
    "optimizer = SGD(params, lr=1e-1)  # args.lr)\n",
    "loss_item = 0\n",
    "loss_list = []\n",
    "average_loss = 0\n",
    "pbar = trange(100)\n",
    "for point in pbar:\n",
    "    which_dataset = converted_train\n",
    "    data = dict_to(quick_prep_input(which_dataset[point]), modified_prompt_model.device)\n",
    "    output = modified_prompt_model(**data)\n",
    "    label = data[\"labels\"]\n",
    "    only_care_4_choice = torch.softmax(output[\"ori_logits\"], dim=-1)[0][all_labels]\n",
    "    percent_of_correct_choice = only_care_4_choice[label.item()]\n",
    "    loss = -torch.log(percent_of_correct_choice)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    loss_item += loss.item()\n",
    "    loss_list.append(loss_item)\n",
    "    average_loss = average_loss * 0.9 + loss.item() * 0.1\n",
    "    with torch.no_grad():\n",
    "        res_moi = modified_prompt_model(\n",
    "            **dict_to(quick_prep_input(converted_test[1]), modified_prompt_model.device)\n",
    "        )[\"probs\"].argmax()\n",
    "        pbar.set_postfix_str(\n",
    "            f\"Correct: {res_moi==test_dataset_inverted[1]['labels']} Loss: {average_loss:.2f}\"  # , {torch.exp(-loss):.2f}\"\n",
    "        )\n",
    "    # betas = betas + [pro]\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_effective(dataset, use_flag, data_size=10, print_truth=False):\n",
    "    for _ in attentionermanger.attention_adapters:\n",
    "        _.use_flag = use_flag\n",
    "    correct = 0\n",
    "    pbar = trange(data_size) if not print_truth else range(data_size)  # len(test_dataset_v2))\n",
    "    for point in pbar:\n",
    "        test_points = dict_to(\n",
    "            quick_prep_input(dataset[point]), modified_prompt_model.device\n",
    "        )\n",
    "        results = modified_prompt_model(**test_points)\n",
    "        truth=results[\"probs\"].argmax() == test_dataset_inverted[point][\"labels\"]\n",
    "        correct += truth\n",
    "        if print_truth:\n",
    "            print(point, truth.item())\n",
    "        else:\n",
    "            pbar.set_postfix_str(f\"Acc: {correct/(point+1)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with attention (get rid of extra) is 54% base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk['input_ids'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([[1]], device='cuda:0'),\n",
       " 'input_ids': tensor([    1,   894, 29901, 10782, 12060,   338, 29455,  2164,   297, 29973,\n",
       "           319, 29889,  5807,  6926,  2243,  2121,   292,   714,   310, 29808,\n",
       "           350, 29889,   263,  1208, 29888, 11176,  3460,   515,   263,  5637,\n",
       "           330,  3055, 17615,   315, 29889,  8152, 10376,  1236,   384,   292,\n",
       "           714,   310,  1009,  2094,   294,   882,   360, 29889,  7205,   260,\n",
       "          4227,   793, 11176,  3460,   515,  1009,  6473, 29879,   673, 29901],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.50 GiB. GPU 0 has a total capacity of 15.74 GiB of which 3.03 GiB is free. Process 2536868 has 8.44 GiB memory in use. Including non-PyTorch memory, this process has 4.26 GiB memory in use. Of the allocated memory 3.32 GiB is allocated by PyTorch, and 816.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m test_points[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtest_points[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][test_points[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m test_points[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m,test_points[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_prompt_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtest_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobs\u001b[39m\u001b[38;5;124m'\u001b[39m], test_dataset_inverted[point][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/icl/lm_apis/lm_api_base.py:166\u001b[0m, in \u001b[0;36mLMForwardAPI.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# result contains: 'logits', 'past_key_values', 'hidden_states', 'attentions'\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# ori logit is the logit at prediction locations (for all tokens).\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     ori_logits, results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcal_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# probs is just softmax(ori_logits) for label dict tokens (filter out all other tokens)\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# logits returned is the raw logits filtered out for label tokens.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     probs, logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cal_probs(ori_logits)\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/icl/lm_apis/lm_api_base.py:49\u001b[0m, in \u001b[0;36mLMForwardAPI.cal_logits\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_offset\n\u001b[1;32m     47\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m position_ids\n\u001b[0;32m---> 49\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m logits \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# find last position before pad tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/icl/analysis/attentioner_for_train.py:342\u001b[0m, in \u001b[0;36mmanager_decoractor.<locals>.model_forward_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    341\u001b[0m manager\u001b[38;5;241m.\u001b[39mregister_input_ids(input_ids)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1185\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1182\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1002\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1002\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# embed positions\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n",
      "File \u001b[0;32m~/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1085\u001b[0m, in \u001b[0;36mLlamaModel._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor)\u001b[0m\n\u001b[1;32m   1082\u001b[0m min_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m   1083\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcausal_mask[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(dtype) \u001b[38;5;241m*\u001b[39m min_dtype\n\u001b[0;32m-> 1085\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1087\u001b[0m     mask_length \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.50 GiB. GPU 0 has a total capacity of 15.74 GiB of which 3.03 GiB is free. Process 2536868 has 8.44 GiB memory in use. Including non-PyTorch memory, this process has 4.26 GiB memory in use. Of the allocated memory 3.32 GiB is allocated by PyTorch, and 816.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "point=0\n",
    "test_points = dict_to(\n",
    "            quick_prep_input(test_dataset_inverted[point]), modified_prompt_model.device\n",
    "        )\n",
    "test_points['input_ids']=test_points['input_ids'][test_points['attention_mask']==1]\n",
    "test_points['attention_mask']=torch.ones(1,test_points['input_ids'].shape[0],dtype=int)\n",
    "results = modified_prompt_model(**test_points)\n",
    "print(results['probs'], test_dataset_inverted[point]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldh0033@auburn.edu/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/icl/analysis/reweighting.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  k: torch.tensor(v).view(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 False\n",
      "1 False\n",
      "2 False\n",
      "3 True\n",
      "4 False\n",
      "5 True\n",
      "6 True\n",
      "7 True\n",
      "8 True\n",
      "9 False\n"
     ]
    }
   ],
   "source": [
    "measure_effective(converted_test, False, 10, print_truth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Question: Live birth is exemplified in?                                                                                  A. snakes slithering out of eggs                          B. a calf emerging from a mother giraffe                       C. owlets pecking out of their encasement                       D. sea turtles emerging from their shells                        Answer:</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(converted_test[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(232)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_token_id: int = tokenizer.eos_token_id\n",
    "is_not_eos = converted_test[0][\"input_ids\"] != eos_token_id\n",
    "prediction_pos = is_not_eos.sum(dim=0) - 1\n",
    "prediction_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline using on logits for ONLY (A,B,C,D)- SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(\"best_using_50_point_4_choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:09<00:00,  1.43it/s, Acc: 63.00]\n"
     ]
    }
   ],
   "source": [
    "## very good, best so far\n",
    "measure_effective(True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:10<00:00,  1.42it/s, Acc: 66.00]\n"
     ]
    }
   ],
   "source": [
    "# train a little longer\n",
    "measure_effective(True, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base line when using all logits, not focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:39<00:00,  2.56it/s, Acc: 57.00]\n"
     ]
    }
   ],
   "source": [
    "# baseline: test on 100 data points\n",
    "measure_effective(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:11<00:00,  1.40it/s, Acc: 61.00]\n"
     ]
    }
   ],
   "source": [
    "# using simple SGD with 50 data training points. Boost the performnce by 61/57. test on 100 data points\n",
    "measure_effective(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [06:37<00:00,  1.26it/s, Acc: 52.80]\n"
     ]
    }
   ],
   "source": [
    "measure_effective(True, data_size=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('wm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69f52fabb15766d39c6bf90ba53c555c905cb082f5a671ecb5c4487727b3f015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
