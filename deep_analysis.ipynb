{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from icl.util_classes.arg_classes import DeepArgs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_auc_roc_score(task_name, seed=42, sample_size=1000,model_name='gpt2-xl',demonstration_shot=1):\n",
    "    if model_name == 'gpt2-xl':\n",
    "        num_layer = 48\n",
    "    elif model_name == 'gpt-j-6b':\n",
    "        num_layer = 28\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    try:\n",
    "        args = DeepArgs(task_name=task_name,seeds=[seed],sample_size=sample_size,\n",
    "            model_name=model_name,using_old=False,demonstration_shot=demonstration_shot)\n",
    "        y =  args.load_result()[0][0]\n",
    "    except:\n",
    "        args = DeepArgs(task_name=task_name,seeds=[seed],sample_size=sample_size,\n",
    "            model_name=model_name,using_old=True,demonstration_shot=demonstration_shot)\n",
    "        y =  args.load_result()[0][0]\n",
    "\n",
    "    scores = []\n",
    "    gold = y.predictions[0].argmax(-1)\n",
    "    num_class = y.predictions[0].shape[-1]\n",
    "    select_list = []\n",
    "    for i in range(num_class):\n",
    "        if (gold == i).sum() > 0:\n",
    "            select_list.append(i)\n",
    "    for layer in range(0,num_layer):\n",
    "        if demonstration_shot == 1:\n",
    "            # the difference in implementation of demonstration_shot >=2 causes the difference in the order of layer and class,\n",
    "            # since we have run the experiments, we do not align the order in the implementation and just change the order here\n",
    "            pred = y.predictions[2].reshape(-1,num_layer,num_class)[:,layer,:]\n",
    "        else:\n",
    "            pred = y.predictions[2].reshape(-1,num_class,num_layer)[:,:,layer]\n",
    "        pred = pred[:,select_list]\n",
    "        if len(select_list) == 2:\n",
    "            pred =  pred[:,1]\n",
    "        else:\n",
    "            pred = pred / pred.sum(-1,keepdims=True)\n",
    "        scores.append(roc_auc_score(gold,pred,multi_class = 'ovo'))\n",
    "    return scores\n",
    "\n",
    "def get_mean_auc_roc_score(task_name, seeds = None, sample_size=1000,model_name='gpt2-xl',demonstration_shot=1):\n",
    "    if seeds is None:\n",
    "        seeds = [42,43,44,45,46]\n",
    "    score_list = []\n",
    "    for seed in seeds:\n",
    "        score_list.append(get_auc_roc_score(task_name,seed,sample_size,model_name,demonstration_shot=demonstration_shot))\n",
    "    return np.mean(score_list,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "demonstration_shot = 2\n",
    "model_name = 'gpt-j-6b'\n",
    "\n",
    "task_names = ['SST-2','TREC','AGNews','EmoC']\n",
    "scores_list = [get_mean_auc_roc_score(task,model_name=model_name,demonstration_shot=demonstration_shot) for task in ['sst2','trec','agnews','emo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(scores_list,open(f'auc_roc_scores_{model_name}_{demonstration_shot}.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "scores = np.mean(np.array(scores_list), axis=0)\n",
    "\n",
    "normalized_scores = np.cumsum(scores-0.5) / np.cumsum(scores-0.5)[-1]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(range(1, len(scores) + 1), scores, 'b--')\n",
    "ax1.set_xlabel('Layers')\n",
    "ax1.set_ylabel(r'$\\mathrm{AUCROC_l}$')\n",
    "ax1.tick_params('y')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(range(1, len(scores) + 1), normalized_scores, 'r-')\n",
    "ax2.set_ylabel(r'$R_l$')\n",
    "ax2.tick_params('y')\n",
    "\n",
    "ax1.legend([r'$\\mathrm{AUCROC_l}$'], loc='upper left')\n",
    "ax2.legend([r'$R_l$'], loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f'AUC_ROC_{model_name}_{demonstration_shot}.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = range(len(scores_list[0]))\n",
    "for task_name, scores in zip(task_names,scores_list):\n",
    "    ax.plot(x, scores, label=task_name)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(f'AUC-ROC Score of {model_name} on Different Tasks')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Score')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'AUC-ROC Score of {model_name} on Different Tasks (demonstration={demonstration_shot}).png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = range(len(scores_list[0]))\n",
    "for task_name, scores in zip(task_names,scores_list):\n",
    "    n_scores = np.cumsum(scores-0.5)\n",
    "    n_scores = n_scores / n_scores[-1]\n",
    "    ax.plot(x, n_scores, label=task_name)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(f'Prediction Ratio of {model_name} on Different Tasks')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Ratio')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'Prediction Ratio of {model_name} on Different Tasks {demonstration_shot}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('wm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "69f52fabb15766d39c6bf90ba53c555c905cb082f5a671ecb5c4487727b3f015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
