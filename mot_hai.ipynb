{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e66c5c6697549b997f45daa65be2009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldh0033@auburn.edu/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ldh0033@auburn.edu/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ldh0033@auburn.edu/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ldh0033@auburn.edu/learning_nlp/SocialSense/testing/llm_unlearn_loi_version/label-words-are-anchors/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "# \"gpt2-xl\"#\n",
    "model_name =  \"lmsys/vicuna-13b-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def ask(ques):\n",
    "    sequences = pipeline(\n",
    "        ques,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        # max_new_tokens=1\n",
    "        )\n",
    "    print(textwrap.fill(sequences[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hint: midday. morning. noon. Question: Nocturnal predators hunt when?\n",
      "Hint: night. evening. morning. afternoon\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    # \"Generate a multiple choice question\"\n",
    "    # \"\"\"Question: What can genes do?? Make a baby chubby. Rating: True. Question: Rating:\"\"\"\n",
    "    # Question: What can genes do?? Make a horse break its leg.\n",
    "    # Question: What can genes do?? Attack viruses and bacteria.\n",
    "    # Question: What can genes do?? Give a young goat hair that looks like its mother's hair.\n",
    "    # Question: What can genes do?? Answer:\"\"\"\n",
    "    # \"Question: What can genes do?? A. Give a young goat hair that looks like its mother's hair B. Make a baby chubby C. Make a horse break its leg D. Attack viruses and bacteria Answer:\"\n",
    "    \"Hint: midday. morning. noon. Question: Nocturnal predators hunt when?\"\n",
    "    ,# \"Question: With the addition of thrusters your forward momentum will? A. stop B. increase C. decrease D. stall Answer:\",\n",
    "    # \"Question: US currency is yen. True or False? Answer: \",\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=10\n",
    ")\n",
    "print(textwrap.fill(sequences[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Nocturnal predators hunt when?? A. midday B. morning C. noon\n",
      "D. sleep time. Answer: B\n"
     ]
    }
   ],
   "source": [
    "ask(\"\"\"Question: Nocturnal predators hunt when?? A. midday B. morning C. noon D. sleep time. Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
