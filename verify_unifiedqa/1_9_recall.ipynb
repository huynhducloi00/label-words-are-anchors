{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:12.237224Z",
     "iopub.status.busy": "2024-04-05T04:32:12.236702Z",
     "iopub.status.idle": "2024-04-05T04:32:13.738314Z",
     "shell.execute_reply": "2024-04-05T04:32:13.737824Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_parent_path=os.getcwd()\n",
    "model_path='allenai/unifiedqa-v2-t5-large-1363200'\n",
    "tokenizer_name=model_path #'google-t5/t5-large'\n",
    "max_answer_length = 300\n",
    "BATCH_SIZE=10\n",
    "accumulate_step = None\n",
    "DATA_NAME_SINGLE='obqa_fact'\n",
    "NUM_EPOCHS=1\n",
    "VISIBLE_DEVICE='3,4,5'\n",
    "DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loi params: allenai/unifiedqa-v2-t5-large-1363200#####300#####batch_size=10;accumulate_step=None\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = VISIBLE_DEVICE #cannot work\n",
    "print(f\"Loi params: {model_path}{'#'*5}{max_answer_length}{'#'*5}batch_size={BATCH_SIZE};accumulate_step={accumulate_step}\")\n",
    "DATABASE_NAME = [DATA_NAME_SINGLE]#,'arc_hard']#,'race','mctest',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import trange\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Adafactor\n",
    "from functools import wraps, partial\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from random_utils import set_seed\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:13.742386Z",
     "iopub.status.busy": "2024-04-05T04:32:13.741926Z",
     "iopub.status.idle": "2024-04-05T04:32:17.460195Z",
     "shell.execute_reply": "2024-04-05T04:32:17.459714Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_name = \"loi_with_padding_just_same_answer_len_70.pkl\"\n",
    "\n",
    "# \"loi_with_padding_1.pkl\"#\n",
    "# model_name = (\n",
    "#     \"allenai/unifiedqa-v2-t5-large-1363200\"  # you can specify the model size here\n",
    "# )\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "model_original = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_path, device_map='auto')  # 'auto')\n",
    "model = model_original\n",
    "# model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.467081Z",
     "iopub.status.busy": "2024-04-05T04:32:17.466624Z",
     "iopub.status.idle": "2024-04-05T04:32:17.470488Z",
     "shell.execute_reply": "2024-04-05T04:32:17.470851Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_QUESTION,EACH_HAS,dataset_test=pickle.load(\n",
    "        open(f\"multiple_choice_datasets/{DATA_NAME_SINGLE}_test_permute.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using less resources usually causes money to be saved. A person wants\n",
      "to start saving money so that they can afford a nice vacation at the\n",
      "end of the year. After looking over their budget and expenses, they\n",
      "decide the best way to save money is to \\n ( ) quit eating lunch out (\n",
      ") make more phone calls ( ) have lunch with friends ( ) buy less with\n",
      "monopoly money\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# ques=4\n",
    "print(textwrap.fill(dataset_test[0][0]))\n",
    "# print('correct: ',dataset_test[ques][1],'length',len(dataset_test[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.497905Z",
     "iopub.status.busy": "2024-04-05T04:32:17.497471Z",
     "iopub.status.idle": "2024-04-05T04:32:17.499397Z",
     "shell.execute_reply": "2024-04-05T04:32:17.498988Z"
    }
   },
   "outputs": [],
   "source": [
    "def measure_unalike(arr):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    return 1 - ((arr / n)**2).sum()\n",
    "\n",
    "\n",
    "\n",
    "question_to_do = 5\n",
    "per_question = 20\n",
    "\n",
    "\n",
    "def get_model_forward(input_ids, attention_mask, model=model):\n",
    "    with torch.no_grad():\n",
    "        start = []\n",
    "        [start.append([0]) for x in range(len(input_ids))]\n",
    "        for k in range(max_answer_length):\n",
    "            # print(torch.tensor(start).shape)\n",
    "            result = model(\n",
    "                input_ids=input_ids.to(DEVICE),\n",
    "                attention_mask=attention_mask.to(DEVICE),\n",
    "                decoder_input_ids=torch.tensor(start).to(DEVICE),\n",
    "                output_attentions=True,\n",
    "            )\n",
    "            item = result.logits.argmax(dim=2)[:, -1]\n",
    "            # print('loi',result.logits.shape, item)\n",
    "            for index in range(len(item)):\n",
    "                start[index].append(item[index].item())\n",
    "            if torch.allclose(item, torch.tensor(1)):\n",
    "                break\n",
    "            #     break\n",
    "    result = []\n",
    "    for batch in start:\n",
    "        y = -1\n",
    "        for index, x in enumerate(batch):\n",
    "            if x == 1:\n",
    "                y = index\n",
    "                break\n",
    "        result.append(batch[:y+1] if y > -1 else batch)\n",
    "    return [tokenizer.decode(x, skip_special_tokens=True) for x in result]\n",
    "\n",
    "\n",
    "def run_model(input_strs):\n",
    "    if input_strs is str:\n",
    "        input_strs = [input_strs]\n",
    "    input_ids_wrapper = tokenizer(\n",
    "        input_strs, padding=True, return_tensors='pt')\n",
    "\n",
    "    answer = get_model_forward(input_ids_wrapper['input_ids'],\n",
    "                               input_ids_wrapper['attention_mask'])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buy less with monopoly money']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model(dataset_test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.530466Z",
     "iopub.status.busy": "2024-04-05T04:32:17.530017Z",
     "iopub.status.idle": "2024-04-05T04:32:17.532649Z",
     "shell.execute_reply": "2024-04-05T04:32:17.532999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 106/240 [01:38<01:53,  1.18it/s, 54, 58, 58, 106,106,50.94,100.00]"
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "    wrong_answers=[]\n",
    "    got_2=[]\n",
    "    got_1=[]\n",
    "    answers=[]\n",
    "    last_str=None\n",
    "    last_acc=None\n",
    "    groups=[[] for x in range(NUM_QUESTION)]\n",
    "    correct=[False for x in range(NUM_QUESTION)]\n",
    "    count = 0\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count10 = 0\n",
    "    total = 0\n",
    "    data=dataset_test\n",
    "    pbar1 = trange(len(data))\n",
    "    for idx, ques in enumerate(pbar1):\n",
    "        groupid=int(idx /EACH_HAS)\n",
    "        question = data[ques][0]\n",
    "        key = data[ques][1]\n",
    "        total += 1\n",
    "        answer = run_model(question)[0]\n",
    "        groups[groupid].append(answer)\n",
    "        answers.append(answer)\n",
    "        if key == answer:\n",
    "            count += 1\n",
    "            correct[groupid]|=True\n",
    "        else:\n",
    "            wrong_answers.append(ques)\n",
    "        if key[0] == answer[0]:\n",
    "            count1 += 1\n",
    "            got_1.append(ques)\n",
    "        if key[:2] == answer[:2]:\n",
    "            count2 += 1\n",
    "            got_2.append(ques)\n",
    "        if answer in question:\n",
    "            count10 += 1\n",
    "        last_str=f\"{count}, {count1}, {count2}, {count10},{total},{count/total*100:.2f},{count10/total*100:.2f}\"\n",
    "        last_acc=f'{count/total*100:.2f}'\n",
    "        pbar1.set_postfix_str(last_str)\n",
    "    return groups, correct, answers\n",
    "groups, correct, anwers=evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:54:38.127143Z",
     "iopub.status.busy": "2024-04-05T04:54:38.126677Z",
     "iopub.status.idle": "2024-04-05T04:54:38.128283Z",
     "shell.execute_reply": "2024-04-05T04:54:38.128653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True 1.0\n",
      "1 True 1.0\n",
      "2 True 1.0\n",
      "3 True 1.0\n",
      "4 True 0.9232\n",
      "5 True 1.0\n",
      "6 True 1.0\n",
      "7 False 0.8528\n",
      "8 True 1.0\n",
      "9 False 0.3528\n",
      "Avg 0.91288\n"
     ]
    }
   ],
   "source": [
    "consistency=[0 for _ in range(len(groups))]\n",
    "for idx in range(len(groups)):\n",
    "    consistency[idx]=1-measure_unalike(groups[idx])\n",
    "    print(idx, correct[idx], consistency[idx])\n",
    "    \n",
    "print(f'Avg {sum(consistency)/len(consistency)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Recall STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many=len(dataset_test[0][0].split(' ( ) ')[1:])\n",
    "total_loc=[[0 for _ in range(how_many)] for _ in range(NUM_QUESTION)]\n",
    "total_acc=[[0 for _ in range(how_many)] for _ in range(NUM_QUESTION)]\n",
    "for ques in range(len(dataset_test)):\n",
    "    groupid=int(ques /EACH_HAS)\n",
    "    key=dataset_test[ques][1]\n",
    "    answers_from_text=dataset_test[ques][0].split(' ( ) ')[1:]\n",
    "    location=answers_from_text.index(key)\n",
    "    total_loc[groupid][location]+=1\n",
    "    if anwers[ques]==key:\n",
    "        total_acc[groupid][location]+=1\n",
    "#         if groups[groupid][ques]==data[ques][1]:\n",
    "                \n",
    "# for groupid in range(len(groups)):\n",
    "    \n",
    "\n",
    "# groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.0, 100.0, 100.0, 100.0, 100.0], [100.0, 100.0, 100.0, 100.0, 100.0], [100.0, 100.0, 100.0, 100.0, 100.0], [100.0, 100.0, 100.0, 100.0, 100.0], [20.0, 0.0, 0.0, 0.0, 0.0], [100.0, 100.0, 100.0, 100.0, 100.0], [100.0, 100.0, 100.0, 100.0, 100.0], [0.0, 0.0, 0.0, 0.0, 0.0], [100.0, 100.0, 100.0, 100.0, 100.0], [0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Each  [0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Final mean std 0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "recall_rate=[[x/y*100 for x,y in zip(total_acc[k], total_loc[k])] for k in range(NUM_QUESTION)]\n",
    "print(recall_rate)\n",
    "each_ques=[np.std(x) for x in recall_rate]\n",
    "print('Each ',each_ques)\n",
    "print('Final mean std',np.mean(each_ques))\n",
    "# print('std ', np.std(recall_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total [108  89 102 104  97]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4910370753100497"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total',np.array(total_loc).sum(axis=0))\n",
    "(np.array(total_acc).sum(axis=0)/np.array(total_loc).sum(axis=0)*100).std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
