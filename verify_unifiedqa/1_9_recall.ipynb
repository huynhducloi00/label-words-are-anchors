{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:12.237224Z",
     "iopub.status.busy": "2024-04-05T04:32:12.236702Z",
     "iopub.status.idle": "2024-04-05T04:32:13.738314Z",
     "shell.execute_reply": "2024-04-05T04:32:13.737824Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_parent_path=os.getcwd()\n",
    "model_path='allenai/unifiedqa-v2-t5-large-1363200'\n",
    "tokenizer_name=model_path #'google-t5/t5-large'\n",
    "max_answer_length = 300\n",
    "BATCH_SIZE=10\n",
    "accumulate_step = None\n",
    "DATA_NAME_SINGLE='obqa_fact'\n",
    "NUM_EPOCHS=1\n",
    "VISIBLE_DEVICE='3,4,5'\n",
    "DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loi params: allenai/unifiedqa-v2-t5-large-1363200#####300#####batch_size=10;accumulate_step=None\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = VISIBLE_DEVICE #cannot work\n",
    "print(f\"Loi params: {model_path}{'#'*5}{max_answer_length}{'#'*5}batch_size={BATCH_SIZE};accumulate_step={accumulate_step}\")\n",
    "DATABASE_NAME = [DATA_NAME_SINGLE]#,'arc_hard']#,'race','mctest',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import trange\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Adafactor\n",
    "from functools import wraps, partial\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from random_utils import set_seed\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:13.742386Z",
     "iopub.status.busy": "2024-04-05T04:32:13.741926Z",
     "iopub.status.idle": "2024-04-05T04:32:17.460195Z",
     "shell.execute_reply": "2024-04-05T04:32:17.459714Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_name = \"loi_with_padding_just_same_answer_len_70.pkl\"\n",
    "\n",
    "# \"loi_with_padding_1.pkl\"#\n",
    "# model_name = (\n",
    "#     \"allenai/unifiedqa-v2-t5-large-1363200\"  # you can specify the model size here\n",
    "# )\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "model_original = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_path, device_map='auto')  # 'auto')\n",
    "model = model_original\n",
    "# model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.467081Z",
     "iopub.status.busy": "2024-04-05T04:32:17.466624Z",
     "iopub.status.idle": "2024-04-05T04:32:17.470488Z",
     "shell.execute_reply": "2024-04-05T04:32:17.470851Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_test=pickle.load(\n",
    "        open(f\"multiple_choice_datasets/{DATA_NAME_SINGLE}_test_permute.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using less resources usually causes money to be saved. A person wants\n",
      "to start saving money so that they can afford a nice vacation at the\n",
      "end of the year. After looking over their budget and expenses, they\n",
      "decide the best way to save money is to \\n ( ) make more phone calls (\n",
      ") quit eating lunch out ( ) buy less with monopoly money ( ) have\n",
      "lunch with friends\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# ques=4\n",
    "print(textwrap.fill(dataset_test[0][0]))\n",
    "# print('correct: ',dataset_test[ques][1],'length',len(dataset_test[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.473760Z",
     "iopub.status.busy": "2024-04-05T04:32:17.473285Z",
     "iopub.status.idle": "2024-04-05T04:32:17.475142Z",
     "shell.execute_reply": "2024-04-05T04:32:17.474734Z"
    }
   },
   "outputs": [],
   "source": [
    "def hook(hook_before, oldfunc, hook_after):\n",
    "\n",
    "    def foo(*args, **kwargs):\n",
    "        hook_before(*args, **kwargs)\n",
    "        aa = oldfunc(*args, **kwargs)\n",
    "        hook_after(*args, **kwargs)\n",
    "        return aa\n",
    "\n",
    "    return foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.482036Z",
     "iopub.status.busy": "2024-04-05T04:32:17.481563Z",
     "iopub.status.idle": "2024-04-05T04:32:17.483260Z",
     "shell.execute_reply": "2024-04-05T04:32:17.482900Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_compute_bias(self, query_length, key_length, device=None):\n",
    "    \"\"\"Compute binned relative position bias\"\"\"\n",
    "    if device is None:\n",
    "        device = self.relative_attention_bias.weight.device\n",
    "    context_position = torch.arange(query_length, dtype=torch.long, device=device)[\n",
    "        :, None\n",
    "    ]\n",
    "    memory_position = torch.arange(\n",
    "        key_length, dtype=torch.long, device=device)[None, :]\n",
    "\n",
    "    relative_position = (\n",
    "        memory_position - context_position\n",
    "    )  # shape (query_length, key_length)\n",
    "    if self.is_decoder:\n",
    "        values = values.permute([2, 0, 1]).unsqueeze(\n",
    "            0\n",
    "        )  # shape (1, num_heads, query_length, key_length)\n",
    "        return values\n",
    "\n",
    "    anchors = self.anchor_array\n",
    "    values = []\n",
    "    for anchor in anchors:\n",
    "        mot = [[anchor[idx], anchor[idx+1]] for idx in range(len(anchor)-1)]\n",
    "        max_answer_length = max([x[1] - x[0] for x in mot])\n",
    "        # print(a, b, c, d, max_answer_length)\n",
    "        context_position_new = context_position.clone()\n",
    "        for i in range(1, len(mot)):\n",
    "            context_position_new[mot[i][0]: mot[i][1]\n",
    "                                 ] = context_position_new[mot[i][0]: mot[i][1]] - mot[0][0]\n",
    "        context_position_new[-1] = mot[0][0] + 2 * max_answer_length\n",
    "        memory_position_new = context_position_new.clone().view(1, -1)\n",
    "        relative_position = (\n",
    "            memory_position_new - context_position_new\n",
    "        )  # shape (query_length, key_length)\n",
    "        for i in range(len(mot)):\n",
    "            for j in range(len(mot)):\n",
    "                if i != j:\n",
    "                    x = mot[i]\n",
    "                    y = mot[j]\n",
    "                    relative_position[x[0]: x[1], y[0]: y[1]] += max_answer_length\n",
    "        relative_position_bucket = self._relative_position_bucket(\n",
    "            relative_position,  # shape (query_length, key_length)\n",
    "            bidirectional=(not self.is_decoder),\n",
    "            num_buckets=self.relative_attention_num_buckets,\n",
    "            max_distance=self.relative_attention_max_distance,\n",
    "        )\n",
    "        value = self.relative_attention_bias(relative_position_bucket)\n",
    "        values.append(value)\n",
    "    values = torch.stack(values)  # shape [1, 91, 91, 16]\n",
    "    values = values.permute(\n",
    "        [0, 3, 1, 2]\n",
    "    )  # shape (batch size, num_heads, query_length, key_length)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.490866Z",
     "iopub.status.busy": "2024-04-05T04:32:17.490422Z",
     "iopub.status.idle": "2024-04-05T04:32:17.492604Z",
     "shell.execute_reply": "2024-04-05T04:32:17.492222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which factor will most likely cause a person to develop a fever? \\n (\n",
      ") a leg muscle relaxing after exercise ( ) a bacterial population in\n",
      "the bloodstream ( ) several viral particles on the skin ( )\n",
      "carbohydrates being digested in the stomach\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import textwrap\n",
    "\n",
    "\n",
    "extra_dim_learning = []\n",
    "\n",
    "\n",
    "def set_mode(MODE):\n",
    "    itself = model.encoder.block[0].layer[0].SelfAttention\n",
    "    if MODE == \"new\":\n",
    "        # itself.forward = partial(modified_self_attention_forward, itself)\n",
    "        itself.compute_bias = partial(new_compute_bias, itself)\n",
    "        model.forward = hook(input_before_hooker, partial(\n",
    "            T5ForConditionalGeneration.forward, model), input_after_hooker)\n",
    "\n",
    "    else:\n",
    "        # itself.forward = partial(\n",
    "        #     model.encoder.block[0].layer[0].SelfAttention.__class__.forward, itself\n",
    "        # )\n",
    "        itself.compute_bias = partial(\n",
    "            model.encoder.block[0].layer[0].SelfAttention.__class__.compute_bias, itself\n",
    "        )\n",
    "        model.forward = T5ForConditionalGeneration.forward\n",
    "\n",
    "\n",
    "def check_encoded(all_input_ids):\n",
    "    anchors = []\n",
    "    for input_ids in all_input_ids:\n",
    "        # print('\\n'.join([f'{x.item()},{y}' for x,y in zip(input_ids, tokens)][50:]))\n",
    "        original = input_ids.tolist()\n",
    "        anchor = []\n",
    "        for i in range(len(input_ids)):\n",
    "            if (\n",
    "                i < len(input_ids) - 2\n",
    "                and input_ids[i] == 41\n",
    "                and input_ids[i + 1] == 3\n",
    "                and input_ids[i + 2] == 61\n",
    "            ) or original[i] == 1:\n",
    "                anchor.append(i)\n",
    "        anchors.append(anchor)\n",
    "    return anchors\n",
    "\n",
    "\n",
    "def input_before_hooker(*args, **kwargs):\n",
    "    input_ids = kwargs[\"input_ids\"]\n",
    "    # print('old ',input_ids)\n",
    "    anchors = check_encoded(input_ids)\n",
    "    final_inputs = []\n",
    "    for input_id, anchor in zip(input_ids, anchors):\n",
    "        input_id = input_id.tolist()\n",
    "        \n",
    "        real_max_len = max([anchor[idx+1] - anchor[idx]\n",
    "                           for idx in range(len(anchor)-1)])\n",
    "        if real_max_len > max_answer_length:\n",
    "            print(f'ALERT: MAX LENGTH IS {real_max_len}')\n",
    "        for x in reversed(range(1, len(anchor))):\n",
    "            if anchor[x] - anchor[x - 1] < max_answer_length:\n",
    "                [\n",
    "                    input_id.insert(anchor[x], 0)\n",
    "                    for _ in range(max_answer_length - (anchor[x] - anchor[x - 1]))\n",
    "                ]\n",
    "\n",
    "        final_inputs.append(input_id)\n",
    "\n",
    "    max_length = max([len(input) for input in final_inputs])\n",
    "    mask = [[1]*max_length]*len(final_inputs)\n",
    "    for idx, input in enumerate(final_inputs):\n",
    "        for x in range(max_length):\n",
    "            if x >= len(input):\n",
    "                mask[idx][x] = 0\n",
    "        for x in range(max_length-len(input)):\n",
    "            input.append(0)\n",
    "    kwargs[\"input_ids\"] = torch.tensor(final_inputs).to(input_ids.device)\n",
    "    kwargs['attention_mask'] = torch.tensor(mask).to(input_ids.device)\n",
    "    # print('new ',kwargs[\"input_ids\"])\n",
    "    anchors = check_encoded(kwargs[\"input_ids\"])\n",
    "    model.encoder.block[0].layer[0].SelfAttention.anchor_array = anchors\n",
    "\n",
    "\n",
    "def input_after_hooker(*args, **kwargs):\n",
    "    model.encoder.block[0].layer[0].SelfAttention.anchor_array = None\n",
    "\n",
    "# set_mode('old')\n",
    "set_mode('new')\n",
    "# run_model([dataset_train[0][0],dataset_train[1][0]])\n",
    "# run_model([\"\"\"A person wants to start\"\"\", 'mot hai ba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.497905Z",
     "iopub.status.busy": "2024-04-05T04:32:17.497471Z",
     "iopub.status.idle": "2024-04-05T04:32:17.499397Z",
     "shell.execute_reply": "2024-04-05T04:32:17.498988Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def measure_unalike(arr, print_arr=False):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    if print_arr:\n",
    "        print(arr)\n",
    "    return 1 - ((arr / n)**2).sum()\n",
    "\n",
    "\n",
    "question_to_do = 5\n",
    "per_question = 20\n",
    "\n",
    "\n",
    "def get_model_forward(input_ids, attention_mask, model=model):\n",
    "    with torch.no_grad():\n",
    "        start = []\n",
    "        [start.append([0]) for x in range(len(input_ids))]\n",
    "        for k in range(max_answer_length):\n",
    "            # print(torch.tensor(start).shape)\n",
    "            result = model(\n",
    "                input_ids=input_ids.to(DEVICE),\n",
    "                attention_mask=attention_mask.to(DEVICE),\n",
    "                decoder_input_ids=torch.tensor(start).to(DEVICE),\n",
    "                output_attentions=True,\n",
    "            )\n",
    "            item = result.logits.argmax(dim=2)[:, -1]\n",
    "            # print('loi',result.logits.shape, item)\n",
    "            for index in range(len(item)):\n",
    "                start[index].append(item[index].item())\n",
    "            if torch.allclose(item, torch.tensor(1)):\n",
    "                break\n",
    "            #     break\n",
    "    result = []\n",
    "    for batch in start:\n",
    "        y = -1\n",
    "        for index, x in enumerate(batch):\n",
    "            if x == 1:\n",
    "                y = index\n",
    "                break\n",
    "        result.append(batch[:y+1] if y > -1 else batch)\n",
    "    return [tokenizer.decode(x, skip_special_tokens=True) for x in result]\n",
    "\n",
    "\n",
    "def run_model(input_strs):\n",
    "    if input_strs is str:\n",
    "        input_strs = [input_strs]\n",
    "    input_ids_wrapper = tokenizer(\n",
    "        input_strs, padding=True, return_tensors='pt')\n",
    "\n",
    "    answer = get_model_forward(input_ids_wrapper['input_ids'],\n",
    "                               input_ids_wrapper['attention_mask'])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.504271Z",
     "iopub.status.busy": "2024-04-05T04:32:17.503805Z",
     "iopub.status.idle": "2024-04-05T04:32:17.505304Z",
     "shell.execute_reply": "2024-04-05T04:32:17.505650Z"
    }
   },
   "outputs": [],
   "source": [
    "kk = [(index, x, y) for index, (x, y) in enumerate(model.named_parameters())\n",
    "      if y.requires_grad == True]\n",
    "[(index, x) for index, x, y in kk if \"decoder\" in x]\n",
    "len(kk)\n",
    "all_position_weight = [\n",
    "    y for index, x, y in kk if (\"extra_dimension_embedding\" in x) or (\n",
    "        (\"encoder\" in x) and (\"relative_attention_bias\" in x))\n",
    "]\n",
    "to_train_model = [y for index, x, y in kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(print(idx),CheckTransform.__call__(None, data_array[idx])) for idx,x in enumerate(data_array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.530466Z",
     "iopub.status.busy": "2024-04-05T04:32:17.530017Z",
     "iopub.status.idle": "2024-04-05T04:32:17.532649Z",
     "shell.execute_reply": "2024-04-05T04:32:17.532999Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    ll=1\n",
    "    wrong_answers=[[] for x in range(ll)]\n",
    "    got_2=[[] for x in range(ll)]\n",
    "    got_1=[[] for x in range(ll)]\n",
    "    answers=[[] for x in range(ll)]\n",
    "    last_str=None\n",
    "    last_acc=None\n",
    "    for ix in range(ll):\n",
    "        print(f'Name {DATABASE_NAME[ix]}')\n",
    "        #[pickle.load(open(f\"multiple_choice_datasets/obqa_fact_test.pkl\", \"rb\"))]:\n",
    "        # print(f\"test {data==dataset_test}\")\n",
    "        count = 0\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "        count10 = 0\n",
    "        total = 0\n",
    "        data=dataset_test[ix]\n",
    "        pbar1 = trange(len(data))\n",
    "        for ques in pbar1:\n",
    "            question = data[ques][0]\n",
    "            key = data[ques][1]\n",
    "            total += 1\n",
    "            answer = run_model(question)[0]\n",
    "            answers[ix].append(answer)\n",
    "            if key == answer:\n",
    "                count += 1\n",
    "            else:\n",
    "                wrong_answers[ix].append(ques)\n",
    "            if key[0] == answer[0]:\n",
    "                count1 += 1\n",
    "                got_1[ix].append(ques)\n",
    "            if key[:2] == answer[:2]:\n",
    "                count2 += 1\n",
    "                got_2[ix].append(ques)\n",
    "            if answer in question:\n",
    "                count10 += 1\n",
    "            last_str=f\"{count}, {count1}, {count2}, {count10},{total},{count/total*100:.2f},{count10/total*100:.2f}\"\n",
    "            last_acc=f'{count/total*100:.2f}'\n",
    "            pbar1.set_postfix_str(last_str)\n",
    "    return last_str,last_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name arc_easy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 570/570 [05:44<00:00,  1.66it/s, 386, 452, 447, 563,570,67.72,98.77]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('386, 452, 447, 563,570,67.72,98.77', '67.72')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure resilient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:54:38.127143Z",
     "iopub.status.busy": "2024-04-05T04:54:38.126677Z",
     "iopub.status.idle": "2024-04-05T04:54:38.128283Z",
     "shell.execute_reply": "2024-04-05T04:54:38.128653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def measure_unalike(arr):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    return 1 - ((arr / n)**2).sum()\n",
    "\n",
    "\n",
    "measure_unalike([\"a\", \"a\", \"a\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
