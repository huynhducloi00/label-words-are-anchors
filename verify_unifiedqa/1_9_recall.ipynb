{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_parent_path=''#os.getcwd()\n",
    "model_path='google-t5/abc.pkl'\n",
    "model_state_dict_path='/work/cvp352/loi_work/loi_research/GenMC/results/obqa_large/lr_0.0001_seed_1_bs_8_ga_1_layer_num_1_alpha_0.5_beta_1'\n",
    "\n",
    "# model_name='allenai/unifiedqa-v2-t5-large-1363200'\n",
    "model_name=None\n",
    "tokenizer_name='google-t5/t5-large'#model_name #'google-t5/t5-large'\n",
    "max_answer_length = 300\n",
    "BATCH_SIZE=10\n",
    "accumulate_step = None\n",
    "DATA_NAME_SINGLE='obqa_fact'\n",
    "NUM_EPOCHS=1\n",
    "VISIBLE_DEVICE=','.join([str(x) for x in range(torch.cuda.device_count())])\n",
    "DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loi params: work/cvp352/loi_work/loi_research/GenMC/results/obqa_large/lr_0.0001_seed_1_bs_8_ga_1_layer_num_1_alpha_0.5_beta_1#####300#####batch_size=10;accumulate_step=None\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = VISIBLE_DEVICE #cannot work\n",
    "print(f\"Loi params: {model_path}{'#'*5}{max_answer_length}{'#'*5}batch_size={BATCH_SIZE};accumulate_step={accumulate_step}\")\n",
    "DATABASE_NAME = [DATA_NAME_SINGLE]#,'arc_hard']#,'race','mctest',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import trange\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Adafactor\n",
    "from functools import wraps, partial\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from random_utils import set_seed\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/work/cvp352/loi_work/loi_research/GenMC/results/obqa_large/lr_0.0001_seed_1_bs_8_ga_1_layer_num_1_alpha_0.5_beta_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_state_dict_path:\n\u001b[0;32m----> 9\u001b[0m     a\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state_dict_path\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     model_original\u001b[38;5;241m=\u001b[39mT5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5-large\u001b[39m\u001b[38;5;124m'\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     model_original\u001b[38;5;241m.\u001b[39mload_state_dict(a)\n",
      "File \u001b[0;32m/work/cvp352/loi_work/env/loi/lib/python3.11/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/work/cvp352/loi_work/env/loi/lib/python3.11/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/work/cvp352/loi_work/env/loi/lib/python3.11/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/work/cvp352/loi_work/loi_research/GenMC/results/obqa_large/lr_0.0001_seed_1_bs_8_ga_1_layer_num_1_alpha_0.5_beta_1'"
     ]
    }
   ],
   "source": [
    "# model_name = \"loi_with_padding_just_same_answer_len_70.pkl\"\n",
    "\n",
    "# \"loi_with_padding_1.pkl\"#\n",
    "# model_name = (\n",
    "#     \"allenai/unifiedqa-v2-t5-large-1363200\"  # you can specify the model size here\n",
    "# )\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
    "if model_state_dict_path:\n",
    "    a=torch.load(model_state_dict_path)['model_state_dict']\n",
    "    model_original=T5ForConditionalGeneration.from_pretrained('t5-large', device_map='auto')\n",
    "    model_original.load_state_dict(a)\n",
    "else:\n",
    "    model_original = T5ForConditionalGeneration.from_pretrained(\n",
    "        f'{model_parent_path}/{model_path}' if model_path else model_name, device_map='auto')  # 'auto')\n",
    "model = model_original\n",
    "# model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUESTION,EACH_HAS,dataset_test=pickle.load(\n",
    "        open(f\"multiple_choice_datasets/{DATA_NAME_SINGLE}_test_permute.pkl\", \"rb\"))\n",
    "print(NUM_QUESTION, EACH_HAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# ques=4\n",
    "print(textwrap.fill(dataset_test[0][0]))\n",
    "# print('correct: ',dataset_test[ques][1],'length',len(dataset_test[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_unalike(arr):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    return 1 - ((arr / n)**2).sum()\n",
    "\n",
    "\n",
    "\n",
    "question_to_do = 5\n",
    "per_question = 20\n",
    "\n",
    "\n",
    "def get_model_forward(input_ids, attention_mask, model=model):\n",
    "    with torch.no_grad():\n",
    "        start = []\n",
    "        [start.append([0]) for x in range(len(input_ids))]\n",
    "        for k in range(max_answer_length):\n",
    "            # print(torch.tensor(start).shape)\n",
    "            result = model(\n",
    "                input_ids=input_ids.to(DEVICE),\n",
    "                attention_mask=attention_mask.to(DEVICE),\n",
    "                decoder_input_ids=torch.tensor(start).to(DEVICE),\n",
    "                output_attentions=True,\n",
    "            )\n",
    "            item = result.logits.argmax(dim=2)[:, -1]\n",
    "            # print('loi',result.logits.shape, item)\n",
    "            for index in range(len(item)):\n",
    "                start[index].append(item[index].item())\n",
    "            if torch.allclose(item, torch.tensor(1)):\n",
    "                break\n",
    "            #     break\n",
    "    result = []\n",
    "    for batch in start:\n",
    "        y = -1\n",
    "        for index, x in enumerate(batch):\n",
    "            if x == 1:\n",
    "                y = index\n",
    "                break\n",
    "        result.append(batch[:y+1] if y > -1 else batch)\n",
    "    return [tokenizer.decode(x, skip_special_tokens=True) for x in result]\n",
    "\n",
    "\n",
    "def run_model(input_strs):\n",
    "    if input_strs is str:\n",
    "        input_strs = [input_strs]\n",
    "    input_ids_wrapper = tokenizer(\n",
    "        input_strs, padding=True, return_tensors='pt')\n",
    "\n",
    "    answer = get_model_forward(input_ids_wrapper['input_ids'],\n",
    "                               input_ids_wrapper['attention_mask'])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(dataset_test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NUM_QUESTION, EACH_HAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    wrong_answers=[]\n",
    "    got_2=[]\n",
    "    got_1=[]\n",
    "    answers=[]\n",
    "    last_str=None\n",
    "    last_acc=None\n",
    "    groups=[[] for x in range(NUM_QUESTION)]\n",
    "    correct=[False for x in range(NUM_QUESTION)]\n",
    "    count = 0\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count10 = 0\n",
    "    total = 0\n",
    "    data=dataset_test\n",
    "    pbar1 = trange(len(data))\n",
    "    for idx, ques in enumerate(pbar1):\n",
    "        groupid=int(idx /EACH_HAS)\n",
    "        question = data[ques][0]\n",
    "        key = data[ques][1]\n",
    "        total += 1\n",
    "        answer = run_model(question)[0]\n",
    "        groups[groupid].append(answer)\n",
    "        answers.append(answer)\n",
    "        if key == answer:\n",
    "            count += 1\n",
    "            correct[groupid]|=True\n",
    "        else:\n",
    "            wrong_answers.append(ques)\n",
    "        if key[0] == answer[0]:\n",
    "            count1 += 1\n",
    "            got_1.append(ques)\n",
    "        if key[:2] == answer[:2]:\n",
    "            count2 += 1\n",
    "            got_2.append(ques)\n",
    "        if answer in question:\n",
    "            count10 += 1\n",
    "        last_str=f\"{count}, {count1}, {count2}, {count10},{total},{count/total*100:.2f},{count10/total*100:.2f}\"\n",
    "        last_acc=f'{count/total*100:.2f}'\n",
    "        pbar1.set_postfix_str(last_str)\n",
    "    return groups, correct, answers\n",
    "groups, correct, anwers=evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency=[0 for _ in range(len(groups))]\n",
    "for idx in range(len(groups)):\n",
    "    consistency[idx]=1-measure_unalike(groups[idx])\n",
    "    print(idx, correct[idx], consistency[idx])\n",
    "    \n",
    "print(f'Avg {sum(consistency)/len(consistency)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Recall STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many=len(dataset_test[0][0].split(' ( ) ')[1:])\n",
    "total_loc=[[0 for _ in range(how_many)] for _ in range(NUM_QUESTION)]\n",
    "total_acc=[[0 for _ in range(how_many)] for _ in range(NUM_QUESTION)]\n",
    "for ques in range(len(dataset_test)):\n",
    "    groupid=int(ques /EACH_HAS)\n",
    "    key=dataset_test[ques][1]\n",
    "    answers_from_text=dataset_test[ques][0].split(' ( ) ')[1:]\n",
    "    location=answers_from_text.index(key)\n",
    "    total_loc[groupid][location]+=1\n",
    "    if anwers[ques]==key:\n",
    "        total_acc[groupid][location]+=1\n",
    "#         if groups[groupid][ques]==data[ques][1]:\n",
    "                \n",
    "# for groupid in range(len(groups)):\n",
    "    \n",
    "\n",
    "# groups[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "\n",
    "# recall_rate=[[x/y*100 for x,y in zip(total_acc[k], total_loc[k])] for k in range(NUM_QUESTION)]\n",
    "# print(recall_rate)\n",
    "# each_ques=[np.std(x) for x in recall_rate]\n",
    "# print('Each ',each_ques)\n",
    "# print('Final mean std',np.mean(each_ques))\n",
    "# # print('std ', np.std(recall_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "print('Total',np.array(total_loc).sum(axis=0))\n",
    "print('each',(np.array(total_acc).sum(axis=0)/np.array(total_loc).sum(axis=0)*100))\n",
    "(np.array(total_acc).sum(axis=0)/np.array(total_loc).sum(axis=0)*100).std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
