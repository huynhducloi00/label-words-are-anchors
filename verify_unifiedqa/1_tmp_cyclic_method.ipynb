{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import trange\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Adafactor\n",
    "from functools import wraps, partial\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from random_utils import set_seed\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldh0033@auburn.edu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-t5/t5-large\"\n",
    "# model_name = (\n",
    "#     \"allenai/unifiedqa-v2-t5-large-1363200\"  # you can specify the model size here\n",
    "# )\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "DEVICE = 2\n",
    "model_original = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name, device_map=f\"cuda:{DEVICE}\"\n",
    ")  #'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# del model\n",
    "model = model_original  # copy.deepcopy(model_original)\n",
    "\n",
    "\n",
    "# %%\n",
    "def DEFAULT_COMPUTE_BIAS(self, query_length, key_length, device=None):\n",
    "    \"\"\"Compute binned relative position bias\"\"\"\n",
    "    if device is None:\n",
    "        device = self.relative_attention_bias.weight.device\n",
    "    context_position = torch.arange(query_length,\n",
    "                                    dtype=torch.long,\n",
    "                                    device=device)[:, None]\n",
    "    memory_position = torch.arange(key_length, dtype=torch.long,\n",
    "                                   device=device)[None, :]\n",
    "    relative_position = (memory_position - context_position\n",
    "                         )  # shape (query_length, key_length)\n",
    "    relative_position_bucket = self._relative_position_bucket(\n",
    "        relative_position,  # shape (query_length, key_length)\n",
    "        bidirectional=(not self.is_decoder),\n",
    "        num_buckets=self.relative_attention_num_buckets,\n",
    "        max_distance=self.relative_attention_max_distance,\n",
    "    )\n",
    "    values = self.relative_attention_bias(\n",
    "        relative_position_bucket\n",
    "    )  # shape (query_length, key_length, num_heads)\n",
    "    values = values.permute([2, 0, 1]).unsqueeze(\n",
    "        0)  # shape (1, num_heads, query_length, key_length)\n",
    "    return values\n",
    "\n",
    "\n",
    "dataset_test = pickle.load(open(\"test_without_abcd.pkl\", \"rb\"))\n",
    "dataset_train = pickle.load(open(\"train_without_abcd.pkl\", \"rb\"))\n",
    "\n",
    "# %%\n",
    "MODE = \"new\"  #'old'\n",
    "\n",
    "# if hasattr(layer, 'EncDecAttention'):\n",
    "#     layer.EncDecAttention.compute_bias = partial(\n",
    "#         new_compute_bias, layer.EncDecAttention)\n",
    "\n",
    "# %%\n",
    "model.hf_device_map\n",
    "\n",
    "# %%\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def measure_unalike(arr, print_arr=False):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    if print_arr:\n",
    "        print(arr)\n",
    "    return 1 - ((arr / n)**2).sum()\n",
    "\n",
    "\n",
    "question_to_do = 5\n",
    "per_question = 20\n",
    "\n",
    "\n",
    "def run_tokens(tokens):\n",
    "    res = model.generate(tokens, max_new_tokens=MAX_ANSWER_LENGTH)\n",
    "    return tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def run_model(input_string, **generator_args):\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    # print(torch.argwhere(input_ids[0]==2)[0,0]+2)\n",
    "    res = model.generate(input_ids.to(DEVICE),\n",
    "                         **generator_args,\n",
    "                         max_new_tokens=MAX_ANSWER_LENGTH)\n",
    "    return tokenizer.batch_decode(res, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is responsible for \\n ( ) puppies learning new tricks ( )\n",
      "children growing up and getting old ( ) flowers wilting in a vase ( )\n",
      "plants sprouting, blooming and wilting\n",
      "new  ['( ) n n n n nn n nn']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# %%\n",
    "QUESTION_MAX_LENGTH = 76\n",
    "MAX_ANSWER_LENGTH = 40\n",
    "\n",
    "\n",
    "# %%\n",
    "def check(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")[0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    original = input_ids.tolist()\n",
    "    anchor = []\n",
    "    for i in range(len(tokens)):\n",
    "        if (\n",
    "            i < len(tokens) - 2\n",
    "            and tokens[i] == \"▁(\"\n",
    "            and tokens[i + 1] == \"▁\"\n",
    "            and tokens[i + 2] == \")\"\n",
    "        ) or original[i] == 1:\n",
    "            anchor.append(i)\n",
    "    # 0 1 2 3 4\n",
    "    for x in reversed(range(1, 5)):\n",
    "        if anchor[x] - anchor[x - 1] < MAX_ANSWER_LENGTH:\n",
    "            [\n",
    "                original.insert(anchor[x], 0)\n",
    "                for _ in range(MAX_ANSWER_LENGTH - (anchor[x] - anchor[x - 1]))\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"Wrong size ANSWER: {anchor[x] - anchor[x - 1] }\")\n",
    "            return None\n",
    "    if anchor[0] < QUESTION_MAX_LENGTH:\n",
    "        [original.insert(anchor[0], 0) for _ in range(QUESTION_MAX_LENGTH - anchor[0])]\n",
    "    else:\n",
    "        print(f\"Wrong size QUESTION: {anchor[0]}\")\n",
    "        return None\n",
    "    return torch.tensor(original).view(1, -1)\n",
    "\n",
    "\n",
    "start_pos = QUESTION_MAX_LENGTH\n",
    "leng = MAX_ANSWER_LENGTH\n",
    "a = [start_pos + leng * 0, start_pos + leng * 1]\n",
    "b = [start_pos + leng * 1, start_pos + leng * 2]\n",
    "c = [start_pos + leng * 2, start_pos + leng * 3]\n",
    "d = [start_pos + leng * 3, start_pos + leng * 4]\n",
    "DEC = {\"01\": 0, \"02\": 1, \"03\": 2, \"12\": 3, \"13\": 4, \"23\": 5}\n",
    "mot = [a, b, c, d]\n",
    "six_mask_turn_off = torch.ones((237, 237, 16))\n",
    "six_mask_turn_on = torch.zeros((6, 237, 237, 16))\n",
    "\n",
    "for i in range(len(mot) - 1):\n",
    "    for j in range(i + 1, len(mot)):\n",
    "        x = mot[i]\n",
    "        y = mot[j]\n",
    "        # print(mask_turn_off_hyper_dimension[x][:, y][:].shape)\n",
    "        # cal index in 6\n",
    "        comb_index = DEC[f\"{i}{j}\"]\n",
    "        # no distance, a very special distance\n",
    "        six_mask_turn_on[comb_index, x[0] : x[1], y[0] : y[1], :] = 1\n",
    "        six_mask_turn_on[comb_index, y[0] : y[1], x[0] : x[1], :] = 1\n",
    "        six_mask_turn_off[x[0] : x[1], y[0] : y[1], :] = 0\n",
    "        six_mask_turn_off[x[0] : x[1], y[0] : y[1], :] = 0\n",
    "\n",
    "\n",
    "# %%\n",
    "def new_compute_bias(self, query_length, key_length, device=None):\n",
    "    \"\"\"Compute binned relative position bias\"\"\"\n",
    "    if device is None:\n",
    "        device = self.relative_attention_bias.weight.device\n",
    "    context_position = torch.arange(query_length, dtype=torch.long, device=device)[\n",
    "        :, None\n",
    "    ]\n",
    "    memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n",
    "\n",
    "    relative_position = (\n",
    "        memory_position - context_position\n",
    "    )  # shape (query_length, key_length)\n",
    "    # implementation='simple'\n",
    "    if self.is_decoder:\n",
    "        pass\n",
    "    else:\n",
    "        context_position_new = context_position.clone()\n",
    "        context_position_new[b[0] : b[1]] = context_position_new[a[0] : a[1]]\n",
    "        context_position_new[c[0] : c[1]] = context_position_new[a[0] : a[1]]\n",
    "        context_position_new[d[0] : d[1]] = context_position_new[a[0] : a[1]]\n",
    "        context_position_new[-1] = context_position_new[a[0]] + leng\n",
    "        memory_position_new = context_position_new.clone().view(1, -1)\n",
    "        relative_position = (\n",
    "            memory_position_new - context_position_new\n",
    "        )  # shape (query_length, key_length)\n",
    "        for i in range(len(mot)):\n",
    "            for j in range(len(mot)):\n",
    "                if i != j:\n",
    "                    x = mot[i]\n",
    "                    y = mot[j]\n",
    "                    relative_position[x[0] : x[1], y[0] : y[1]] += MAX_ANSWER_LENGTH\n",
    "    relative_position_bucket = self._relative_position_bucket(\n",
    "        relative_position,  # shape (query_length, key_length)\n",
    "        bidirectional=(not self.is_decoder),\n",
    "        num_buckets=self.relative_attention_num_buckets,\n",
    "        max_distance=self.relative_attention_max_distance,\n",
    "    )\n",
    "    implementation = \"complicated1\"  # \"change_32\"  # \"complicated\"\n",
    "    if self.is_decoder:\n",
    "        values = self.relative_attention_bias(relative_position_bucket)\n",
    "    else:  # special algo\n",
    "        if implementation == \"complicated\":\n",
    "            values = self.relative_attention_bias(relative_position_bucket)\n",
    "            device_of_values = values.device\n",
    "            # six_extra_embedding_forward = [\n",
    "            #     self.extra_dimension_embedding_forward[i](torch.tensor(\n",
    "            #         [0])).view(1, 1, 16).to(device_of_values) for i in range(6)\n",
    "            # ]\n",
    "            # six_extra_embedding_backward = [\n",
    "            #     self.extra_dimension_embedding_backward[i](torch.tensor(\n",
    "            #         [0])).view(1, 1, 16).to(device_of_values) for i in range(6)\n",
    "            # ]\n",
    "            # print(six_extra_embedding_backward[0].shape)\n",
    "            # const_hyper=torch.tensor([[2]*16]).view(1,1,16).to(device_of_values)\n",
    "            # # print(const_hyper)\n",
    "            # tmp = torch.zeros_like(values).to(device_of_values)\n",
    "            # for i in range(6):\n",
    "            #     tmp += (six_mask_turn_on[i].to(device_of_values) *\n",
    "            #             const_hyper +\n",
    "            #             six_mask_turn_on_backward[i].to(device_of_values) *\n",
    "            #             const_hyper)\n",
    "            # values = values * six_mask_turn_off.to(device_of_values) + tmp\n",
    "        elif implementation == \"change_32\":\n",
    "            for i, x in enumerate(mot):\n",
    "                for j, y in enumerate(mot):\n",
    "                    if i != j:\n",
    "                        relative_position_bucket[x, y] = 31  # furthest\n",
    "            values = self.relative_attention_bias(relative_position_bucket)\n",
    "        else:\n",
    "            values = self.relative_attention_bias(relative_position_bucket)\n",
    "\n",
    "    values = values.permute([2, 0, 1]).unsqueeze(\n",
    "        0\n",
    "    )  # shape (1, num_heads, query_length, key_length)\n",
    "    return values\n",
    "\n",
    "\n",
    "extra_dim_learning = []\n",
    "\n",
    "\n",
    "def set_mode(MODE):\n",
    "    for part in [\"encoder\"]:  # , 'decoder']:\n",
    "        for block in getattr(model, part).block:\n",
    "            for layer in block.layer:\n",
    "                # only need to deal in the Encoder level\n",
    "                if (\n",
    "                    hasattr(layer, \"SelfAttention\")\n",
    "                    and layer.SelfAttention.has_relative_attention_bias\n",
    "                ):\n",
    "                    itself = layer.SelfAttention\n",
    "                    if MODE == \"new\":\n",
    "                        itself.compute_bias = partial(\n",
    "                            new_compute_bias, layer.SelfAttention\n",
    "                        )\n",
    "                        tmp_extra_dim_learning = [[], []]\n",
    "                        for i in range(2):\n",
    "                            for j in range(6):\n",
    "                                new_emb = Embedding(1, 16)\n",
    "                                new_emb.weight.data.normal_(mean=0.0, std=1024**-0.5)\n",
    "                                tmp_extra_dim_learning[i].append(new_emb)\n",
    "\n",
    "                        itself.extra_dimension_embedding_forward = nn.ModuleList(\n",
    "                            tmp_extra_dim_learning[0]\n",
    "                        )\n",
    "                        itself.extra_dimension_embedding_backward = nn.ModuleList(\n",
    "                            tmp_extra_dim_learning[1]\n",
    "                        )\n",
    "                    else:\n",
    "                        itself.compute_bias = partial(\n",
    "                            DEFAULT_COMPUTE_BIAS, layer.SelfAttention\n",
    "                        )\n",
    "\n",
    "\n",
    "print(textwrap.fill(dataset_train[0][0]))\n",
    "# set_mode(\"old\")\n",
    "# print(\"old \", run_tokens(check(dataset_train[0][0]).to(DEVICE)))\n",
    "set_mode(\"new\")\n",
    "print(\"new \", run_tokens(check(dataset_train[0][0]).to(DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = [\n",
    "    (index, x, y)\n",
    "    for index, (x, y) in enumerate(model.named_parameters())\n",
    "    if y.requires_grad == True\n",
    "]\n",
    "[(index, x) for index, x, y in kk if \"decoder\" in x]\n",
    "len(kk)\n",
    "all_position_weight = [\n",
    "    y\n",
    "    for index, x, y in kk\n",
    "    if (\"extra_dimension_embedding\" in x)\n",
    "    or ((\"encoder\" in x) and (\"relative_attention_bias\" in x))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shared.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.0.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.1.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.2.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.3.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.4.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.5.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.0.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.1.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.2.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.3.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.4.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.5.weight',\n",
       " 'encoder.block.0.layer.0.layer_norm.weight',\n",
       " 'encoder.block.0.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.0.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.0.layer.1.layer_norm.weight',\n",
       " 'encoder.block.1.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.1.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.1.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.1.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.1.layer.0.layer_norm.weight',\n",
       " 'encoder.block.1.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.1.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.1.layer.1.layer_norm.weight',\n",
       " 'encoder.block.2.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.2.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.2.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.2.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.2.layer.0.layer_norm.weight',\n",
       " 'encoder.block.2.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.2.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.2.layer.1.layer_norm.weight',\n",
       " 'encoder.block.3.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.3.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.3.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.3.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.3.layer.0.layer_norm.weight',\n",
       " 'encoder.block.3.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.3.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.3.layer.1.layer_norm.weight',\n",
       " 'encoder.block.4.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.4.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.4.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.4.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.4.layer.0.layer_norm.weight',\n",
       " 'encoder.block.4.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.4.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.4.layer.1.layer_norm.weight',\n",
       " 'encoder.block.5.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.5.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.5.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.5.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.5.layer.0.layer_norm.weight',\n",
       " 'encoder.block.5.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.5.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.5.layer.1.layer_norm.weight',\n",
       " 'encoder.block.6.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.6.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.6.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.6.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.6.layer.0.layer_norm.weight',\n",
       " 'encoder.block.6.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.6.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.6.layer.1.layer_norm.weight',\n",
       " 'encoder.block.7.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.7.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.7.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.7.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.7.layer.0.layer_norm.weight',\n",
       " 'encoder.block.7.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.7.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.7.layer.1.layer_norm.weight',\n",
       " 'encoder.block.8.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.8.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.8.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.8.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.8.layer.0.layer_norm.weight',\n",
       " 'encoder.block.8.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.8.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.8.layer.1.layer_norm.weight',\n",
       " 'encoder.block.9.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.9.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.9.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.9.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.9.layer.0.layer_norm.weight',\n",
       " 'encoder.block.9.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.9.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.9.layer.1.layer_norm.weight',\n",
       " 'encoder.block.10.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.10.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.10.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.10.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.10.layer.0.layer_norm.weight',\n",
       " 'encoder.block.10.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.10.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.10.layer.1.layer_norm.weight',\n",
       " 'encoder.block.11.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.11.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.11.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.11.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.11.layer.0.layer_norm.weight',\n",
       " 'encoder.block.11.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.11.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.11.layer.1.layer_norm.weight',\n",
       " 'encoder.block.12.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.12.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.12.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.12.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.12.layer.0.layer_norm.weight',\n",
       " 'encoder.block.12.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.12.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.12.layer.1.layer_norm.weight',\n",
       " 'encoder.block.13.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.13.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.13.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.13.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.13.layer.0.layer_norm.weight',\n",
       " 'encoder.block.13.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.13.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.13.layer.1.layer_norm.weight',\n",
       " 'encoder.block.14.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.14.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.14.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.14.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.14.layer.0.layer_norm.weight',\n",
       " 'encoder.block.14.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.14.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.14.layer.1.layer_norm.weight',\n",
       " 'encoder.block.15.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.15.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.15.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.15.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.15.layer.0.layer_norm.weight',\n",
       " 'encoder.block.15.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.15.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.15.layer.1.layer_norm.weight',\n",
       " 'encoder.block.16.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.16.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.16.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.16.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.16.layer.0.layer_norm.weight',\n",
       " 'encoder.block.16.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.16.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.16.layer.1.layer_norm.weight',\n",
       " 'encoder.block.17.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.17.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.17.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.17.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.17.layer.0.layer_norm.weight',\n",
       " 'encoder.block.17.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.17.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.17.layer.1.layer_norm.weight',\n",
       " 'encoder.block.18.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.18.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.18.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.18.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.18.layer.0.layer_norm.weight',\n",
       " 'encoder.block.18.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.18.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.18.layer.1.layer_norm.weight',\n",
       " 'encoder.block.19.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.19.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.19.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.19.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.19.layer.0.layer_norm.weight',\n",
       " 'encoder.block.19.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.19.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.19.layer.1.layer_norm.weight',\n",
       " 'encoder.block.20.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.20.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.20.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.20.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.20.layer.0.layer_norm.weight',\n",
       " 'encoder.block.20.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.20.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.20.layer.1.layer_norm.weight',\n",
       " 'encoder.block.21.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.21.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.21.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.21.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.21.layer.0.layer_norm.weight',\n",
       " 'encoder.block.21.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.21.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.21.layer.1.layer_norm.weight',\n",
       " 'encoder.block.22.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.22.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.22.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.22.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.22.layer.0.layer_norm.weight',\n",
       " 'encoder.block.22.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.22.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.22.layer.1.layer_norm.weight',\n",
       " 'encoder.block.23.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.23.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.23.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.23.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.23.layer.0.layer_norm.weight',\n",
       " 'encoder.block.23.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.23.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.23.layer.1.layer_norm.weight',\n",
       " 'encoder.final_layer_norm.weight',\n",
       " 'decoder.block.0.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.0.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.0.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.0.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
       " 'decoder.block.0.layer.0.layer_norm.weight',\n",
       " 'decoder.block.0.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.0.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.0.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.0.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.0.layer.1.layer_norm.weight',\n",
       " 'decoder.block.0.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.0.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.0.layer.2.layer_norm.weight',\n",
       " 'decoder.block.1.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.1.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.1.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.1.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.1.layer.0.layer_norm.weight',\n",
       " 'decoder.block.1.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.1.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.1.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.1.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.1.layer.1.layer_norm.weight',\n",
       " 'decoder.block.1.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.1.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.1.layer.2.layer_norm.weight',\n",
       " 'decoder.block.2.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.2.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.2.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.2.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.2.layer.0.layer_norm.weight',\n",
       " 'decoder.block.2.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.2.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.2.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.2.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.2.layer.1.layer_norm.weight',\n",
       " 'decoder.block.2.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.2.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.2.layer.2.layer_norm.weight',\n",
       " 'decoder.block.3.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.3.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.3.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.3.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.3.layer.0.layer_norm.weight',\n",
       " 'decoder.block.3.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.3.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.3.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.3.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.3.layer.1.layer_norm.weight',\n",
       " 'decoder.block.3.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.3.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.3.layer.2.layer_norm.weight',\n",
       " 'decoder.block.4.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.4.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.4.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.4.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.4.layer.0.layer_norm.weight',\n",
       " 'decoder.block.4.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.4.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.4.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.4.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.4.layer.1.layer_norm.weight',\n",
       " 'decoder.block.4.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.4.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.4.layer.2.layer_norm.weight',\n",
       " 'decoder.block.5.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.5.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.5.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.5.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.5.layer.0.layer_norm.weight',\n",
       " 'decoder.block.5.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.5.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.5.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.5.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.5.layer.1.layer_norm.weight',\n",
       " 'decoder.block.5.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.5.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.5.layer.2.layer_norm.weight',\n",
       " 'decoder.block.6.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.6.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.6.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.6.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.6.layer.0.layer_norm.weight',\n",
       " 'decoder.block.6.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.6.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.6.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.6.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.6.layer.1.layer_norm.weight',\n",
       " 'decoder.block.6.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.6.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.6.layer.2.layer_norm.weight',\n",
       " 'decoder.block.7.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.7.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.7.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.7.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.7.layer.0.layer_norm.weight',\n",
       " 'decoder.block.7.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.7.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.7.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.7.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.7.layer.1.layer_norm.weight',\n",
       " 'decoder.block.7.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.7.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.7.layer.2.layer_norm.weight',\n",
       " 'decoder.block.8.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.8.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.8.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.8.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.8.layer.0.layer_norm.weight',\n",
       " 'decoder.block.8.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.8.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.8.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.8.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.8.layer.1.layer_norm.weight',\n",
       " 'decoder.block.8.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.8.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.8.layer.2.layer_norm.weight',\n",
       " 'decoder.block.9.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.9.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.9.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.9.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.9.layer.0.layer_norm.weight',\n",
       " 'decoder.block.9.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.9.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.9.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.9.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.9.layer.1.layer_norm.weight',\n",
       " 'decoder.block.9.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.9.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.9.layer.2.layer_norm.weight',\n",
       " 'decoder.block.10.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.10.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.10.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.10.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.10.layer.0.layer_norm.weight',\n",
       " 'decoder.block.10.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.10.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.10.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.10.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.10.layer.1.layer_norm.weight',\n",
       " 'decoder.block.10.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.10.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.10.layer.2.layer_norm.weight',\n",
       " 'decoder.block.11.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.11.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.11.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.11.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.11.layer.0.layer_norm.weight',\n",
       " 'decoder.block.11.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.11.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.11.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.11.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.11.layer.1.layer_norm.weight',\n",
       " 'decoder.block.11.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.11.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.11.layer.2.layer_norm.weight',\n",
       " 'decoder.block.12.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.12.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.12.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.12.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.12.layer.0.layer_norm.weight',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.12.layer.1.layer_norm.weight',\n",
       " 'decoder.block.12.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.12.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.12.layer.2.layer_norm.weight',\n",
       " 'decoder.block.13.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.13.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.13.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.13.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.13.layer.0.layer_norm.weight',\n",
       " 'decoder.block.13.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.13.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.13.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.13.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.13.layer.1.layer_norm.weight',\n",
       " 'decoder.block.13.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.13.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.13.layer.2.layer_norm.weight',\n",
       " 'decoder.block.14.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.14.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.14.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.14.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.14.layer.0.layer_norm.weight',\n",
       " 'decoder.block.14.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.14.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.14.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.14.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.14.layer.1.layer_norm.weight',\n",
       " 'decoder.block.14.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.14.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.14.layer.2.layer_norm.weight',\n",
       " 'decoder.block.15.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.15.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.15.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.15.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.15.layer.0.layer_norm.weight',\n",
       " 'decoder.block.15.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.15.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.15.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.15.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.15.layer.1.layer_norm.weight',\n",
       " 'decoder.block.15.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.15.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.15.layer.2.layer_norm.weight',\n",
       " 'decoder.block.16.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.16.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.16.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.16.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.16.layer.0.layer_norm.weight',\n",
       " 'decoder.block.16.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.16.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.16.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.16.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.16.layer.1.layer_norm.weight',\n",
       " 'decoder.block.16.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.16.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.16.layer.2.layer_norm.weight',\n",
       " 'decoder.block.17.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.17.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.17.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.17.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.17.layer.0.layer_norm.weight',\n",
       " 'decoder.block.17.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.17.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.17.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.17.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.17.layer.1.layer_norm.weight',\n",
       " 'decoder.block.17.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.17.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.17.layer.2.layer_norm.weight',\n",
       " 'decoder.block.18.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.18.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.18.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.18.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.18.layer.0.layer_norm.weight',\n",
       " 'decoder.block.18.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.18.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.18.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.18.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.18.layer.1.layer_norm.weight',\n",
       " 'decoder.block.18.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.18.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.18.layer.2.layer_norm.weight',\n",
       " 'decoder.block.19.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.19.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.19.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.19.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.19.layer.0.layer_norm.weight',\n",
       " 'decoder.block.19.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.19.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.19.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.19.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.19.layer.1.layer_norm.weight',\n",
       " 'decoder.block.19.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.19.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.19.layer.2.layer_norm.weight',\n",
       " 'decoder.block.20.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.20.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.20.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.20.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.20.layer.0.layer_norm.weight',\n",
       " 'decoder.block.20.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.20.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.20.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.20.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.20.layer.1.layer_norm.weight',\n",
       " 'decoder.block.20.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.20.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.20.layer.2.layer_norm.weight',\n",
       " 'decoder.block.21.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.21.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.21.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.21.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.21.layer.0.layer_norm.weight',\n",
       " 'decoder.block.21.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.21.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.21.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.21.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.21.layer.1.layer_norm.weight',\n",
       " 'decoder.block.21.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.21.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.21.layer.2.layer_norm.weight',\n",
       " 'decoder.block.22.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.22.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.22.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.22.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.22.layer.0.layer_norm.weight',\n",
       " 'decoder.block.22.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.22.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.22.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.22.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.22.layer.1.layer_norm.weight',\n",
       " 'decoder.block.22.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.22.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.22.layer.2.layer_norm.weight',\n",
       " 'decoder.block.23.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.23.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.23.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.23.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.23.layer.0.layer_norm.weight',\n",
       " 'decoder.block.23.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.23.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.23.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.23.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.23.layer.1.layer_norm.weight',\n",
       " 'decoder.block.23.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.23.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.23.layer.2.layer_norm.weight',\n",
       " 'decoder.final_layer_norm.weight']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_train = [\n",
    "    \"shared.weight\",\n",
    "    \"encoder.block.0.layer.0.SelfAttention.q.weight\",\n",
    "    \"encoder.block.0.layer.0.SelfAttention.k.weight\",\n",
    "    \"encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\",\n",
    "]\n",
    "to_train_model = [(x, y) for index, x, y in kk]  # [:196]]\n",
    "# to_train_model=to_train_model+\n",
    "# to_train_model=[(x, y) for x, y in model.named_parameters() if x==\"encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding.weight\"]\n",
    "# to_train=[]\n",
    "to_train = [\n",
    "    \"encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\",\n",
    "    \"encoder.block.0.layer.0.layer_norm.weight\",\n",
    "    \"encoder.block.0.layer.1.DenseReluDense.wi.weight\",\n",
    "    \"encoder.block.0.layer.1.DenseReluDense.wo.weight\",\n",
    "    \"encoder.block.0.layer.1.layer_norm.weight\",\n",
    "    \"encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding.weight\",\n",
    "]\n",
    "# to_train_model=[(x,y) for x, y in model.named_parameters() if x in to_train]\n",
    "# to_train_model = [(x, y) for x, y in model.named_parameters()\n",
    "#                   if not x in not_train]\n",
    "\n",
    "for y in model.parameters():\n",
    "    y.requires_grad = False\n",
    "for x, y in to_train_model:\n",
    "    y.requires_grad = True\n",
    "[x for x, y in to_train_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# to_train_model=[y for x, y in model.named_parameters() if x in train_name_list ]\n",
    "# for y in to_train_model:\n",
    "#     y.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def shape(input):\n",
    "    return input.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# no_decay = ['layer_norm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in to_train_model], 'weight_decay': 0.0},\n",
    "#     ]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, eps=1e-8)\n",
    "# scheduler =  get_linear_schedule_with_warmup(optimizer,\n",
    "#                                 num_warmup_steps=0,\n",
    "#                                 num_training_steps=100000)\n",
    "to_train_model = [y for x, y in to_train_model]\n",
    "# optimizer = SGD(to_train_model, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_forward(input_tokens, model=model):\n",
    "    encoder_attentions = None\n",
    "    last_hidden = None\n",
    "    with torch.no_grad():\n",
    "        start = [0]\n",
    "        for k in range(MAX_ANSWER_LENGTH):\n",
    "            result = model(\n",
    "                input_ids=input_tokens.to(DEVICE),\n",
    "                decoder_input_ids=torch.tensor([start]).to(DEVICE),\n",
    "                output_attentions=True,\n",
    "            )\n",
    "            encoder_attentions = result.encoder_attentions\n",
    "            last_hidden = result.encoder_last_hidden_state\n",
    "            item = result.logits.argmax(dim=2)[0][-1].item()\n",
    "            start.append(item)\n",
    "            if item == 1:\n",
    "                break\n",
    "            # print(start)\n",
    "    return (\n",
    "        tokenizer.decode(start, skip_special_tokens=True),\n",
    "        tokenizer.convert_ids_to_tokens(start),\n",
    "        last_hidden,\n",
    "        encoder_attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "\n",
    "def get_loss(logits, labels):\n",
    "    loss = torch.tensor(0)\n",
    "    found = False\n",
    "    for i in range(len(labels[0])):\n",
    "        current_loss = loss_fn(logits[0][i], labels[0][i].to(DEVICE))\n",
    "\n",
    "        current_certainly = torch.exp(-current_loss)\n",
    "        if current_certainly < 0.9:\n",
    "            loss = current_loss\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        loss = loss_fn(logits[0], labels[0].to(DEVICE))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # pbar = trange(0, len(dataset_train), 24)\n",
    "    # loss_score = 0\n",
    "    # count = 0\n",
    "    # extra_info = \"\"\n",
    "    # step=0\n",
    "    # # if count>20:\n",
    "    # #     break\n",
    "    # # print(textwrap.fill(dataset_train[0][0]))\n",
    "    step = 0\n",
    "    pbar = trange(200)\n",
    "    for re in pbar:\n",
    "        input_tokens = check(dataset_train[step][0])\n",
    "        labels = tokenizer.encode(dataset_train[step][1], return_tensors=\"pt\")\n",
    "        result = model(\n",
    "            input_ids=input_tokens.to(DEVICE), labels=shape(labels).to(DEVICE)\n",
    "        )\n",
    "        loss = get_loss(result.logits, labels)\n",
    "        # print(result.logits.argmax(dim=2), labels)\n",
    "        optimizer.zero_grad()\n",
    "        # loss = result.loss\n",
    "        # print(result.logits, labels, loss)\n",
    "        if loss.item() != 0:\n",
    "            loss_score = loss.item()  # loss_score * 0.9 + loss.item() * 0.1\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # with torch.no_grad():\n",
    "        #     mong= model(input_ids=check(dataset_train[0][0]).to(DEVICE), decoder_input_ids=torch.tensor([[0]]).to(DEVICE))\n",
    "        #     print(mong.logits.argmax(dim=2).shape)\n",
    "        # print(tokenizer.decode())\n",
    "\n",
    "        extra_info = get_model_forward(check(dataset_train[step][0]).to(DEVICE))\n",
    "        pbar.set_postfix_str(f\"Loss: {loss_score:.10f}:{extra_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong size QUESTION: 88\n"
     ]
    }
   ],
   "source": [
    "data_array = [\n",
    "    (k, v, l.split(\" ( ) \")[1:])\n",
    "    for l, k, v in [\n",
    "        (dataset_train[x][0], check(dataset_train[x][0]), dataset_train[x][1])\n",
    "        for x in range(0, len(dataset_train), 24)\n",
    "    ]\n",
    "    if k is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckTransform(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # print(f\"'{sample[1]}'\")\n",
    "        return {\n",
    "            \"input_ids\": sample[0][0],\n",
    "            \"label_index\": sample[2].index(sample[1]),\n",
    "            \"all_labels\": sample[2],\n",
    "        }\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_array, transform=None):\n",
    "        self.dataset = dataset_array\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.dataset[idx])\n",
    "\n",
    "\n",
    "def collate(datas):\n",
    "    label_ids = tokenizer(sum([x[\"all_labels\"] for x in datas], []), padding=True)\n",
    "    wrapper = label_ids\n",
    "    wrapper[\"all_label_ids\"] = torch.tensor(wrapper.pop(\"input_ids\"))\n",
    "    # wrapper[\"label_index\"] = torch.tensor([x[\"label_index\"] for x in datas])\n",
    "    for k in wrapper[\"all_label_ids\"]:\n",
    "        k[k == tokenizer.pad_token_id] = -100\n",
    "    wrapper[\"all_decoder_attention_masks\"] = torch.tensor(wrapper.pop(\"attention_mask\"))\n",
    "    wrapper[\"input_ids\"] = torch.stack([x[\"input_ids\"] for x in datas])\n",
    "    wrapper[\"label_index\"] = torch.tensor([x[\"label_index\"] for x in datas])\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "loi_dataloader = DataLoader(\n",
    "    CustomDataset(\n",
    "        data_array,\n",
    "        CheckTransform(),\n",
    "    ),\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "# for k in loi_dataloader:\n",
    "#     print(k[\"all_label_ids\"])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 898704\n",
    "# hidden state 242688\n",
    "# classification_layer = nn.Linear(242688, 4).to(DEVICE)\n",
    "optimizer = Adafactor(\n",
    "    to_train_model,  # + [x for x in classification_layer.parameters()],\n",
    "    relative_step=True,\n",
    "    warmup_init=True,\n",
    "    lr=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/496 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.290: 100%|██████████| 496/496 [08:43<00:00,  1.06s/it, buy less with monopoly money]                                                                                     \n",
      "Loss: 0.200: 100%|██████████| 496/496 [08:25<00:00,  1.02s/it, have lunch with friends]     \n",
      "Loss: 0.075: 100%|██████████| 496/496 [08:26<00:00,  1.02s/it, buy less with monopoly money]\n",
      "Loss: 0.066: 100%|██████████| 496/496 [08:27<00:00,  1.02s/it, buy less with monopoly money]\n",
      "Loss: 0.061:   5%|▍         | 24/496 [00:24<08:09,  1.04s/it, buy less with monopoly money]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_733599/3573476198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mloss_running_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_running_score\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.9\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def turn_position_learning(on):\n",
    "    for x in all_position_weight:\n",
    "        x.requires_grad = on\n",
    "\n",
    "\n",
    "loss_running_score = 0\n",
    "correct_running_score = 0\n",
    "conform_running_score = 0\n",
    "count = 0\n",
    "extra_info = \"\"\n",
    "res_tokens = []\n",
    "accumulate = 10\n",
    "optimizer.zero_grad()\n",
    "set_seed(42)\n",
    "turn_position = False\n",
    "turn_position_learning(False)\n",
    "for learn_pos in range(6):\n",
    "    pbar = tqdm(loi_dataloader)\n",
    "    for wrapper in pbar:\n",
    "        count += 1\n",
    "        # if count%20==0:\n",
    "        #     turn_position=not turn_position\n",
    "        #     turn_position_learning(turn_position)\n",
    "        # if count>20:\n",
    "        #     break\n",
    "        # print(textwrap.fill(dataset_train[0][0]))\n",
    "        only_correct_label_ids = torch.stack(\n",
    "            [\n",
    "                wrapper[\"all_label_ids\"][batch_index * 4 + x]\n",
    "                for batch_index, x in enumerate(wrapper[\"label_index\"])\n",
    "            ]\n",
    "        )\n",
    "        only_correct_decoder_attention_mask = torch.stack(\n",
    "            [\n",
    "                wrapper[\"all_decoder_attention_masks\"][batch_index * 4 + x]\n",
    "                for batch_index, x in enumerate(wrapper[\"label_index\"])\n",
    "            ]\n",
    "        )\n",
    "        result = model(\n",
    "            input_ids=wrapper[\"input_ids\"].to(DEVICE),\n",
    "            labels=only_correct_label_ids.to(DEVICE),\n",
    "            decoder_attention_mask=only_correct_decoder_attention_mask.to(\n",
    "                DEVICE\n",
    "            ),  # output_attentions=True\n",
    "        )\n",
    "        # conform_loss = 0\n",
    "        # for batch in range(wrapper[\"input_ids\"].shape[0]):\n",
    "        #     selected_answer = result.logits[batch].argmax(dim=1)\n",
    "        #     found = False\n",
    "        #     conform_losses = [0, 0, 0, 0]\n",
    "        #     for each_answer in range(4):\n",
    "        #         tui_batch = wrapper[\"all_label_ids\"][batch * 4 + each_answer]\n",
    "        #         conform_losses[each_answer] += loss_fn(\n",
    "        #                     result.logits[batch], tui_batch.to(DEVICE)\n",
    "        #                 )\n",
    "        #         # for m in range(len(tui_batch)):\n",
    "        #         #     if selected_answer[m] != tui_batch[m] and tui_batch[m] != -100:\n",
    "        #         #         conform_losses[each_answer] += loss_fn(\n",
    "        #         #             result.logits[batch][m], tui_batch[m].to(DEVICE)\n",
    "        #         #         )\n",
    "        #         # conform_min_index = torch.argmin(conform_losses)\n",
    "        #         # print(conform_min_index)\n",
    "        #     conform_loss += min(conform_losses)  # conform_losses[conform_min_index]\n",
    "        # conform_loss = conform_loss / wrapper[\"input_ids\"].shape[0]\n",
    "        # kk1=result.encoder_attentions\n",
    "        # break\n",
    "        # final_logits = classification_layer(\n",
    "        #     torch.flatten(result.encoder_last_hidden_state, start_dim=1)\n",
    "        # )\n",
    "        # loss = loss_fn(final_logits, wrapper[\"label_index\"].to(DEVICE))\n",
    "        loss = result.loss\n",
    "        loss_running_score = loss_running_score * 0.9 + loss.item() * 0.1\n",
    "        if loss != 0:\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # scheduler.step()\n",
    "        with torch.no_grad():\n",
    "            if count % 10 == 0:\n",
    "                extra_info, res_tokens, _, _ = get_model_forward(\n",
    "                    check(dataset_test[0][0]).to(DEVICE)\n",
    "                )\n",
    "                # final_logits = classification_layer(torch.flatten(hidden, start_dim=1))\n",
    "                # extra_info = str(final_logits.argmax())\n",
    "            pbar.set_description_str(f\"Loss: {loss_running_score:.3f}\")\n",
    "            pbar.set_postfix_str(extra_info)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"loi_best_model.pkl\", from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure accuracy and answer coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:51<00:00,  4.47it/s, 267, 346, 336, 488,500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 351/4957 [01:20<17:34,  4.37it/s, 331, 338, 337, 350,351]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_733599/3184291621.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_convert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_733599/2040238227.py\u001b[0m in \u001b[0;36mget_model_forward\u001b[0;34m(input_tokens)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_ANSWER_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             result = model(\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1709\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m             \u001b[0;31m# Convert encoder inputs in embeddings if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1711\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1712\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 )\n\u001b[1;32m   1114\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1116\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    696\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    600\u001b[0m     ):\n\u001b[1;32m    601\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# compute scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         scores = torch.matmul(\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for data in [dataset_test, dataset_train]:\n",
    "    print(f\"test {data==dataset_test}\")\n",
    "    count = 0\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count10 = 0\n",
    "    total = 0\n",
    "    pbar1 = trange(int(len(data) / 24))\n",
    "    for ques in pbar1:\n",
    "        question = data[24 * ques][0]\n",
    "        key = data[24 * ques][1]\n",
    "        question_convert = check(question)\n",
    "        if question_convert is None:\n",
    "            continue\n",
    "        total += 1\n",
    "        answer, _, _, _ = get_model_forward(question_convert.to(DEVICE))\n",
    "        if key == answer:\n",
    "            count += 1\n",
    "        if key[0] == answer[0]:\n",
    "            count1 += 1\n",
    "        if key[:2] == answer[:2]:\n",
    "            count2 += 1\n",
    "        if answer in question:\n",
    "            count10 += 1\n",
    "        pbar1.set_postfix_str(f\"{count}, {count1}, {count2}, {count10},{total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure resilient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def measure_unalike(arr):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    return 1 - ((arr / n) ** 2).sum()\n",
    "\n",
    "\n",
    "measure_unalike([\"a\", \"a\", \"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:07<00:00,  3.19it/s]\n",
      "100%|██████████| 24/24 [00:04<00:00,  5.32it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  6.63it/s]\n",
      "100%|██████████| 24/24 [00:05<00:00,  4.41it/s]\n",
      "100%|██████████| 24/24 [00:04<00:00,  5.28it/s]\n",
      "100%|██████████| 5/5 [00:25<00:00,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean unalikeability: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for data in [dataset_test]:\n",
    "    count = 0\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count10 = 0\n",
    "    total = 0\n",
    "    question_index = range(5)\n",
    "    pbar1 = tqdm(question_index)\n",
    "    unalike = []\n",
    "    for ques1 in pbar1:\n",
    "        answer_set = []\n",
    "        for m in trange(24):\n",
    "            ques = ques1 * 24 + m\n",
    "            question = data[ques][0]\n",
    "            key = data[ques][1]\n",
    "            question_convert = check(question)\n",
    "            if question_convert is None:\n",
    "                continue\n",
    "            total += 1\n",
    "            answer, _, _, _ = get_model_forward(\n",
    "                question_convert.to(DEVICE), model=model2\n",
    "            )\n",
    "            answer_set.append(answer)\n",
    "        unalike.append(measure_unalike(answer_set))\n",
    "print(f\"Mean unalikeability: {sum(unalike)/len(unalike)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbar = trange(0, len(dataset_train), 24)\n",
    "# loss_score = 0\n",
    "# count = 0\n",
    "# extra_info = \"\"\n",
    "# set_seed(42)\n",
    "# res_tokens=[]\n",
    "# for learn_pos in range(10):\n",
    "#     for step in pbar:\n",
    "#         count += 1\n",
    "#         # if count>20:\n",
    "#         #     break\n",
    "#         # print(textwrap.fill(dataset_train[0][0]))\n",
    "#         input_tokens = check(dataset_train[step][0])\n",
    "#         if input_tokens is None:\n",
    "#             continue\n",
    "#         labels = tokenizer.encode(dataset_train[step][1], return_tensors=\"pt\")\n",
    "#         result = model(input_ids=input_tokens.to(DEVICE), labels=shape(labels).to(DEVICE))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss =loss_fn(result.logits[0][learn_pos],labels[0][learn_pos].to(DEVICE))\n",
    "#         loss_score = loss_score * 0.9 + loss.item() * 0.1\n",
    "#         if loss.item()!=0:\n",
    "#             loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # scheduler.step()\n",
    "#         with torch.no_grad():\n",
    "#             if count % 10 == 0:\n",
    "#                 extra_info, res_tokens = get_model_forward(check(dataset_test[0][0]).to(DEVICE))\n",
    "#             pbar.set_description_str(f\"Loss: {loss_score:.2f}\")\n",
    "#             pbar.set_postfix_str(res_tokens[:learn_pos+2])\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"loi_vanilla.pkl\", device_map=f\"cuda:{DEVICE}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at loi_best_model.pkl were not used when initializing T5ForConditionalGeneration: ['encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.0.weight', 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.1.weight', 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.2.weight', 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.3.weight', 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.4.weight', 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.5.weight', 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.0.weight', 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.1.weight', 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.2.weight', 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.3.weight', 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.4.weight', 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.5.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model2=T5ForConditionalGeneration.from_pretrained(\n",
    "    \"loi_best_model.pkl\", device_map=f\"cuda:{DEVICE}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
