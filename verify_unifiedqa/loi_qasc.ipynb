{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf083f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:41.114964Z",
     "iopub.status.busy": "2024-04-06T18:05:41.114334Z",
     "iopub.status.idle": "2024-04-06T18:05:43.057785Z",
     "shell.execute_reply": "2024-04-06T18:05:43.058206Z"
    },
    "papermill": {
     "duration": 1.971373,
     "end_time": "2024-04-06T18:05:43.058462",
     "exception": false,
     "start_time": "2024-04-06T18:05:41.087089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import trange\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Adafactor\n",
    "from functools import wraps, partial\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from random_utils import set_seed\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f21b991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:43.098957Z",
     "iopub.status.busy": "2024-04-06T18:05:43.097640Z",
     "iopub.status.idle": "2024-04-06T18:05:43.101181Z",
     "shell.execute_reply": "2024-04-06T18:05:43.100654Z"
    },
    "papermill": {
     "duration": 0.024363,
     "end_time": "2024-04-06T18:05:43.101286",
     "exception": false,
     "start_time": "2024-04-06T18:05:43.076923",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_name='google-t5/t5-large'\n",
    "max_answer_length = 300\n",
    "BATCH_SIZE=10\n",
    "DATA_NAME_SINGLE='arc_easy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9754f4cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:43.141476Z",
     "iopub.status.busy": "2024-04-06T18:05:43.140870Z",
     "iopub.status.idle": "2024-04-06T18:05:43.143058Z",
     "shell.execute_reply": "2024-04-06T18:05:43.142472Z"
    },
    "papermill": {
     "duration": 0.023566,
     "end_time": "2024-04-06T18:05:43.143162",
     "exception": false,
     "start_time": "2024-04-06T18:05:43.119596",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_name = \"google-t5/t5-large\"\n",
    "BATCH_SIZE = 10\n",
    "DATA_NAME_SINGLE = \"qasc\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5529a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:43.183847Z",
     "iopub.status.busy": "2024-04-06T18:05:43.183246Z",
     "iopub.status.idle": "2024-04-06T18:05:43.185381Z",
     "shell.execute_reply": "2024-04-06T18:05:43.185793Z"
    },
    "papermill": {
     "duration": 0.02434,
     "end_time": "2024-04-06T18:05:43.185913",
     "exception": false,
     "start_time": "2024-04-06T18:05:43.161573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loi params: google-t5/t5-large#####300#####10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loi params: {model_name}{'#'*5}{max_answer_length}{'#'*5}{BATCH_SIZE}\")\n",
    "DATABASE_NAME = [DATA_NAME_SINGLE]#,'arc_hard']#,'race','mctest',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6587f049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:43.227205Z",
     "iopub.status.busy": "2024-04-06T18:05:43.226597Z",
     "iopub.status.idle": "2024-04-06T18:05:48.589360Z",
     "shell.execute_reply": "2024-04-06T18:05:48.589835Z"
    },
    "papermill": {
     "duration": 5.384975,
     "end_time": "2024-04-06T18:05:48.589976",
     "exception": false,
     "start_time": "2024-04-06T18:05:43.205001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldh0033@auburn.edu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"loi_with_padding_just_same_answer_len_70.pkl\"\n",
    "\n",
    "# \"loi_with_padding_1.pkl\"#\n",
    "# model_name = (\n",
    "#     \"allenai/unifiedqa-v2-t5-large-1363200\"  # you can specify the model size here\n",
    "# )\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_original = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name, device_map=f\"auto\")  # 'auto')\n",
    "model = model_original\n",
    "# model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0176a646",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:48.633654Z",
     "iopub.status.busy": "2024-04-06T18:05:48.633009Z",
     "iopub.status.idle": "2024-04-06T18:05:48.634968Z",
     "shell.execute_reply": "2024-04-06T18:05:48.634347Z"
    },
    "papermill": {
     "duration": 0.025248,
     "end_time": "2024-04-06T18:05:48.635086",
     "exception": false,
     "start_time": "2024-04-06T18:05:48.609838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00eb9967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:48.679950Z",
     "iopub.status.busy": "2024-04-06T18:05:48.676357Z",
     "iopub.status.idle": "2024-04-06T18:05:48.694350Z",
     "shell.execute_reply": "2024-04-06T18:05:48.694837Z"
    },
    "papermill": {
     "duration": 0.039919,
     "end_time": "2024-04-06T18:05:48.694949",
     "exception": false,
     "start_time": "2024-04-06T18:05:48.655030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max: 712 \n",
      "        min:104\n",
      "            avg:227.3320629456602\n",
      "            len: 8134\n",
      " max: 71 \n",
      "        min:2\n",
      "            avg:11.033316941234325\n",
      "            len: 8134\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_test=[]\n",
    "dataset_train=[]\n",
    "[dataset_test.append([]) for x in DATABASE_NAME]\n",
    "moi_index=[]\n",
    "\n",
    "for i, dataname in enumerate(DATABASE_NAME):\n",
    "    dataset_test[i]=pickle.load(\n",
    "        open(f\"multiple_choice_datasets/{dataname}_test.pkl\", \"rb\"))\n",
    "    moi_index.append(len(dataset_train))\n",
    "    dataset_train.extend(pickle.load(\n",
    "        open(f\"multiple_choice_datasets/{dataname}_train.pkl\", \"rb\")))\n",
    "for i in range(2):    \n",
    "    print(f\"\"\" max: {max([len(x[i]) for x in dataset_train])} \n",
    "        min:{min([len(x[i]) for x in dataset_train])}\n",
    "            avg:{sum([len(x[i]) for x in dataset_train])/len(dataset_train)}\n",
    "            len: {len(dataset_train)}\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "717e8fa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:48.737418Z",
     "iopub.status.busy": "2024-04-06T18:05:48.736807Z",
     "iopub.status.idle": "2024-04-06T18:05:48.738635Z",
     "shell.execute_reply": "2024-04-06T18:05:48.739066Z"
    },
    "papermill": {
     "duration": 0.024695,
     "end_time": "2024-04-06T18:05:48.739176",
     "exception": false,
     "start_time": "2024-04-06T18:05:48.714481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import textwrap\n",
    "\n",
    "# ques=4\n",
    "# print(textwrap.fill(dataset_test[ques][0]))\n",
    "# print('correct: ',dataset_test[ques][1],'length',len(dataset_test[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85ff155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:48.783930Z",
     "iopub.status.busy": "2024-04-06T18:05:48.782067Z",
     "iopub.status.idle": "2024-04-06T18:05:48.785700Z",
     "shell.execute_reply": "2024-04-06T18:05:48.786131Z"
    },
    "papermill": {
     "duration": 0.026635,
     "end_time": "2024-04-06T18:05:48.786246",
     "exception": false,
     "start_time": "2024-04-06T18:05:48.759611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hook(hook_before, oldfunc, hook_after):\n",
    "\n",
    "    def foo(*args, **kwargs):\n",
    "        hook_before(*args, **kwargs)\n",
    "        aa = oldfunc(*args, **kwargs)\n",
    "        hook_after(*args, **kwargs)\n",
    "        return aa\n",
    "\n",
    "    return foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ccb3a51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:48.833872Z",
     "iopub.status.busy": "2024-04-06T18:05:48.833240Z",
     "iopub.status.idle": "2024-04-06T18:05:48.837593Z",
     "shell.execute_reply": "2024-04-06T18:05:48.838008Z"
    },
    "papermill": {
     "duration": 0.031964,
     "end_time": "2024-04-06T18:05:48.838117",
     "exception": false,
     "start_time": "2024-04-06T18:05:48.806153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def new_compute_bias(self, query_length, key_length, device=None):\n",
    "    \"\"\"Compute binned relative position bias\"\"\"\n",
    "    if device is None:\n",
    "        device = self.relative_attention_bias.weight.device\n",
    "    context_position = torch.arange(query_length, dtype=torch.long, device=device)[\n",
    "        :, None\n",
    "    ]\n",
    "    memory_position = torch.arange(\n",
    "        key_length, dtype=torch.long, device=device)[None, :]\n",
    "\n",
    "    relative_position = (\n",
    "        memory_position - context_position\n",
    "    )  # shape (query_length, key_length)\n",
    "    if self.is_decoder:\n",
    "        values = values.permute([2, 0, 1]).unsqueeze(\n",
    "            0\n",
    "        )  # shape (1, num_heads, query_length, key_length)\n",
    "        return values\n",
    "\n",
    "    anchors = self.anchor_array\n",
    "    values = []\n",
    "    for anchor in anchors:\n",
    "        mot = [[anchor[idx], anchor[idx+1]] for idx in range(len(anchor)-1)]\n",
    "        max_answer_length = max([x[1] - x[0] for x in mot])\n",
    "        # print(a, b, c, d, max_answer_length)\n",
    "        context_position_new = context_position.clone()\n",
    "        for i in range(1, len(mot)):\n",
    "            context_position_new[mot[i][0]: mot[i][1]\n",
    "                                 ] = context_position_new[mot[i][0]: mot[i][1]] - mot[0][0]\n",
    "        context_position_new[-1] = mot[0][0] + 2 * max_answer_length\n",
    "        memory_position_new = context_position_new.clone().view(1, -1)\n",
    "        relative_position = (\n",
    "            memory_position_new - context_position_new\n",
    "        )  # shape (query_length, key_length)\n",
    "        for i in range(len(mot)):\n",
    "            for j in range(len(mot)):\n",
    "                if i != j:\n",
    "                    x = mot[i]\n",
    "                    y = mot[j]\n",
    "                    relative_position[x[0]: x[1], y[0]: y[1]] += max_answer_length\n",
    "        relative_position_bucket = self._relative_position_bucket(\n",
    "            relative_position,  # shape (query_length, key_length)\n",
    "            bidirectional=(not self.is_decoder),\n",
    "            num_buckets=self.relative_attention_num_buckets,\n",
    "            max_distance=self.relative_attention_max_distance,\n",
    "        )\n",
    "        value = self.relative_attention_bias(relative_position_bucket)\n",
    "        values.append(value)\n",
    "    values = torch.stack(values)  # shape [1, 91, 91, 16]\n",
    "    values = values.permute(\n",
    "        [0, 3, 1, 2]\n",
    "    )  # shape (batch size, num_heads, query_length, key_length)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8728d813",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:48.889629Z",
     "iopub.status.busy": "2024-04-06T18:05:48.884357Z",
     "iopub.status.idle": "2024-04-06T18:05:48.893433Z",
     "shell.execute_reply": "2024-04-06T18:05:48.892856Z"
    },
    "papermill": {
     "duration": 0.035952,
     "end_time": "2024-04-06T18:05:48.893530",
     "exception": false,
     "start_time": "2024-04-06T18:05:48.857578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beads of water can be formed by clouds. What type of water formation\n",
      "is formed by clouds? \\n ( ) pearls ( ) streams ( ) shells ( ) diamonds\n",
      "( ) rain ( ) beads ( ) cooled ( ) liquid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import textwrap\n",
    "\n",
    "\n",
    "extra_dim_learning = []\n",
    "\n",
    "\n",
    "def set_mode(MODE):\n",
    "    itself = model.encoder.block[0].layer[0].SelfAttention\n",
    "    if MODE == \"new\":\n",
    "        # itself.forward = partial(modified_self_attention_forward, itself)\n",
    "        itself.compute_bias = partial(new_compute_bias, itself)\n",
    "        model.forward = hook(input_before_hooker, partial(\n",
    "            T5ForConditionalGeneration.forward, model), input_after_hooker)\n",
    "\n",
    "    else:\n",
    "        # itself.forward = partial(\n",
    "        #     model.encoder.block[0].layer[0].SelfAttention.__class__.forward, itself\n",
    "        # )\n",
    "        itself.compute_bias = partial(\n",
    "            model.encoder.block[0].layer[0].SelfAttention.__class__.compute_bias, itself\n",
    "        )\n",
    "        model.forward = T5ForConditionalGeneration.forward\n",
    "\n",
    "\n",
    "def check_encoded(all_input_ids):\n",
    "    anchors = []\n",
    "    for input_ids in all_input_ids:\n",
    "        # print('\\n'.join([f'{x.item()},{y}' for x,y in zip(input_ids, tokens)][50:]))\n",
    "        original = input_ids.tolist()\n",
    "        anchor = []\n",
    "        for i in range(len(input_ids)):\n",
    "            if (\n",
    "                i < len(input_ids) - 2\n",
    "                and input_ids[i] == 41\n",
    "                and input_ids[i + 1] == 3\n",
    "                and input_ids[i + 2] == 61\n",
    "            ) or original[i] == 1:\n",
    "                anchor.append(i)\n",
    "        anchors.append(anchor)\n",
    "    return anchors\n",
    "\n",
    "\n",
    "def input_before_hooker(*args, **kwargs):\n",
    "    input_ids = kwargs[\"input_ids\"]\n",
    "    # print('old ',input_ids)\n",
    "    anchors = check_encoded(input_ids)\n",
    "    final_inputs = []\n",
    "    for input_id, anchor in zip(input_ids, anchors):\n",
    "        input_id = input_id.tolist()\n",
    "        \n",
    "        real_max_len = max([anchor[idx+1] - anchor[idx]\n",
    "                           for idx in range(len(anchor)-1)])\n",
    "        if real_max_len > max_answer_length:\n",
    "            print(f'ALERT: MAX LENGTH IS {real_max_len}')\n",
    "        for x in reversed(range(1, len(anchor))):\n",
    "            if anchor[x] - anchor[x - 1] < max_answer_length:\n",
    "                [\n",
    "                    input_id.insert(anchor[x], 0)\n",
    "                    for _ in range(max_answer_length - (anchor[x] - anchor[x - 1]))\n",
    "                ]\n",
    "\n",
    "        final_inputs.append(input_id)\n",
    "\n",
    "    max_length = max([len(input) for input in final_inputs])\n",
    "    mask = [[1]*max_length]*len(final_inputs)\n",
    "    for idx, input in enumerate(final_inputs):\n",
    "        for x in range(max_length):\n",
    "            if x >= len(input):\n",
    "                mask[idx][x] = 0\n",
    "        for x in range(max_length-len(input)):\n",
    "            input.append(0)\n",
    "    kwargs[\"input_ids\"] = torch.tensor(final_inputs).to(input_ids.device)\n",
    "    kwargs['attention_mask'] = torch.tensor(mask).to(input_ids.device)\n",
    "    # print('new ',kwargs[\"input_ids\"])\n",
    "    anchors = check_encoded(kwargs[\"input_ids\"])\n",
    "    model.encoder.block[0].layer[0].SelfAttention.anchor_array = anchors\n",
    "\n",
    "\n",
    "def input_after_hooker(*args, **kwargs):\n",
    "    model.encoder.block[0].layer[0].SelfAttention.anchor_array = None\n",
    "\n",
    "\n",
    "print(textwrap.fill(dataset_train[0][0]))\n",
    "\n",
    "# set_mode('old')\n",
    "set_mode('new')\n",
    "# run_model([dataset_train[0][0],dataset_train[1][0]])\n",
    "# run_model([\"\"\"A person wants to start\"\"\", 'mot hai ba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b1a5252",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:48.941796Z",
     "iopub.status.busy": "2024-04-06T18:05:48.940413Z",
     "iopub.status.idle": "2024-04-06T18:05:48.943867Z",
     "shell.execute_reply": "2024-04-06T18:05:48.943331Z"
    },
    "papermill": {
     "duration": 0.030376,
     "end_time": "2024-04-06T18:05:48.943965",
     "exception": false,
     "start_time": "2024-04-06T18:05:48.913589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def measure_unalike(arr, print_arr=False):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    if print_arr:\n",
    "        print(arr)\n",
    "    return 1 - ((arr / n)**2).sum()\n",
    "\n",
    "\n",
    "question_to_do = 5\n",
    "per_question = 20\n",
    "\n",
    "\n",
    "def get_model_forward(input_ids, attention_mask, model=model):\n",
    "    with torch.no_grad():\n",
    "        start = []\n",
    "        [start.append([0]) for x in range(len(input_ids))]\n",
    "        for k in range(max_answer_length):\n",
    "            # print(torch.tensor(start).shape)\n",
    "            result = model(\n",
    "                input_ids=input_ids.to(DEVICE),\n",
    "                attention_mask=attention_mask.to(DEVICE),\n",
    "                decoder_input_ids=torch.tensor(start).to(DEVICE),\n",
    "                output_attentions=True,\n",
    "            )\n",
    "            item = result.logits.argmax(dim=2)[:, -1]\n",
    "            # print('loi',result.logits.shape, item)\n",
    "            for index in range(len(item)):\n",
    "                start[index].append(item[index].item())\n",
    "            if torch.allclose(item, torch.tensor(1)):\n",
    "                break\n",
    "            #     break\n",
    "    result = []\n",
    "    for batch in start:\n",
    "        y = -1\n",
    "        for index, x in enumerate(batch):\n",
    "            if x == 1:\n",
    "                y = index\n",
    "                break\n",
    "        result.append(batch[:y+1] if y > -1 else batch)\n",
    "    return [tokenizer.decode(x, skip_special_tokens=True) for x in result]\n",
    "\n",
    "\n",
    "def run_model(input_strs):\n",
    "    if input_strs is str:\n",
    "        input_strs = [input_strs]\n",
    "    input_ids_wrapper = tokenizer(\n",
    "        input_strs, padding=True, return_tensors='pt')\n",
    "\n",
    "    answer = get_model_forward(input_ids_wrapper['input_ids'],\n",
    "                               input_ids_wrapper['attention_mask'])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b984145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:48.992595Z",
     "iopub.status.busy": "2024-04-06T18:05:48.992009Z",
     "iopub.status.idle": "2024-04-06T18:05:48.993776Z",
     "shell.execute_reply": "2024-04-06T18:05:48.994184Z"
    },
    "papermill": {
     "duration": 0.030545,
     "end_time": "2024-04-06T18:05:48.994292",
     "exception": false,
     "start_time": "2024-04-06T18:05:48.963747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kk = [(index, x, y) for index, (x, y) in enumerate(model.named_parameters())\n",
    "      if y.requires_grad == True]\n",
    "[(index, x) for index, x, y in kk if \"decoder\" in x]\n",
    "len(kk)\n",
    "all_position_weight = [\n",
    "    y for index, x, y in kk if (\"extra_dimension_embedding\" in x) or (\n",
    "        (\"encoder\" in x) and (\"relative_attention_bias\" in x))\n",
    "]\n",
    "to_train_model = [y for index, x, y in kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3acf66ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:49.056318Z",
     "iopub.status.busy": "2024-04-06T18:05:49.046444Z",
     "iopub.status.idle": "2024-04-06T18:05:49.058555Z",
     "shell.execute_reply": "2024-04-06T18:05:49.058006Z"
    },
    "papermill": {
     "duration": 0.044382,
     "end_time": "2024-04-06T18:05:49.058650",
     "exception": false,
     "start_time": "2024-04-06T18:05:49.014268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_array = [\n",
    "    (ques, answer, ques.split(\" ( ) \")[1:])\n",
    "    for ques, answer in [\n",
    "        (\n",
    "            dataset_train[x][0],\n",
    "            dataset_train[x][1],\n",
    "        )\n",
    "        for x in range(len(dataset_train))\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f199847b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:49.101794Z",
     "iopub.status.busy": "2024-04-06T18:05:49.101208Z",
     "iopub.status.idle": "2024-04-06T18:05:49.103427Z",
     "shell.execute_reply": "2024-04-06T18:05:49.102853Z"
    },
    "papermill": {
     "duration": 0.025147,
     "end_time": "2024-04-06T18:05:49.103521",
     "exception": false,
     "start_time": "2024-04-06T18:05:49.078374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [(print(idx),CheckTransform.__call__(None, data_array[idx])) for idx,x in enumerate(data_array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b289a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:49.152974Z",
     "iopub.status.busy": "2024-04-06T18:05:49.152359Z",
     "iopub.status.idle": "2024-04-06T18:05:49.154276Z",
     "shell.execute_reply": "2024-04-06T18:05:49.154702Z"
    },
    "papermill": {
     "duration": 0.031328,
     "end_time": "2024-04-06T18:05:49.154808",
     "exception": false,
     "start_time": "2024-04-06T18:05:49.123480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CheckTransform(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # print(f\"'{sample[1]}'\")\n",
    "        try:\n",
    "            return {\n",
    "            \"input_ids\": sample[0],\n",
    "            \"label_index\": sample[2].index(sample[1]),\n",
    "            \"all_labels\": sample[2],\n",
    "            }\n",
    "        except:\n",
    "            raise Exception('cao')\n",
    "            print('all answer: ',sample[2])\n",
    "            print('answer',sample[1])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_array, transform=None):\n",
    "        self.dataset = dataset_array\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.dataset[idx])\n",
    "\n",
    "\n",
    "def collate(datas):\n",
    "    wrapper = tokenizer([x[\"all_labels\"][x['label_index']] for x in datas],\n",
    "                        padding=True)\n",
    "    wrapper[\"label_ids\"] = torch.tensor(wrapper.pop(\"input_ids\"))\n",
    "    # wrapper[\"label_index\"] = torch.tensor([x[\"label_index\"] for x in datas])\n",
    "    for k in wrapper[\"label_ids\"]:\n",
    "        k[k == tokenizer.pad_token_id] = -100\n",
    "    wrapper[\"all_decoder_attention_masks\"] = torch.tensor(\n",
    "        wrapper.pop(\"attention_mask\"))\n",
    "\n",
    "    for_input = tokenizer([x[\"input_ids\"] for x in datas], padding=True)\n",
    "    wrapper['input_ids'] = torch.tensor(for_input.pop('input_ids'))\n",
    "    wrapper['attention_mask'] = torch.tensor(for_input.pop('attention_mask'))\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "loi_dataloader = DataLoader(\n",
    "    CustomDataset(\n",
    "        data_array,\n",
    "        CheckTransform(),\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "# for k in loi_dataloader:\n",
    "#     print(k[\"all_label_ids\"])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1abad30c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:49.202452Z",
     "iopub.status.busy": "2024-04-06T18:05:49.201818Z",
     "iopub.status.idle": "2024-04-06T18:05:49.206377Z",
     "shell.execute_reply": "2024-04-06T18:05:49.206804Z"
    },
    "papermill": {
     "duration": 0.032208,
     "end_time": "2024-04-06T18:05:49.206912",
     "exception": false,
     "start_time": "2024-04-06T18:05:49.174704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "# attention 898704\n",
    "# hidden state 242688\n",
    "# classification_layer = nn.Linear(242688, 4).to(DEVICE)\n",
    "# optimizer =AdamW(lr=1e-4)\n",
    "optimizer = Adafactor(\n",
    "    to_train_model,  # + [x for x in classification_layer.parameters()],\n",
    "    relative_step=False,\n",
    "    lr=1e-4,\n",
    ")\n",
    "# lr_schedule=AdafactorSchedule(optimizer)\n",
    "\n",
    "def evaluate():\n",
    "    ll=1\n",
    "    wrong_answers=[[] for x in range(ll)]\n",
    "    got_2=[[] for x in range(ll)]\n",
    "    got_1=[[] for x in range(ll)]\n",
    "    answers=[[] for x in range(ll)]\n",
    "    for ix in range(ll):\n",
    "        print(f'Name {DATABASE_NAME[ix]}')\n",
    "        #[pickle.load(open(f\"multiple_choice_datasets/obqa_fact_test.pkl\", \"rb\"))]:\n",
    "        # print(f\"test {data==dataset_test}\")\n",
    "        count = 0\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "        count10 = 0\n",
    "        total = 0\n",
    "        data=dataset_test[ix]\n",
    "        pbar1 = trange(len(data))\n",
    "        for ques in pbar1:\n",
    "            question = data[ques][0]\n",
    "            key = data[ques][1]\n",
    "            total += 1\n",
    "            answer = run_model(question)[0]\n",
    "            answers[ix].append(answer)\n",
    "            if key == answer:\n",
    "                count += 1\n",
    "            else:\n",
    "                wrong_answers[ix].append(ques)\n",
    "            if key[0] == answer[0]:\n",
    "                count1 += 1\n",
    "                got_1[ix].append(ques)\n",
    "            if key[:2] == answer[:2]:\n",
    "                count2 += 1\n",
    "                got_2[ix].append(ques)\n",
    "            if answer in question:\n",
    "                count10 += 1\n",
    "            pbar1.set_postfix_str(\n",
    "                f\"{count}, {count1}, {count2}, {count10},{total},{count/total*100:.2f},{count10/total*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78ac6334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:05:49.254634Z",
     "iopub.status.busy": "2024-04-06T18:05:49.253783Z",
     "iopub.status.idle": "2024-04-05T04:52:18.151577Z",
     "shell.execute_reply": "2024-04-05T04:52:18.151177Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-04-06T18:05:49.226973",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                              | 0/814 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 1.206:   0%|                                                 | 0/814 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 1.206:   0%|                                                 | 0/814 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 1.206:   0%|                                       | 1/814 [00:04<1:01:34,  4.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.258:   0%|                                       | 1/814 [00:06<1:01:34,  4.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.258:   0%|                                       | 1/814 [00:06<1:01:34,  4.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.258:   0%|                                         | 2/814 [00:06<40:08,  2.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.667:   0%|                                         | 2/814 [00:08<40:08,  2.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.667:   0%|                                         | 2/814 [00:08<40:08,  2.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.667:   0%|▏                                        | 3/814 [00:08<33:10,  2.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.276:   0%|▏                                        | 3/814 [00:10<33:10,  2.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.276:   0%|▏                                        | 3/814 [00:10<33:10,  2.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.276:   0%|▏                                        | 4/814 [00:10<29:34,  2.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.981:   0%|▏                                        | 4/814 [00:11<29:34,  2.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.981:   0%|▏                                        | 4/814 [00:11<29:34,  2.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.981:   1%|▎                                        | 5/814 [00:11<27:48,  2.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.727:   1%|▎                                        | 5/814 [00:13<27:48,  2.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.727:   1%|▎                                        | 5/814 [00:13<27:48,  2.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.727:   1%|▎                                        | 6/814 [00:13<26:44,  1.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.292:   1%|▎                                        | 6/814 [00:15<26:44,  1.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.292:   1%|▎                                        | 6/814 [00:15<26:44,  1.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.292:   1%|▎                                        | 7/814 [00:15<25:58,  1.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.644:   1%|▎                                        | 7/814 [00:17<25:58,  1.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.644:   1%|▎                                        | 7/814 [00:17<25:58,  1.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.644:   1%|▍                                        | 8/814 [00:17<25:33,  1.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.989:   1%|▍                                        | 8/814 [00:19<25:33,  1.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.989:   1%|▍                                        | 8/814 [00:19<25:33,  1.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.989:   1%|▍                                        | 9/814 [00:19<25:10,  1.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.253:   1%|▍                                        | 9/814 [00:20<25:10,  1.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.253:   1%|▍                                        | 9/814 [00:20<25:10,  1.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.253:   1%|▍                                       | 10/814 [00:20<24:50,  1.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.376:   1%|▍                                       | 10/814 [00:22<24:50,  1.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.376:   1%|▍                                       | 10/814 [00:22<24:50,  1.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.376:   1%|▌                                       | 11/814 [00:22<24:39,  1.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.643:   1%|▌                                       | 11/814 [00:24<24:39,  1.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.643:   1%|▌                                       | 11/814 [00:24<24:39,  1.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.643:   1%|▌                                       | 12/814 [00:24<24:43,  1.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.516:   1%|▌                                       | 12/814 [00:26<24:43,  1.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.516:   1%|▌                                       | 12/814 [00:26<24:43,  1.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.516:   2%|▋                                       | 13/814 [00:26<24:36,  1.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.434:   2%|▋                                       | 13/814 [00:27<24:36,  1.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.434:   2%|▋                                       | 13/814 [00:27<24:36,  1.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.434:   2%|▋                                       | 14/814 [00:27<23:02,  1.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.359:   2%|▋                                       | 14/814 [00:29<23:02,  1.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.359:   2%|▋                                       | 14/814 [00:29<23:02,  1.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.359:   2%|▋                                       | 15/814 [00:29<23:23,  1.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.085:   2%|▋                                       | 15/814 [00:31<23:23,  1.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.085:   2%|▋                                       | 15/814 [00:31<23:23,  1.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.085:   2%|▊                                       | 16/814 [00:31<23:39,  1.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.129:   2%|▊                                       | 16/814 [00:33<23:39,  1.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.129:   2%|▊                                       | 16/814 [00:33<23:39,  1.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.129:   2%|▊                                       | 17/814 [00:33<23:42,  1.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.300:   2%|▊                                       | 17/814 [00:34<23:42,  1.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.300:   2%|▊                                       | 17/814 [00:34<23:42,  1.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.300:   2%|▉                                       | 18/814 [00:34<22:19,  1.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.198:   2%|▉                                       | 18/814 [00:35<22:19,  1.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.198:   2%|▉                                       | 18/814 [00:35<22:19,  1.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.198:   2%|▉                                       | 19/814 [00:35<20:00,  1.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.124:   2%|▉                                       | 19/814 [00:37<20:00,  1.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.124:   2%|▉                                       | 19/814 [00:37<20:00,  1.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 7.124:   2%|▉                                       | 20/814 [00:37<21:26,  1.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.853:   2%|▉                                       | 20/814 [00:39<21:26,  1.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.853:   2%|▉                                       | 20/814 [00:39<21:26,  1.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.853:   3%|█                                       | 21/814 [00:39<19:45,  1.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.752:   3%|█                                       | 21/814 [00:40<19:45,  1.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.752:   3%|█                                       | 21/814 [00:40<19:45,  1.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.752:   3%|█                                       | 22/814 [00:40<18:55,  1.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.424:   3%|█                                       | 22/814 [00:41<18:55,  1.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.424:   3%|█                                       | 22/814 [00:41<18:55,  1.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 6.424:   3%|█▏                                      | 23/814 [00:41<17:52,  1.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.977:   3%|█▏                                      | 23/814 [00:42<17:52,  1.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.977:   3%|█▏                                      | 23/814 [00:42<17:52,  1.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.977:   3%|█▏                                      | 24/814 [00:42<17:35,  1.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.636:   3%|█▏                                      | 24/814 [00:44<17:35,  1.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.636:   3%|█▏                                      | 24/814 [00:44<17:35,  1.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.636:   3%|█▏                                      | 25/814 [00:44<17:17,  1.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.318:   3%|█▏                                      | 25/814 [00:45<17:17,  1.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.318:   3%|█▏                                      | 25/814 [00:45<17:17,  1.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 5.318:   3%|█▎                                      | 26/814 [00:45<19:18,  1.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.916:   3%|█▎                                      | 26/814 [00:47<19:18,  1.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.916:   3%|█▎                                      | 26/814 [00:47<19:18,  1.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.916:   3%|█▎                                      | 27/814 [00:47<18:34,  1.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.562:   3%|█▎                                      | 27/814 [00:48<18:34,  1.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.562:   3%|█▎                                      | 27/814 [00:48<18:34,  1.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.562:   3%|█▍                                      | 28/814 [00:48<17:09,  1.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.262:   3%|█▍                                      | 28/814 [00:49<17:09,  1.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.262:   3%|█▍                                      | 28/814 [00:49<17:09,  1.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 4.262:   4%|█▍                                      | 29/814 [00:49<16:16,  1.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.957:   4%|█▍                                      | 29/814 [00:50<16:16,  1.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.957:   4%|█▍                                      | 29/814 [00:50<16:16,  1.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.957:   4%|█▍                                      | 30/814 [00:50<15:49,  1.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.677:   4%|█▍                                      | 30/814 [00:51<15:49,  1.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.677:   4%|█▍                                      | 30/814 [00:51<15:49,  1.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.677:   4%|█▌                                      | 31/814 [00:51<15:40,  1.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.448:   4%|█▌                                      | 31/814 [00:53<15:40,  1.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.448:   4%|█▌                                      | 31/814 [00:53<15:40,  1.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.448:   4%|█▌                                      | 32/814 [00:53<18:12,  1.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.203:   4%|█▌                                      | 32/814 [00:55<18:12,  1.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.203:   4%|█▌                                      | 32/814 [00:55<18:12,  1.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 3.203:   4%|█▌                                      | 33/814 [00:55<19:54,  1.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.964:   4%|█▌                                      | 33/814 [00:57<19:54,  1.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.964:   4%|█▌                                      | 33/814 [00:57<19:54,  1.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.964:   4%|█▋                                      | 34/814 [00:57<21:01,  1.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.757:   4%|█▋                                      | 34/814 [00:59<21:01,  1.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.757:   4%|█▋                                      | 34/814 [00:59<21:01,  1.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.757:   4%|█▋                                      | 35/814 [00:59<21:50,  1.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.585:   4%|█▋                                      | 35/814 [01:00<21:50,  1.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.585:   4%|█▋                                      | 35/814 [01:00<21:50,  1.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.585:   4%|█▊                                      | 36/814 [01:00<22:25,  1.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 2.411:   4%|█▊                                      | 36/814 [01:02<22:25,  1.73s/it]"
     ]
    }
   ],
   "source": [
    "def turn_position_learning(on):\n",
    "    for x in all_position_weight:\n",
    "        x.requires_grad = on\n",
    "\n",
    "\n",
    "loss_running_score = 0\n",
    "correct_running_score = 0\n",
    "conform_running_score = 0\n",
    "count = 0\n",
    "extra_info = \"\"\n",
    "res_tokens = []\n",
    "accumulate = 10\n",
    "optimizer.zero_grad()\n",
    "set_seed(42)\n",
    "turn_position = False\n",
    "turn_position_learning(False)\n",
    "for learn_pos in range(5):\n",
    "    pbar = tqdm(loi_dataloader)\n",
    "    for wrapper in pbar:\n",
    "        count += 1\n",
    "        result = model(\n",
    "            input_ids=wrapper[\"input_ids\"].to(DEVICE),\n",
    "            attention_mask=wrapper['attention_mask'].to(DEVICE),\n",
    "            labels=wrapper['label_ids'].to(DEVICE),\n",
    "            decoder_attention_mask=wrapper[\"all_decoder_attention_masks\"].to(\n",
    "                DEVICE),  # output_attentions=True\n",
    "        )\n",
    "        loss = result.loss\n",
    "        loss_running_score = loss_running_score * 0.9 + loss.item() * 0.1\n",
    "        if loss != 0:\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            if count % 100 == 0:\n",
    "                extra_info = run_model(dataset_test[0][0][0])\n",
    "            pbar.set_description_str(f\"Loss: {loss_running_score:.3f}\")\n",
    "            pbar.set_postfix_str(extra_info)\n",
    "    if learn_pos>1:\n",
    "        evaluate()\n",
    "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (learn_pos, before_lr, after_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605cb83b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:52:18.154091Z",
     "iopub.status.busy": "2024-04-05T04:52:18.153625Z",
     "iopub.status.idle": "2024-04-05T04:52:23.163957Z",
     "shell.execute_reply": "2024-04-05T04:52:23.163539Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"loi_model_{DATA_NAME_SINGLE}.pkl\", from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6298b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Measure resilient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba2500",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:54:38.127143Z",
     "iopub.status.busy": "2024-04-05T04:54:38.126677Z",
     "iopub.status.idle": "2024-04-05T04:54:38.128283Z",
     "shell.execute_reply": "2024-04-05T04:54:38.128653Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def measure_unalike(arr):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    return 1 - ((arr / n)**2).sum()\n",
    "\n",
    "\n",
    "measure_unalike([\"a\", \"a\", \"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1ed48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:54:38.131867Z",
     "iopub.status.busy": "2024-04-05T04:54:38.131396Z",
     "iopub.status.idle": "2024-04-05T04:54:38.132776Z",
     "shell.execute_reply": "2024-04-05T04:54:38.132388Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for data in [dataset_test]:\n",
    "#     count = 0\n",
    "#     count1 = 0\n",
    "#     count2 = 0\n",
    "#     count10 = 0\n",
    "#     total = 0\n",
    "#     question_index = range(5)\n",
    "#     pbar1 = tqdm(question_index)\n",
    "#     unalike = []\n",
    "#     for ques1 in pbar1:\n",
    "#         answer_set = []\n",
    "#         for m in trange(24):\n",
    "#             ques = ques1 * 24 + m\n",
    "#             question = data[ques][0]\n",
    "#             key = data[ques][1]\n",
    "#             question_convert = check(question)\n",
    "#             if question_convert is None:\n",
    "#                 continue\n",
    "#             total += 1\n",
    "#             answer, _, _, _ = get_model_forward(question_convert.to(DEVICE),\n",
    "#                                                 model=model2)\n",
    "#             answer_set.append(answer)\n",
    "#         unalike.append(measure_unalike(answer_set))\n",
    "# print(f\"Mean unalikeability: {sum(unalike)/len(unalike)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e235a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:54:38.135719Z",
     "iopub.status.busy": "2024-04-05T04:54:38.135249Z",
     "iopub.status.idle": "2024-04-05T04:54:38.137093Z",
     "shell.execute_reply": "2024-04-05T04:54:38.136683Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pbar = trange(0, len(dataset_train), 24)\n",
    "# loss_score = 0\n",
    "# count = 0\n",
    "# extra_info = \"\"\n",
    "# set_seed(42)\n",
    "# res_tokens=[]\n",
    "# for learn_pos in range(10):\n",
    "#     for step in pbar:\n",
    "#         count += 1\n",
    "#         # if count>20:\n",
    "#         #     break\n",
    "#         # print(textwrap.fill(dataset_train[0][0]))\n",
    "#         input_tokens = check(dataset_train[step][0])\n",
    "#         if input_tokens is None:\n",
    "#             continue\n",
    "#         labels = tokenizer.encode(dataset_train[step][1], return_tensors=\"pt\")\n",
    "#         result = model(input_ids=input_tokens.to(DEVICE), labels=shape(labels).to(DEVICE))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss =loss_fn(result.logits[0][learn_pos],labels[0][learn_pos].to(DEVICE))\n",
    "#         loss_score = loss_score * 0.9 + loss.item() * 0.1\n",
    "#         if loss.item()!=0:\n",
    "#             loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # scheduler.step()\n",
    "#         with torch.no_grad():\n",
    "#             if count % 10 == 0:\n",
    "#                 extra_info, res_tokens = get_model_forward(check(dataset_test[0][0]).to(DEVICE))\n",
    "#             pbar.set_description_str(f\"Loss: {loss_score:.2f}\")\n",
    "#             pbar.set_postfix_str(res_tokens[:learn_pos+2])\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5a4d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:54:38.139460Z",
     "iopub.status.busy": "2024-04-05T04:54:38.139015Z",
     "iopub.status.idle": "2024-04-05T04:54:38.140562Z",
     "shell.execute_reply": "2024-04-05T04:54:38.140933Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ListDataset(Dataset):\n",
    "#     def __init__(self, li):\n",
    "#         super(ListDataset, self).__init__()\n",
    "#         self.li=li\n",
    "#     def __getitem__(self, index):\n",
    "#         x,y=self.li[index]\n",
    "#         return x,y\n",
    "#     def __len__(self):\n",
    "#         return len(self.li)\n",
    "# test_loader=DataLoader(ListDataset(dataset_test), batch_size=10, shuffle=True)\n",
    "# for x,y in test_loader:\n",
    "#     print(x,y)\n",
    "#     break\n",
    "# count=0\n",
    "# pbar=tqdm(test_loader)\n",
    "# for question,key in pbar:\n",
    "#     answer = run_model(question)\n",
    "#     count+=sum([answer[x]==key[x] for x in range(len(answer))])\n",
    "#     if key == answer:\n",
    "#         count += 1\n",
    "#     pbar.set_postfix_str(f'{count}')\n",
    "# print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "1_7_0_filled_0_and_fixed_max_length.ipynb",
   "output_path": "loi_qasc.ipynb",
   "parameters": {
    "BATCH_SIZE": 10,
    "DATA_NAME_SINGLE": "qasc",
    "model_name": "google-t5/t5-large"
   },
   "start_time": "2024-04-06T18:05:40.364589",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}