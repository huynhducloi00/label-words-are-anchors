{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "model_name = 'google-t5/t5-large'\n",
    "max_answer_length = 300\n",
    "BATCH_SIZE = 5\n",
    "accumulate_step = None\n",
    "DATA_NAME_SINGLE = 'arc_ir_combine'\n",
    "DATABASE_TEST_SINGLE = 'arc_ir_easy'\n",
    "NUM_EPOCHS = 1\n",
    "VISIBLE_DEVICE ='0,1,2,3,4,5' #','.join([str(x) for x in range(torch.cuda.device_count())])\n",
    "DEVICE = 0\n",
    "STORE_MODE = 'save_model'#loss_only' #save_model'  # loss_only\n",
    "SET_MODE = 'new'\n",
    "EVALUATE = True\n",
    "USE_16_bit = False\n",
    "MAX_MEMORY = None  # max_memory="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loi params: google-t5/t5-large#####300#####batch_size=5;accumulate_step=None\n"
     ]
    }
   ],
   "source": [
    "if not DATABASE_TEST_SINGLE:\n",
    "    DATABASE_TEST_SINGLE = DATA_NAME_SINGLE\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(VISIBLE_DEVICE)  # cannot work\n",
    "print(f\"Loi params: {model_name}{'#'*5}{max_answer_length}{'#'*5}batch_size={BATCH_SIZE};accumulate_step={accumulate_step}\")\n",
    "DATABASE_NAME = [DATA_NAME_SINGLE]  # ,'arc_hard']#,'race','mctest',]\n",
    "DATABASE_TESTS = [DATABASE_TEST_SINGLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Adafactor\n",
    "from functools import wraps, partial\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "from random_utils import set_seed\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldh0033@auburn.edu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"loi_with_padding_just_same_answer_len_70.pkl\"\n",
    "\n",
    "# \"loi_with_padding_1.pkl\"#\n",
    "# model_name = (\n",
    "#     \"allenai/unifiedqa-v2-t5-large-1363200\"  # you can specify the model size here\n",
    "# )\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "kwargs = {}\n",
    "if USE_16_bit:\n",
    "    kwargs['torch_dtype'] = torch.float16\n",
    "kwargs['device_map'] = 'auto'\n",
    "if MAX_MEMORY:\n",
    "    max_memory = {0: \"5GB\", 1: \"5GB\", 2: \"5GB\", \"cpu\": \"504GB\"}\n",
    "model_original = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name, **kwargs)  # 'auto')\n",
    "model = model_original\n",
    "# model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice max: 152 min:1 avg:24.30 len: 13478\n",
      "Total max: 2974 min:437 avg:1398.27 len: 3370\n",
      " max: 136 \n",
      "            min:1\n",
      "            avg:22.280181033575413\n",
      "            len: 9501\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_test = [[] for x in DATABASE_TESTS]\n",
    "dataset_train = []\n",
    "moi_index = []\n",
    "if EVALUATE:\n",
    "    for i, dataname in enumerate(DATABASE_TESTS):\n",
    "        dataset_test[i] = pickle.load(\n",
    "            open(f\"multiple_choice_datasets/{dataname}_test.pkl\", \"rb\"))\n",
    "        # print(dataset_test[i])\n",
    "\n",
    "for i, dataname in enumerate(DATABASE_NAME):\n",
    "    moi_index.append(len(dataset_train))\n",
    "    dataset_train.extend(pickle.load(\n",
    "        open(f\"multiple_choice_datasets/{dataname}_train.pkl\", \"rb\")))\n",
    "datas = dataset_train\n",
    "def ques_choice_split(ques):\n",
    "    spl=ques.split(' ( ) ')\n",
    "    return spl[1:]\n",
    "combine_all_choices_train=sum([ques_choice_split(x[0]) for x in datas],[])\n",
    "combine_all_total=[x[0] for x in datas]\n",
    "print(f\"\"\"Choice max: {max([len(x) for x in combine_all_choices_train])} min:{min([len(x) for x in combine_all_choices_train])} avg:{sum([len(x) for x in combine_all_choices_train])/len(combine_all_choices_train):.2f} len: {len(combine_all_choices_train)}\"\"\")\n",
    "print(f\"\"\"Total max: {max([len(x) for x in combine_all_total])} min:{min([len(x) for x in combine_all_total])} avg:{sum([len(x) for x in combine_all_total])/len(combine_all_total):.2f} len: {len(combine_all_total)}\"\"\")\n",
    "\n",
    "if EVALUATE:\n",
    "    datas = dataset_test[0]\n",
    "    combine_all_choices_test=sum([ques_choice_split(x[0]) for x in datas],[])\n",
    "    print(f\"\"\" max: {max([len(x) for x in combine_all_choices_test])} \n",
    "            min:{min([len(x) for x in combine_all_choices_test])}\n",
    "            avg:{sum([len(x) for x in combine_all_choices_test])/len(combine_all_choices_test)}\n",
    "            len: {len(combine_all_choices_test)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Clue: The largest contributor to excess carbon dioxide in the atmosphere is from the burning of fossil fuels. Carbon dioxide from burning fossil fuels is a principal greenhouse gas that scientists believe is causing a warming of the earth as it accumulates in the atmosphere. Carbon dioxide is removed from the Earth's atmosphere by plant photosynthesis, the weathering of rocks, and reactions with rain or surface waters to form carbonic acid and carbonate. As part of the process of photosynthesis, plants remove carbon dioxide from the atmosphere. Plant photosynthesis removes carbon dioxide gas from the atmosphere. Plants remove carbon dioxide from the atmosphere during photosynthesis. Plants remove carbon in the form of carbon dioxide from the atmosphere during the process of photosynthesis, but release some carbon dioxide back into the atmosphere during normal respiration. Rain, as it falls to earth, collects carbon dioxide from the atmosphere, and as it soaks into the ground, it collects more carbon dioxide from decaying organic matter. Carbon dioxide emitted by the burning of fossil fuels can warm the earth's atmosphere. During photosynthesis, plants absorb carbon dioxide from Earth's atmosphere. Question: Carbon dioxide is removed from Earth's atmosphere by \\\\n ( ) animal respiration ( ) decaying organisms ( ) plant photosynthesis ( ) burning fossil fuels\",\n",
       " \"Clue: The residue from carbon dioxide removal is a gas that if not contained will escape back into the atmosphere. Plant photosynthesis removes carbon dioxide gas from the atmosphere. and lithium hydroxide (LiOH) filter cannisters used to remove carbon dioxide from the spacecraft from the spacecraft cabin atmosphere. These data are used to understand the role of oceans in removing carbon dioxide from the atmosphere, the ocean s productivity, and as a bonus, the spacecraft is providing valuable images of the land. c. Atmospheric air, which provides the carbon and oxygen from carbon dioxide. The system produces carbon dioxide, but a filter removes the carbon dioxide gas, preventing its release to the atmosphere (carbon dioxide is a greenhouse gas, which raises the temperature of the atmosphere by trapping heat radiated from the earth's surface). Plants remove carbon dioxide from the atmosphere, produce oxygen, and provide shade. Lithium hydroxide (LiOH) is used to remove carbon dioxide from the atmosphere of spacecraft. Trees remove carbon dioxide from the atmosphere, produce oxygen, and provide shelter. LESSON PLAN Topic Decomposition of water by electrolysis Rationale The Space Shuttle s cabin atmosphere is generated from the release of oxygen and nitrogen gas which is stored in a liquid state in tanks located in the fuselage of the spacecraft. Question: As humans travel in space, which gas is provided in the atmosphere of the spacecraft and which gas is removed from the atmosphere of the spacecraft? \\\\n ( ) Oxygen is provided. Carbon dioxide is removed ( ) Carbon dioxide is provided. Oxygen is removed ( ) Carbon dioxide is provided. Nitrogen is removed ( ) Oxygen is provided. Nitrogen is removed\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in combine_all_total if 'Carbon dioxide is removed' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook(hook_before, oldfunc, hook_after):\n",
    "\n",
    "    def foo(*args, **kwargs):\n",
    "        hook_before(*args, **kwargs)\n",
    "        aa = oldfunc(*args, **kwargs)\n",
    "        hook_after(*args, **kwargs)\n",
    "        return aa\n",
    "\n",
    "    return foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_compute_bias(self, query_length, key_length, device=None):\n",
    "    \"\"\"Compute binned relative position bias\"\"\"\n",
    "    if device is None:\n",
    "        device = self.relative_attention_bias.weight.device\n",
    "    context_position = torch.arange(query_length, dtype=torch.long, device=device)[\n",
    "        :, None\n",
    "    ]\n",
    "    memory_position = torch.arange(\n",
    "        key_length, dtype=torch.long, device=device)[None, :]\n",
    "\n",
    "    relative_position = (\n",
    "        memory_position - context_position\n",
    "    )  # shape (query_length, key_length)\n",
    "    if self.is_decoder:\n",
    "        values = values.permute([2, 0, 1]).unsqueeze(\n",
    "            0\n",
    "        )  # shape (1, num_heads, query_length, key_length)\n",
    "        return values\n",
    "\n",
    "    anchors = self.anchor_array\n",
    "    values = []\n",
    "    for anchor in anchors:\n",
    "        mot = [[anchor[idx], anchor[idx+1]] for idx in range(len(anchor)-1)]\n",
    "        max_answer_length = max([x[1] - x[0] for x in mot])\n",
    "        # print(a, b, c, d, max_answer_length)\n",
    "        context_position_new = context_position.clone()\n",
    "        for i in range(1, len(mot)):\n",
    "            context_position_new[mot[i][0]: mot[i][1]\n",
    "                                 ] = context_position_new[mot[i][0]: mot[i][1]] - mot[0][0]\n",
    "        context_position_new[-1] = mot[0][0] + 2 * max_answer_length\n",
    "        memory_position_new = context_position_new.clone().view(1, -1)\n",
    "        relative_position = (\n",
    "            memory_position_new - context_position_new\n",
    "        )  # shape (query_length, key_length)\n",
    "        for i in range(len(mot)):\n",
    "            for j in range(len(mot)):\n",
    "                if i != j:\n",
    "                    x = mot[i]\n",
    "                    y = mot[j]\n",
    "                    relative_position[x[0]: x[1], y[0]\n",
    "                        : y[1]] += max_answer_length\n",
    "        relative_position_bucket = self._relative_position_bucket(\n",
    "            relative_position,  # shape (query_length, key_length)\n",
    "            bidirectional=(not self.is_decoder),\n",
    "            num_buckets=self.relative_attention_num_buckets,\n",
    "            max_distance=self.relative_attention_max_distance,\n",
    "        )\n",
    "        value = self.relative_attention_bias(relative_position_bucket)\n",
    "        values.append(value)\n",
    "    values = torch.stack(values)  # shape [1, 91, 91, 16]\n",
    "    values = values.permute(\n",
    "        [0, 3, 1, 2]\n",
    "    )  # shape (batch size, num_heads, query_length, key_length)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clue: Yellow fever or \"breakbone\" fever, a severe flu-like viral\n",
      "illness that can cause fatal internal bleeding, is also spreading. It\n",
      "is also the most common cause of bacterial bloodstream infections in\n",
      "children. bacterial meningitis Causes, incidence, and risk factors The\n",
      "most common causes of meningitis are bacterial infections that start\n",
      "in other parts of the body and spread to the brain or spinal cord via\n",
      "the bloodstream. Prof Hughes said there was a definite risk of\n",
      "developing GBS following campylobacter infection, as well as others,\n",
      "mostly viral, such as a virus which caused a glandular fever like\n",
      "illness, known as cytomegalovirus. Causes, incidence, and risk\n",
      "factors: The most common causes of meningitis are bacterial infections\n",
      "that start in other parts of the body and spread to the brain or\n",
      "spinal cord via the bloodstream. the next most common cause is a\n",
      "bacterial infection of the bloodstream (bacterial sepsis). Exercise\n",
      "develops muscle, which requires fuel for contraction and relaxation.\n",
      "But Erasistratus himself agrees that human beings digest badly in\n",
      "fevers, adding as the cause that the activity of the stomach has been\n",
      "impaired. Yellow Fever This viral disease is characterised by a severe\n",
      "flu-like illness in which a bleeding tendency and jaundice may\n",
      "develop. Diphtheria is an acute (suddenly developing) bacterial\n",
      "illness which causes a sore throat and fever, and can be life-\n",
      "threatening when the bacteria release a toxin into the bloodstream.\n",
      "Question: Which factor will most likely cause a person to develop a\n",
      "fever? \\n ( ) a leg muscle relaxing after exercise ( ) a bacterial\n",
      "population in the bloodstream ( ) several viral particles on the skin\n",
      "( ) carbohydrates being digested in the stomach\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import textwrap\n",
    "\n",
    "\n",
    "extra_dim_learning = []\n",
    "\n",
    "\n",
    "def set_mode(MODE):\n",
    "    itself = model.encoder.block[0].layer[0].SelfAttention\n",
    "    if MODE == \"new\":\n",
    "        # itself.forward = partial(modified_self_attention_forward, itself)\n",
    "        itself.compute_bias = partial(new_compute_bias, itself)\n",
    "        model.forward = hook(input_before_hooker, partial(\n",
    "            T5ForConditionalGeneration.forward, model), input_after_hooker)\n",
    "\n",
    "    else:\n",
    "        # itself.forward = partial(\n",
    "        #     model.encoder.block[0].layer[0].SelfAttention.__class__.forward, itself\n",
    "        # )\n",
    "        itself.compute_bias = partial(\n",
    "            model.encoder.block[0].layer[0].SelfAttention.__class__.compute_bias, itself\n",
    "        )\n",
    "        model.forward = partial(T5ForConditionalGeneration.forward, model)\n",
    "\n",
    "\n",
    "def check_encoded(all_input_ids):\n",
    "    anchors = []\n",
    "    for input_ids in all_input_ids:\n",
    "        # print('\\n'.join([f'{x.item()},{y}' for x,y in zip(input_ids, tokens)][50:]))\n",
    "        original = input_ids.tolist()\n",
    "        anchor = []\n",
    "        for i in range(len(input_ids)):\n",
    "            if (\n",
    "                i < len(input_ids) - 2\n",
    "                and input_ids[i] == 41\n",
    "                and input_ids[i + 1] == 3\n",
    "                and input_ids[i + 2] == 61\n",
    "            ) or original[i] == 1:\n",
    "                anchor.append(i)\n",
    "        anchors.append(anchor)\n",
    "    return anchors\n",
    "\n",
    "\n",
    "def input_before_hooker(*args, **kwargs):\n",
    "    input_ids = kwargs[\"input_ids\"]\n",
    "    # print('old ',input_ids)\n",
    "    anchors = check_encoded(input_ids)\n",
    "    final_inputs = []\n",
    "    for input_id, anchor in zip(input_ids, anchors):\n",
    "        input_id = input_id.tolist()\n",
    "\n",
    "        real_max_len = max([anchor[idx+1] - anchor[idx]\n",
    "                           for idx in range(len(anchor)-1)])\n",
    "        if real_max_len > max_answer_length:\n",
    "            print(f'ALERT: MAX LENGTH IS {real_max_len}')\n",
    "        for x in reversed(range(1, len(anchor))):\n",
    "            if anchor[x] - anchor[x - 1] < max_answer_length:\n",
    "                [\n",
    "                    input_id.insert(anchor[x], 0)\n",
    "                    for _ in range(max_answer_length - (anchor[x] - anchor[x - 1]))\n",
    "                ]\n",
    "\n",
    "        final_inputs.append(input_id)\n",
    "\n",
    "    max_length = max([len(input) for input in final_inputs])\n",
    "    mask = [[1]*max_length]*len(final_inputs)\n",
    "    for idx, input in enumerate(final_inputs):\n",
    "        for x in range(max_length):\n",
    "            if x >= len(input):\n",
    "                mask[idx][x] = 0\n",
    "        for x in range(max_length-len(input)):\n",
    "            input.append(0)\n",
    "    kwargs[\"input_ids\"] = torch.tensor(final_inputs).to(input_ids.device)\n",
    "    kwargs['attention_mask'] = torch.tensor(mask).to(input_ids.device)\n",
    "    # print('new ',kwargs[\"input_ids\"])\n",
    "    anchors = check_encoded(kwargs[\"input_ids\"])\n",
    "    model.encoder.block[0].layer[0].SelfAttention.anchor_array = anchors\n",
    "\n",
    "\n",
    "def input_after_hooker(*args, **kwargs):\n",
    "    model.encoder.block[0].layer[0].SelfAttention.anchor_array = None\n",
    "\n",
    "\n",
    "print(textwrap.fill(dataset_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def measure_unalike(arr, print_arr=False):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    if print_arr:\n",
    "        print(arr)\n",
    "    return 1 - ((arr / n)**2).sum()\n",
    "\n",
    "\n",
    "question_to_do = 5\n",
    "per_question = 20\n",
    "\n",
    "\n",
    "def get_model_forward(input_ids, attention_mask, model=model):\n",
    "    with torch.no_grad():\n",
    "        start = []\n",
    "        [start.append([0]) for x in range(len(input_ids))]\n",
    "        for k in range(max_answer_length):\n",
    "            # print(torch.tensor(start).shape)\n",
    "            result = model(\n",
    "                input_ids=input_ids.to(DEVICE),\n",
    "                attention_mask=attention_mask.to(DEVICE),\n",
    "                decoder_input_ids=torch.tensor(start).to(DEVICE),\n",
    "                output_attentions=True,\n",
    "            )\n",
    "            item = result.logits.argmax(dim=2)[:, -1]\n",
    "            # print('loi',result.logits.shape, item)\n",
    "            for index in range(len(item)):\n",
    "                start[index].append(item[index].item())\n",
    "            if torch.allclose(item, torch.tensor(1)):\n",
    "                break\n",
    "            #     break\n",
    "    result = []\n",
    "    for batch in start:\n",
    "        y = -1\n",
    "        for index, x in enumerate(batch):\n",
    "            if x == 1:\n",
    "                y = index\n",
    "                break\n",
    "        result.append(batch[:y+1] if y > -1 else batch)\n",
    "    return [tokenizer.decode(x, skip_special_tokens=True) for x in result]\n",
    "\n",
    "\n",
    "def run_model(input_strs):\n",
    "    if input_strs is str:\n",
    "        input_strs = [input_strs]\n",
    "    input_ids_wrapper = tokenizer(\n",
    "        input_strs, padding=True, return_tensors='pt')\n",
    "\n",
    "    answer = get_model_forward(input_ids_wrapper['input_ids'],\n",
    "                               input_ids_wrapper['attention_mask'])\n",
    "    return answer\n",
    "\n",
    "\n",
    "set_mode(SET_MODE)\n",
    "# print(SET_MODE, run_model(dataset_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = [(index, x, y) for index, (x, y) in enumerate(model.named_parameters())\n",
    "      if y.requires_grad == True]\n",
    "[(index, x) for index, x, y in kk if \"decoder\" in x]\n",
    "len(kk)\n",
    "all_position_weight = [\n",
    "    y for index, x, y in kk if (\"extra_dimension_embedding\" in x) or (\n",
    "        (\"encoder\" in x) and (\"relative_attention_bias\" in x))\n",
    "]\n",
    "to_train_model = [y for index, x, y in kk]\n",
    "for x in to_train_model:\n",
    "    x.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = [\n",
    "    (ques, answer, ques.split(\" ( ) \")[1:])\n",
    "    for ques, answer in [\n",
    "        (\n",
    "            dataset_train[x][0],\n",
    "            dataset_train[x][1],\n",
    "        )\n",
    "        for x in range(len(dataset_train))\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(print(idx),CheckTransform.__call__(None, data_array[idx])) for idx,x in enumerate(data_array)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1945125/688003588.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m loi_dataloader = DataLoader(\n\u001b[1;32m     60\u001b[0m     CustomDataset(\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mdata_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mCheckTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     ),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_array' is not defined"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ClueKind(Enum):\n",
    "    HEADER = 1\n",
    "    BODY = 2\n",
    "\n",
    "\n",
    "clue_kind = ClueKind.HEADER\n",
    "\n",
    "\n",
    "class CheckTransform(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # print(f\"'{sample[1]}'\")\n",
    "        try:\n",
    "            all_labels = ([x.split(\". \")[-1] for x in sample[2]]\n",
    "                          if clue_kind == ClueKind.BODY else sample[2])\n",
    "            return {\n",
    "                \"input_ids\": sample[0],\n",
    "                \"label_index\": all_labels.index(sample[1]),\n",
    "                \"all_labels\": all_labels,\n",
    "            }\n",
    "        except:\n",
    "            raise Exception(f\"ERror: {sample}\")\n",
    "            print(\"all answer: \", sample[2])\n",
    "            print(\"answer\", sample[1])\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_array, transform=None):\n",
    "        self.dataset = dataset_array\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.dataset[idx])\n",
    "\n",
    "\n",
    "def collate(datas):\n",
    "    wrapper = tokenizer([x[\"all_labels\"][x[\"label_index\"]] for x in datas],\n",
    "                        padding=True)\n",
    "    wrapper[\"label_ids\"] = torch.tensor(wrapper.pop(\"input_ids\"))\n",
    "    # wrapper[\"label_index\"] = torch.tensor([x[\"label_index\"] for x in datas])\n",
    "    for k in wrapper[\"label_ids\"]:\n",
    "        k[k == tokenizer.pad_token_id] = -100\n",
    "    wrapper[\"all_decoder_attention_masks\"] = torch.tensor(\n",
    "        wrapper.pop(\"attention_mask\"))\n",
    "\n",
    "    for_input = tokenizer([x[\"input_ids\"] for x in datas], padding=True)\n",
    "    wrapper[\"input_ids\"] = torch.tensor(for_input.pop(\"input_ids\"))\n",
    "    wrapper[\"attention_mask\"] = torch.tensor(for_input.pop(\"attention_mask\"))\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "loi_dataloader = DataLoader(\n",
    "    CustomDataset(\n",
    "        data_array,\n",
    "        CheckTransform(),\n",
    "    ),\n",
    "    batch_size=1 if accumulate_step else BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "# for k in loi_dataloader:\n",
    "#     print(k[\"all_label_ids\"])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "# attention 898704\n",
    "# hidden state 242688\n",
    "# classification_layer = nn.Linear(242688, 4).to(DEVICE)\n",
    "# optimizer =AdamW(lr=1e-4)\n",
    "optimizer = Adafactor(\n",
    "    to_train_model,  # + [x for x in classification_layer.parameters()],\n",
    "    relative_step=False,\n",
    "    lr=1e-4,\n",
    ")\n",
    "# lr_schedule=AdafactorSchedule(optimizer)\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    ll = 1\n",
    "    wrong_answers = [[] for x in range(ll)]\n",
    "    got_2 = [[] for x in range(ll)]\n",
    "    got_1 = [[] for x in range(ll)]\n",
    "    answers = [[] for x in range(ll)]\n",
    "    last_str = None\n",
    "    last_acc = None\n",
    "    for ix in range(ll):\n",
    "        print(f'Name {DATABASE_TESTS[ix]}')\n",
    "        # [pickle.load(open(f\"multiple_choice_datasets/obqa_fact_test.pkl\", \"rb\"))]:\n",
    "        # print(f\"test {data==dataset_test}\")\n",
    "        count = 0\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "        count10 = 0\n",
    "        total = 0\n",
    "        data = dataset_test[ix]\n",
    "        pbar1 = trange(len(data))\n",
    "        for ques in pbar1:\n",
    "            question = data[ques][0]\n",
    "            key = data[ques][1]\n",
    "            total += 1\n",
    "            answer = run_model(question)[0]\n",
    "            answers[ix].append(answer)\n",
    "            if key == answer:\n",
    "                count += 1\n",
    "            else:\n",
    "                wrong_answers[ix].append(ques)\n",
    "            if key[0] == answer[0]:\n",
    "                count1 += 1\n",
    "                got_1[ix].append(ques)\n",
    "            if key[:2] == answer[:2]:\n",
    "                count2 += 1\n",
    "                got_2[ix].append(ques)\n",
    "            if answer in question:\n",
    "                count10 += 1\n",
    "            last_str = f\"{count}, {count1}, {count2}, {count10},{total},{count/total*100:.2f},{count10/total*100:.2f}\"\n",
    "            last_acc = f'{count/total*100:.2f}'\n",
    "            pbar1.set_postfix_str(last_str)\n",
    "    return last_str, last_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.156:   1%|          | 3/337 [00:30<53:32,  9.62s/it]  Token indices sequence length is longer than the specified maximum sequence length for this model (931 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loss: 0.156:   1%|          | 3/337 [00:44<1:22:33, 14.83s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 530.00 MiB. GPU 1 has a total capacity of 15.74 GiB of which 18.62 MiB is free. Process 2338689 has 3.41 GiB memory in use. Including non-PyTorch memory, this process has 12.30 GiB memory in use. Of the allocated memory 11.48 GiB is allocated by PyTorch, and 685.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1618031/2492556181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwrapper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         result = model(\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1618031/2496884920.py\u001b[0m in \u001b[0;36mfoo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mhook_before\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moldfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mhook_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1709\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m             \u001b[0;31m# Convert encoder inputs in embeddings if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1711\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1712\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 )\n\u001b[1;32m   1114\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1116\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    696\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    600\u001b[0m     ):\n\u001b[1;32m    601\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# compute scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         scores = torch.matmul(\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 530.00 MiB. GPU 1 has a total capacity of 15.74 GiB of which 18.62 MiB is free. Process 2338689 has 3.41 GiB memory in use. Including non-PyTorch memory, this process has 12.30 GiB memory in use. Of the allocated memory 11.48 GiB is allocated by PyTorch, and 685.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# def turn_position_learning(on):\n",
    "#     for x in all_position_weight:\n",
    "#         x.requires_grad = on\n",
    "\n",
    "\n",
    "loss_running_score = 0\n",
    "correct_running_score = 0\n",
    "conform_running_score = 0\n",
    "count = 0\n",
    "extra_info = \"\"\n",
    "res_tokens = []\n",
    "optimizer.zero_grad()\n",
    "set_seed(42)\n",
    "turn_position = False\n",
    "# turn_position_learning(False)\n",
    "evaluates = []\n",
    "loss_array = []\n",
    "from tqdm import tqdm as tqdm_each\n",
    "for learn_pos in range(NUM_EPOCHS):\n",
    "    pbar = tqdm_each(loi_dataloader)\n",
    "    for wrapper in pbar:\n",
    "        count += 1\n",
    "        result = model(\n",
    "            input_ids=wrapper[\"input_ids\"].to(DEVICE),\n",
    "            attention_mask=wrapper['attention_mask'].to(DEVICE),\n",
    "            labels=wrapper['label_ids'].to(DEVICE),\n",
    "            decoder_attention_mask=wrapper[\"all_decoder_attention_masks\"].to(\n",
    "                DEVICE),  # output_attentions=True\n",
    "        )\n",
    "        loss = result.loss\n",
    "        if accumulate_step:\n",
    "            loss = loss/accumulate_step\n",
    "        loss.backward()\n",
    "        if accumulate_step:\n",
    "            if count % accumulate_step == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss_running_score = loss_running_score * 0.9 + result.loss.item() * 0.1\n",
    "        else:\n",
    "            loss_running_score = loss_running_score * 0.9 + loss.item() * 0.1\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if EVALUATE:\n",
    "            with torch.no_grad():\n",
    "                if count % 100 == 0:\n",
    "                    extra_info = run_model(dataset_test[0][0][0])\n",
    "                pbar.set_description_str(f\"Loss: {loss_running_score:.3f}\")\n",
    "                pbar.set_postfix_str(extra_info)\n",
    "        loss_array.append(loss_running_score)\n",
    "    path_split = model_name.split('/')\n",
    "    prefix = f\"{path_split[0]}/{SET_MODE}_{path_split[1]}_{DATA_NAME_SINGLE}_epoch_{learn_pos}\"\n",
    "    if STORE_MODE == 'loss_only':\n",
    "        pickle.dump(loss_array, open(f\"{prefix}_loss.pkl\", 'wb'))\n",
    "    else:\n",
    "        eval_str = ''\n",
    "        if EVALUATE:\n",
    "            all_evaluate_infos = evaluate()\n",
    "            evaluates.append(\n",
    "                f'{all_evaluate_infos[0]}-Loss:{loss_running_score}')\n",
    "            eval_str = f'_eval_{DATABASE_TEST_SINGLE}_acc_{all_evaluate_infos[1]}'\n",
    "        model.save_pretrained(f\"{prefix}_{eval_str}.pkl\", from_pt=True)\n",
    "    # before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    # after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    # print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (learn_pos, before_lr, after_lr))\n",
    "print(evaluates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google-t5/new_t5-large_qasc_epoch_2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shared.weight', True),\n",
       " ('encoder.block.0.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.0.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.0.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.0.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
       "  True),\n",
       " ('encoder.block.0.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.0.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.0.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.0.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.1.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.1.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.1.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.1.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.1.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.1.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.1.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.1.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.2.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.2.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.2.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.2.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.2.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.2.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.2.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.2.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.3.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.3.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.3.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.3.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.3.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.3.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.3.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.3.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.4.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.4.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.4.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.4.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.4.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.4.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.4.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.4.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.5.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.5.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.5.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.5.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.5.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.5.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.5.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.5.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.6.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.6.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.6.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.6.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.6.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.6.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.6.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.6.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.7.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.7.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.7.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.7.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.7.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.7.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.7.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.7.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.8.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.8.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.8.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.8.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.8.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.8.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.8.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.8.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.9.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.9.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.9.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.9.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.9.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.9.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.9.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.9.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.10.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.10.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.10.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.10.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.10.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.10.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.10.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.10.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.11.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.11.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.11.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.11.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.11.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.11.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.11.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.11.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.12.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.12.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.12.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.12.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.12.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.12.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.12.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.12.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.13.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.13.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.13.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.13.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.13.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.13.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.13.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.13.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.14.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.14.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.14.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.14.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.14.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.14.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.14.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.14.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.15.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.15.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.15.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.15.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.15.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.15.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.15.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.15.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.16.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.16.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.16.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.16.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.16.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.16.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.16.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.16.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.17.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.17.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.17.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.17.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.17.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.17.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.17.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.17.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.18.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.18.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.18.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.18.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.18.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.18.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.18.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.18.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.19.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.19.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.19.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.19.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.19.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.19.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.19.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.19.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.20.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.20.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.20.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.20.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.20.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.20.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.20.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.20.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.21.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.21.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.21.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.21.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.21.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.21.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.21.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.21.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.22.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.22.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.22.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.22.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.22.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.22.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.22.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.22.layer.1.layer_norm.weight', True),\n",
       " ('encoder.block.23.layer.0.SelfAttention.q.weight', True),\n",
       " ('encoder.block.23.layer.0.SelfAttention.k.weight', True),\n",
       " ('encoder.block.23.layer.0.SelfAttention.v.weight', True),\n",
       " ('encoder.block.23.layer.0.SelfAttention.o.weight', True),\n",
       " ('encoder.block.23.layer.0.layer_norm.weight', True),\n",
       " ('encoder.block.23.layer.1.DenseReluDense.wi.weight', True),\n",
       " ('encoder.block.23.layer.1.DenseReluDense.wo.weight', True),\n",
       " ('encoder.block.23.layer.1.layer_norm.weight', True),\n",
       " ('encoder.final_layer_norm.weight', True),\n",
       " ('decoder.block.0.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.0.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.0.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.0.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
       "  True),\n",
       " ('decoder.block.0.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.0.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.0.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.0.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.0.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.0.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.0.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.0.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.0.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.1.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.1.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.1.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.1.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.1.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.1.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.1.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.1.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.1.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.1.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.1.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.1.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.1.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.2.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.2.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.2.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.2.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.2.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.2.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.2.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.2.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.2.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.2.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.2.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.2.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.2.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.3.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.3.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.3.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.3.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.3.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.3.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.3.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.3.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.3.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.3.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.3.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.3.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.3.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.4.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.4.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.4.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.4.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.4.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.4.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.4.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.4.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.4.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.4.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.4.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.4.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.4.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.5.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.5.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.5.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.5.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.5.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.5.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.5.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.5.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.5.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.5.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.5.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.5.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.5.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.6.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.6.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.6.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.6.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.6.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.6.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.6.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.6.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.6.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.6.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.6.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.6.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.6.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.7.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.7.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.7.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.7.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.7.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.7.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.7.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.7.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.7.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.7.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.7.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.7.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.7.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.8.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.8.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.8.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.8.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.8.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.8.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.8.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.8.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.8.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.8.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.8.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.8.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.8.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.9.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.9.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.9.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.9.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.9.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.9.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.9.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.9.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.9.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.9.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.9.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.9.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.9.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.10.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.10.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.10.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.10.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.10.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.10.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.10.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.10.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.10.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.10.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.10.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.10.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.10.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.11.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.11.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.11.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.11.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.11.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.11.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.11.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.11.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.11.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.11.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.11.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.11.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.11.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.12.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.12.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.12.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.12.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.12.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.12.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.12.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.12.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.12.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.12.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.12.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.12.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.12.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.13.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.13.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.13.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.13.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.13.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.13.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.13.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.13.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.13.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.13.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.13.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.13.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.13.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.14.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.14.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.14.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.14.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.14.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.14.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.14.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.14.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.14.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.14.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.14.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.14.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.14.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.15.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.15.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.15.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.15.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.15.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.15.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.15.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.15.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.15.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.15.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.15.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.15.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.15.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.16.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.16.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.16.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.16.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.16.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.16.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.16.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.16.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.16.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.16.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.16.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.16.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.16.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.17.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.17.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.17.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.17.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.17.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.17.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.17.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.17.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.17.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.17.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.17.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.17.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.17.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.18.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.18.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.18.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.18.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.18.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.18.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.18.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.18.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.18.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.18.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.18.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.18.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.18.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.19.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.19.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.19.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.19.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.19.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.19.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.19.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.19.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.19.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.19.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.19.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.19.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.19.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.20.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.20.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.20.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.20.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.20.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.20.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.20.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.20.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.20.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.20.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.20.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.20.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.20.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.21.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.21.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.21.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.21.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.21.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.21.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.21.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.21.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.21.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.21.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.21.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.21.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.21.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.22.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.22.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.22.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.22.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.22.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.22.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.22.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.22.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.22.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.22.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.22.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.22.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.22.layer.2.layer_norm.weight', True),\n",
       " ('decoder.block.23.layer.0.SelfAttention.q.weight', True),\n",
       " ('decoder.block.23.layer.0.SelfAttention.k.weight', True),\n",
       " ('decoder.block.23.layer.0.SelfAttention.v.weight', True),\n",
       " ('decoder.block.23.layer.0.SelfAttention.o.weight', True),\n",
       " ('decoder.block.23.layer.0.layer_norm.weight', True),\n",
       " ('decoder.block.23.layer.1.EncDecAttention.q.weight', True),\n",
       " ('decoder.block.23.layer.1.EncDecAttention.k.weight', True),\n",
       " ('decoder.block.23.layer.1.EncDecAttention.v.weight', True),\n",
       " ('decoder.block.23.layer.1.EncDecAttention.o.weight', True),\n",
       " ('decoder.block.23.layer.1.layer_norm.weight', True),\n",
       " ('decoder.block.23.layer.2.DenseReluDense.wi.weight', True),\n",
       " ('decoder.block.23.layer.2.DenseReluDense.wo.weight', True),\n",
       " ('decoder.block.23.layer.2.layer_norm.weight', True),\n",
       " ('decoder.final_layer_norm.weight', True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x[1],x[2].requires_grad) for x in kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=[x.cpu().detach().numpy() for x in model.encoder.block[0].layer[0].SelfAttention.relative_attention_bias.weight]\n",
    "pickle.dump(l1,open(f'Scheme.MC-ED_positional_emb.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.091118"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import linalg\n",
    "linalg.norm(l1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure resilient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def measure_unalike(arr):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    return 1 - ((arr / n)**2).sum()\n",
    "\n",
    "\n",
    "measure_unalike([\"a\", \"a\", \"a\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
