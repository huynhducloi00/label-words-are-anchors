{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:12.237224Z",
     "iopub.status.busy": "2024-04-05T04:32:12.236702Z",
     "iopub.status.idle": "2024-04-05T04:32:13.738314Z",
     "shell.execute_reply": "2024-04-05T04:32:13.737824Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_parent_path = os.getcwd()\n",
    "model_path = 'google-t5/old_t5-large_qasc_epoch_0_.pkl'\n",
    "model_name = None\n",
    "tokenizer_name = \"google-t5/t5-large\"\n",
    "max_answer_length = 300\n",
    "BATCH_SIZE = 10\n",
    "accumulate_step = None\n",
    "DATA_NAME_SINGLE = \"qasc\"\n",
    "NUM_EPOCHS = 1\n",
    "VISIBLE_DEVICE = \"0,1,2\"\n",
    "DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loi params: google-t5/old_t5-large_qasc_epoch_0_.pkl#####300#####batch_size=10;accumulate_step=None\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = VISIBLE_DEVICE  # cannot work\n",
    "print(\n",
    "    f\"Loi params: {model_path}{'#'*5}{max_answer_length}{'#'*5}batch_size={BATCH_SIZE};accumulate_step={accumulate_step}\"\n",
    ")\n",
    "DATABASE_NAME = [DATA_NAME_SINGLE]  # ,'arc_hard']#,'race','mctest',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import trange\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Adafactor\n",
    "from functools import wraps, partial\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from random_utils import set_seed\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:13.742386Z",
     "iopub.status.busy": "2024-04-05T04:32:13.741926Z",
     "iopub.status.idle": "2024-04-05T04:32:17.460195Z",
     "shell.execute_reply": "2024-04-05T04:32:17.459714Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldh0033@auburn.edu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"loi_with_padding_just_same_answer_len_70.pkl\"\n",
    "\n",
    "# \"loi_with_padding_1.pkl\"#\n",
    "# model_name = (\n",
    "#     \"allenai/unifiedqa-v2-t5-large-1363200\"  # you can specify the model size here\n",
    "# )\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "model_original = (\n",
    "    T5ForConditionalGeneration.from_pretrained(\n",
    "        f\"{model_parent_path}/{model_path}\", device_map=\"auto\"\n",
    "    )\n",
    "    if model_path\n",
    "    else T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n",
    ")\n",
    "model = model_original\n",
    "# model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.467081Z",
     "iopub.status.busy": "2024-04-05T04:32:17.466624Z",
     "iopub.status.idle": "2024-04-05T04:32:17.470488Z",
     "shell.execute_reply": "2024-04-05T04:32:17.470851Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_test = []\n",
    "[dataset_test.append([]) for x in DATABASE_NAME]\n",
    "moi_index = []\n",
    "\n",
    "for i, dataname in enumerate(DATABASE_NAME):\n",
    "    dataset_test[i] = pickle.load(\n",
    "        open(f\"multiple_choice_datasets/{dataname}_test.pkl\", \"rb\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate is generally described in terms of local weather conditions.\n",
      "Climate is generally described in terms of what? \\n ( ) sand ( )\n",
      "occurs over a wide range ( ) forests ( ) Global warming ( ) rapid\n",
      "changes occur ( ) local weather conditions ( ) measure of motion ( )\n",
      "city life\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# ques=4\n",
    "print(textwrap.fill(dataset_test[0][0][0]))\n",
    "# print('correct: ',dataset_test[ques][1],'length',len(dataset_test[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.473760Z",
     "iopub.status.busy": "2024-04-05T04:32:17.473285Z",
     "iopub.status.idle": "2024-04-05T04:32:17.475142Z",
     "shell.execute_reply": "2024-04-05T04:32:17.474734Z"
    }
   },
   "outputs": [],
   "source": [
    "def hook(hook_before, oldfunc, hook_after):\n",
    "\n",
    "    def foo(*args, **kwargs):\n",
    "        hook_before(*args, **kwargs)\n",
    "        aa = oldfunc(*args, **kwargs)\n",
    "        hook_after(*args, **kwargs)\n",
    "        return aa\n",
    "\n",
    "    return foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.482036Z",
     "iopub.status.busy": "2024-04-05T04:32:17.481563Z",
     "iopub.status.idle": "2024-04-05T04:32:17.483260Z",
     "shell.execute_reply": "2024-04-05T04:32:17.482900Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_compute_bias(self, query_length, key_length, device=None):\n",
    "    \"\"\"Compute binned relative position bias\"\"\"\n",
    "    if device is None:\n",
    "        device = self.relative_attention_bias.weight.device\n",
    "    context_position = torch.arange(query_length, dtype=torch.long, device=device)[\n",
    "        :, None\n",
    "    ]\n",
    "    memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n",
    "\n",
    "    relative_position = (\n",
    "        memory_position - context_position\n",
    "    )  # shape (query_length, key_length)\n",
    "    if self.is_decoder:\n",
    "        values = values.permute([2, 0, 1]).unsqueeze(\n",
    "            0\n",
    "        )  # shape (1, num_heads, query_length, key_length)\n",
    "        return values\n",
    "\n",
    "    anchors = self.anchor_array\n",
    "    values = []\n",
    "    for anchor in anchors:\n",
    "        mot = [[anchor[idx], anchor[idx + 1]] for idx in range(len(anchor) - 1)]\n",
    "        max_answer_length = max([x[1] - x[0] for x in mot])\n",
    "        # print(a, b, c, d, max_answer_length)\n",
    "        context_position_new = context_position.clone()\n",
    "        for i in range(1, len(mot)):\n",
    "            context_position_new[mot[i][0] : mot[i][1]] = (\n",
    "                context_position_new[mot[i][0] : mot[i][1]] - mot[0][0]\n",
    "            )\n",
    "        context_position_new[-1] = mot[0][0] + 2 * max_answer_length\n",
    "        memory_position_new = context_position_new.clone().view(1, -1)\n",
    "        relative_position = (\n",
    "            memory_position_new - context_position_new\n",
    "        )  # shape (query_length, key_length)\n",
    "        for i in range(len(mot)):\n",
    "            for j in range(len(mot)):\n",
    "                if i != j:\n",
    "                    x = mot[i]\n",
    "                    y = mot[j]\n",
    "                    relative_position[x[0] : x[1], y[0] : y[1]] += max_answer_length\n",
    "        relative_position_bucket = self._relative_position_bucket(\n",
    "            relative_position,  # shape (query_length, key_length)\n",
    "            bidirectional=(not self.is_decoder),\n",
    "            num_buckets=self.relative_attention_num_buckets,\n",
    "            max_distance=self.relative_attention_max_distance,\n",
    "        )\n",
    "        value = self.relative_attention_bias(relative_position_bucket)\n",
    "        values.append(value)\n",
    "    values = torch.stack(values)  # shape [1, 91, 91, 16]\n",
    "    values = values.permute(\n",
    "        [0, 3, 1, 2]\n",
    "    )  # shape (batch size, num_heads, query_length, key_length)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.490866Z",
     "iopub.status.busy": "2024-04-05T04:32:17.490422Z",
     "iopub.status.idle": "2024-04-05T04:32:17.492604Z",
     "shell.execute_reply": "2024-04-05T04:32:17.492222Z"
    }
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "\n",
    "extra_dim_learning = []\n",
    "\n",
    "\n",
    "def set_mode(MODE):\n",
    "    itself = model.encoder.block[0].layer[0].SelfAttention\n",
    "    if MODE == \"new\":\n",
    "        # itself.forward = partial(modified_self_attention_forward, itself)\n",
    "        itself.compute_bias = partial(new_compute_bias, itself)\n",
    "        model.forward = hook(\n",
    "            input_before_hooker,\n",
    "            partial(T5ForConditionalGeneration.forward, model),\n",
    "            input_after_hooker,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # itself.forward = partial(\n",
    "        #     model.encoder.block[0].layer[0].SelfAttention.__class__.forward, itself\n",
    "        # )\n",
    "        itself.compute_bias = partial(\n",
    "            model.encoder.block[0].layer[0].SelfAttention.__class__.compute_bias, itself\n",
    "        )\n",
    "        model.forward = T5ForConditionalGeneration.forward\n",
    "\n",
    "\n",
    "def check_encoded(all_input_ids):\n",
    "    anchors = []\n",
    "    for input_ids in all_input_ids:\n",
    "        # print('\\n'.join([f'{x.item()},{y}' for x,y in zip(input_ids, tokens)][50:]))\n",
    "        original = input_ids.tolist()\n",
    "        anchor = []\n",
    "        for i in range(len(input_ids)):\n",
    "            if (\n",
    "                i < len(input_ids) - 2\n",
    "                and input_ids[i] == 41\n",
    "                and input_ids[i + 1] == 3\n",
    "                and input_ids[i + 2] == 61\n",
    "            ) or original[i] == 1:\n",
    "                anchor.append(i)\n",
    "        anchors.append(anchor)\n",
    "    return anchors\n",
    "\n",
    "\n",
    "def input_before_hooker(*args, **kwargs):\n",
    "    input_ids = kwargs[\"input_ids\"]\n",
    "    # print('old ',input_ids)\n",
    "    anchors = check_encoded(input_ids)\n",
    "    final_inputs = []\n",
    "    for input_id, anchor in zip(input_ids, anchors):\n",
    "        input_id = input_id.tolist()\n",
    "\n",
    "        real_max_len = max(\n",
    "            [anchor[idx + 1] - anchor[idx] for idx in range(len(anchor) - 1)]\n",
    "        )\n",
    "        if real_max_len > max_answer_length:\n",
    "            print(f\"ALERT: MAX LENGTH IS {real_max_len}\")\n",
    "        for x in reversed(range(1, len(anchor))):\n",
    "            if anchor[x] - anchor[x - 1] < max_answer_length:\n",
    "                [\n",
    "                    input_id.insert(anchor[x], 0)\n",
    "                    for _ in range(max_answer_length - (anchor[x] - anchor[x - 1]))\n",
    "                ]\n",
    "\n",
    "        final_inputs.append(input_id)\n",
    "\n",
    "    max_length = max([len(input) for input in final_inputs])\n",
    "    mask = [[1] * max_length] * len(final_inputs)\n",
    "    for idx, input in enumerate(final_inputs):\n",
    "        for x in range(max_length):\n",
    "            if x >= len(input):\n",
    "                mask[idx][x] = 0\n",
    "        for x in range(max_length - len(input)):\n",
    "            input.append(0)\n",
    "    kwargs[\"input_ids\"] = torch.tensor(final_inputs).to(input_ids.device)\n",
    "    kwargs[\"attention_mask\"] = torch.tensor(mask).to(input_ids.device)\n",
    "    # print('new ',kwargs[\"input_ids\"])\n",
    "    anchors = check_encoded(kwargs[\"input_ids\"])\n",
    "    model.encoder.block[0].layer[0].SelfAttention.anchor_array = anchors\n",
    "\n",
    "\n",
    "def input_after_hooker(*args, **kwargs):\n",
    "    model.encoder.block[0].layer[0].SelfAttention.anchor_array = None\n",
    "\n",
    "\n",
    "# set_mode('old')\n",
    "set_mode(\"new\")\n",
    "# run_model([dataset_train[0][0],dataset_train[1][0]])\n",
    "# run_model([\"\"\"A person wants to start\"\"\", 'mot hai ba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.497905Z",
     "iopub.status.busy": "2024-04-05T04:32:17.497471Z",
     "iopub.status.idle": "2024-04-05T04:32:17.499397Z",
     "shell.execute_reply": "2024-04-05T04:32:17.498988Z"
    }
   },
   "outputs": [],
   "source": [
    "def measure_unalike(arr, print_arr=False):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    if print_arr:\n",
    "        print(arr)\n",
    "    return 1 - ((arr / n) ** 2).sum()\n",
    "\n",
    "\n",
    "question_to_do = 5\n",
    "per_question = 20\n",
    "\n",
    "\n",
    "def get_model_forward(input_ids, attention_mask, model=model):\n",
    "    with torch.no_grad():\n",
    "        start = []\n",
    "        [start.append([0]) for x in range(len(input_ids))]\n",
    "        for k in range(max_answer_length):\n",
    "            # print(torch.tensor(start).shape)\n",
    "            result = model(\n",
    "                input_ids=input_ids.to(DEVICE),\n",
    "                attention_mask=attention_mask.to(DEVICE),\n",
    "                decoder_input_ids=torch.tensor(start).to(DEVICE),\n",
    "                output_attentions=True,\n",
    "            )\n",
    "            item = result.logits.argmax(dim=2)[:, -1]\n",
    "            # print('loi',result.logits.shape, item)\n",
    "            for index in range(len(item)):\n",
    "                start[index].append(item[index].item())\n",
    "            if torch.allclose(item, torch.tensor(1)):\n",
    "                break\n",
    "            #     break\n",
    "    result = []\n",
    "    for batch in start:\n",
    "        y = -1\n",
    "        for index, x in enumerate(batch):\n",
    "            if x == 1:\n",
    "                y = index\n",
    "                break\n",
    "        result.append(batch[: y + 1] if y > -1 else batch)\n",
    "    return [tokenizer.decode(x, skip_special_tokens=True) for x in result]\n",
    "\n",
    "\n",
    "def run_model(input_strs):\n",
    "    if input_strs is str:\n",
    "        input_strs = [input_strs]\n",
    "    input_ids_wrapper = tokenizer(input_strs, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    answer = get_model_forward(\n",
    "        input_ids_wrapper[\"input_ids\"], input_ids_wrapper[\"attention_mask\"]\n",
    "    )\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.504271Z",
     "iopub.status.busy": "2024-04-05T04:32:17.503805Z",
     "iopub.status.idle": "2024-04-05T04:32:17.505304Z",
     "shell.execute_reply": "2024-04-05T04:32:17.505650Z"
    }
   },
   "outputs": [],
   "source": [
    "kk = [\n",
    "    (index, x, y)\n",
    "    for index, (x, y) in enumerate(model.named_parameters())\n",
    "    if y.requires_grad == True\n",
    "]\n",
    "[(index, x) for index, x, y in kk if \"decoder\" in x]\n",
    "len(kk)\n",
    "all_position_weight = [\n",
    "    y\n",
    "    for index, x, y in kk\n",
    "    if (\"extra_dimension_embedding\" in x)\n",
    "    or ((\"encoder\" in x) and (\"relative_attention_bias\" in x))\n",
    "]\n",
    "to_train_model = [y for index, x, y in kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(print(idx),CheckTransform.__call__(None, data_array[idx])) for idx,x in enumerate(data_array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:32:17.530466Z",
     "iopub.status.busy": "2024-04-05T04:32:17.530017Z",
     "iopub.status.idle": "2024-04-05T04:32:17.532649Z",
     "shell.execute_reply": "2024-04-05T04:32:17.532999Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    ll = 1\n",
    "    wrong_answers = [[] for x in range(ll)]\n",
    "    got_2 = [[] for x in range(ll)]\n",
    "    got_1 = [[] for x in range(ll)]\n",
    "    answers = [[] for x in range(ll)]\n",
    "    last_str = None\n",
    "    last_acc = None\n",
    "    for ix in range(ll):\n",
    "        print(f\"Name {DATABASE_NAME[ix]}\")\n",
    "        # [pickle.load(open(f\"multiple_choice_datasets/obqa_fact_test.pkl\", \"rb\"))]:\n",
    "        # print(f\"test {data==dataset_test}\")\n",
    "        count = 0\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "        count10 = 0\n",
    "        total = 0\n",
    "        data = dataset_test[ix]\n",
    "        pbar1 = trange(len(data))\n",
    "        for ques in pbar1:\n",
    "            question = data[ques][0]\n",
    "            key = data[ques][1]\n",
    "            total += 1\n",
    "            answer = run_model(question)[0]\n",
    "            answers[ix].append(answer)\n",
    "            if key == answer:\n",
    "                count += 1\n",
    "            else:\n",
    "                wrong_answers[ix].append(ques)\n",
    "            if key[0] == answer[0]:\n",
    "                count1 += 1\n",
    "                got_1[ix].append(ques)\n",
    "            if key[:2] == answer[:2]:\n",
    "                count2 += 1\n",
    "                got_2[ix].append(ques)\n",
    "            if answer in question:\n",
    "                count10 += 1\n",
    "            last_str = f\"{count}, {count1}, {count2}, {count10},{total},{count/total*100:.2f},{count10/total*100:.2f}\"\n",
    "            last_acc = f\"{count/total*100:.2f}\"\n",
    "            pbar1.set_postfix_str(last_str)\n",
    "    return last_str, last_acc,wrong_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name qasc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 926/926 [05:55<00:00,  2.60it/s, 913, 920, 920, 923,926,98.60,99.68]  \n"
     ]
    }
   ],
   "source": [
    "_,_,wrong_answers= evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure resilient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T04:54:38.127143Z",
     "iopub.status.busy": "2024-04-05T04:54:38.126677Z",
     "iopub.status.idle": "2024-04-05T04:54:38.128283Z",
     "shell.execute_reply": "2024-04-05T04:54:38.128653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def measure_unalike(arr):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    return 1 - ((arr / n) ** 2).sum()\n",
    "\n",
    "\n",
    "measure_unalike([\"a\", \"a\", \"a\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
