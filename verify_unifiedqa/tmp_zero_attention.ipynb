{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldh0033@auburn.edu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Adafactor\n",
    "from functools import wraps, partial\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "\n",
    "model_name = \"google-t5/t5-large\"\n",
    "# model_name = (\n",
    "#     \"allenai/unifiedqa-v2-t5-large-1363200\"  # you can specify the model size here\n",
    "# )\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "DEVICE = 0\n",
    "model_original = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name, device_map=f\"cuda:{DEVICE}\")  #'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# del model\n",
    "model = model_original  # copy.deepcopy(model_original)\n",
    "\n",
    "\n",
    "# %%\n",
    "def DEFAULT_COMPUTE_BIAS(self, query_length, key_length, device=None):\n",
    "    \"\"\"Compute binned relative position bias\"\"\"\n",
    "    if device is None:\n",
    "        device = self.relative_attention_bias.weight.device\n",
    "    context_position = torch.arange(query_length, dtype=torch.long, device=device)[\n",
    "        :, None\n",
    "    ]\n",
    "    memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n",
    "    relative_position = (\n",
    "        memory_position - context_position\n",
    "    )  # shape (query_length, key_length)\n",
    "    relative_position_bucket = self._relative_position_bucket(\n",
    "        relative_position,  # shape (query_length, key_length)\n",
    "        bidirectional=(not self.is_decoder),\n",
    "        num_buckets=self.relative_attention_num_buckets,\n",
    "        max_distance=self.relative_attention_max_distance,\n",
    "    )\n",
    "    values = self.relative_attention_bias(\n",
    "        relative_position_bucket\n",
    "    )  # shape (query_length, key_length, num_heads)\n",
    "    values = values.permute([2, 0, 1]).unsqueeze(\n",
    "        0\n",
    "    )  # shape (1, num_heads, query_length, key_length)\n",
    "    return values\n",
    "\n",
    "\n",
    "# %%\n",
    "import pickle\n",
    "\n",
    "dataset_test = pickle.load(open(\"test_without_abcd.pkl\", \"rb\"))\n",
    "dataset_train = pickle.load(open(\"train_without_abcd.pkl\", \"rb\"))\n",
    "\n",
    "# %%\n",
    "MODE = \"new\"  #'old'\n",
    "\n",
    "# if hasattr(layer, 'EncDecAttention'):\n",
    "#     layer.EncDecAttention.compute_bias = partial(\n",
    "#         new_compute_bias, layer.EncDecAttention)\n",
    "\n",
    "# %%\n",
    "model.hf_device_map\n",
    "\n",
    "# %%\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def measure_unalike(arr, print_arr=False):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    if print_arr:\n",
    "        print(arr)\n",
    "    return 1 - ((arr / n) ** 2).sum()\n",
    "\n",
    "\n",
    "question_to_do = 5\n",
    "per_question = 20\n",
    "\n",
    "\n",
    "def run_tokens(tokens):\n",
    "    res = model.generate(tokens, max_new_tokens=MAX_ANSWER_LENGTH)\n",
    "    return tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def run_model(input_string, **generator_args):\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    # print(torch.argwhere(input_ids[0]==2)[0,0]+2)\n",
    "    res = model.generate(\n",
    "        input_ids.to(DEVICE), **generator_args, max_new_tokens=MAX_ANSWER_LENGTH\n",
    "    )\n",
    "    return tokenizer.batch_decode(res, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is responsible for \\n ( ) puppies learning new tricks ( )\n",
      "children growing up and getting old ( ) flowers wilting in a vase ( )\n",
      "plants sprouting, blooming and wilting\n",
      "old  ['dogs learning new tricks ( ) ( ) n n n n n n n']\n",
      "new  ['growing up and getting old ( ).']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# %%\n",
    "QUESTION_MAX_LENGTH = 76\n",
    "MAX_ANSWER_LENGTH = 40\n",
    "\n",
    "\n",
    "# %%\n",
    "def check(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")[0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    original = input_ids.tolist()\n",
    "    anchor = []\n",
    "    for i in range(len(tokens)):\n",
    "        if (\n",
    "            i < len(tokens) - 2\n",
    "            and tokens[i] == \"▁(\"\n",
    "            and tokens[i + 1] == \"▁\"\n",
    "            and tokens[i + 2] == \")\"\n",
    "        ) or original[i] == 1:\n",
    "            anchor.append(i)\n",
    "    # 0 1 2 3 4\n",
    "    for x in reversed(range(1, 5)):\n",
    "        if anchor[x] - anchor[x - 1] < MAX_ANSWER_LENGTH:\n",
    "            [\n",
    "                original.insert(anchor[x], 0)\n",
    "                for _ in range(MAX_ANSWER_LENGTH - (anchor[x] - anchor[x - 1]))\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"Wrong size ANSWER: {anchor[x] - anchor[x - 1] }\")\n",
    "            return None\n",
    "    if anchor[0] < QUESTION_MAX_LENGTH:\n",
    "        [original.insert(anchor[0], 0) for _ in range(QUESTION_MAX_LENGTH - anchor[0])]\n",
    "    else:\n",
    "        print(f\"Wrong size QUESTION: {anchor[0]}\")\n",
    "        return None\n",
    "    return torch.tensor(original).view(1, -1)\n",
    "\n",
    "\n",
    "start_pos = QUESTION_MAX_LENGTH\n",
    "leng = MAX_ANSWER_LENGTH\n",
    "a = torch.arange(start_pos + leng * 0, start_pos + leng * 1, dtype=int)\n",
    "b = torch.arange(start_pos + leng * 1, start_pos + leng * 2, dtype=int)\n",
    "c = torch.arange(start_pos + leng * 2, start_pos + leng * 3, dtype=int)\n",
    "d = torch.arange(start_pos + leng * 3, start_pos + leng * 4, dtype=int)\n",
    "question_portion=torch.arange(0, start_pos)\n",
    "question_ending=start_pos + leng * 4\n",
    "DEC = {\"01\": 0, \"02\": 1, \"03\": 2, \"12\": 3, \"13\": 4, \"23\": 5}\n",
    "mot = [a, b, c, d]\n",
    "six_mask_turn_off = torch.ones((237, 237, 16))\n",
    "six_mask_turn_on = torch.zeros((6, 237, 237, 16))\n",
    "\n",
    "for i in range(len(mot) - 1):\n",
    "    for j in range(i + 1, len(mot)):\n",
    "        x = mot[i]\n",
    "        y = mot[j]\n",
    "        # print(mask_turn_off_hyper_dimension[x][:, y][:].shape)\n",
    "        # cal index in 6\n",
    "        comb_index = DEC[f\"{i}{j}\"]\n",
    "        # no distance, a very special distance\n",
    "        six_mask_turn_on[comb_index][x][:, y][:] = 2\n",
    "        six_mask_turn_on[comb_index][y][:, x][:] = 2\n",
    "        six_mask_turn_off[x][:, y][:] = 0\n",
    "        six_mask_turn_off[y][:, x][:] = 0\n",
    "\n",
    "\n",
    "# %%\n",
    "def new_compute_bias(self, query_length, key_length, device=None):\n",
    "    \"\"\"Compute binned relative position bias\"\"\"\n",
    "    if device is None:\n",
    "        device = self.relative_attention_bias.weight.device\n",
    "    context_position = torch.arange(query_length, dtype=torch.long, device=device)[\n",
    "        :, None\n",
    "    ]\n",
    "    memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n",
    "\n",
    "    relative_position = (\n",
    "        memory_position - context_position\n",
    "    )  # shape (query_length, key_length)\n",
    "    # implementation='simple'\n",
    "    if self.is_decoder:\n",
    "        pass\n",
    "    else:\n",
    "        context_position_new = context_position.clone()\n",
    "        context_position_new[b] = context_position_new[a]\n",
    "        context_position_new[c] = context_position_new[a]\n",
    "        context_position_new[d] = context_position_new[a]\n",
    "        context_position_new[-1] = context_position_new[a[0]] + leng\n",
    "        memory_position_new = context_position_new.clone().view(1, -1)\n",
    "        relative_position = (\n",
    "            memory_position_new - context_position_new\n",
    "        )  # shape (query_length, key_length)\n",
    "\n",
    "    relative_position_bucket = self._relative_position_bucket(\n",
    "        relative_position,  # shape (query_length, key_length)\n",
    "        bidirectional=(not self.is_decoder),\n",
    "        num_buckets=self.relative_attention_num_buckets,\n",
    "        max_distance=self.relative_attention_max_distance,\n",
    "    )\n",
    "\n",
    "    values = self.relative_attention_bias(relative_position_bucket)\n",
    "    values = values.permute([2, 0, 1]).unsqueeze(\n",
    "        0\n",
    "    )  # shape (1, num_heads, query_length, key_length)\n",
    "    return values\n",
    "\n",
    "\n",
    "def DEFAULT_FORWARD(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    mask=None,\n",
    "    key_value_states=None,\n",
    "    position_bias=None,\n",
    "    past_key_value=None,\n",
    "    layer_head_mask=None,\n",
    "    query_length=None,\n",
    "    use_cache=False,\n",
    "    output_attentions=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n",
    "    \"\"\"\n",
    "    # Input is (batch_size, seq_length, dim)\n",
    "    # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n",
    "    # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n",
    "    batch_size, seq_length = hidden_states.shape[:2]\n",
    "\n",
    "    real_seq_length = seq_length\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        if len(past_key_value) != 2:\n",
    "            raise ValueError(\n",
    "                f\"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states\"\n",
    "            )\n",
    "        real_seq_length += (\n",
    "            past_key_value[0].shape[2] if query_length is None else query_length\n",
    "        )\n",
    "\n",
    "    key_length = (\n",
    "        real_seq_length if key_value_states is None else key_value_states.shape[1]\n",
    "    )\n",
    "\n",
    "    def shape(states):\n",
    "        \"\"\"projection\"\"\"\n",
    "        return states.view(\n",
    "            batch_size, -1, self.n_heads, self.key_value_proj_dim\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "    def unshape(states):\n",
    "        \"\"\"reshape\"\"\"\n",
    "        return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n",
    "\n",
    "    def project(hidden_states, proj_layer, key_value_states, past_key_value):\n",
    "        \"\"\"projects hidden states correctly to key/query states\"\"\"\n",
    "        if key_value_states is None:\n",
    "            # self-attn\n",
    "            # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "            hidden_states = shape(proj_layer(hidden_states))\n",
    "        elif past_key_value is None:\n",
    "            # cross-attn\n",
    "            # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "            hidden_states = shape(proj_layer(key_value_states))\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            if key_value_states is None:\n",
    "                # self-attn\n",
    "                # (batch_size, n_heads, key_length, dim_per_head)\n",
    "                hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n",
    "            elif past_key_value.shape[2] != key_value_states.shape[1]:\n",
    "                # checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "                # the provided `key_value_states` to support prefix tuning\n",
    "                # cross-attn\n",
    "                # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "                hidden_states = shape(proj_layer(key_value_states))\n",
    "            else:\n",
    "                # cross-attn\n",
    "                hidden_states = past_key_value\n",
    "        return hidden_states\n",
    "\n",
    "    # get query states\n",
    "    query_states = shape(\n",
    "        self.q(hidden_states)\n",
    "    )  # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "\n",
    "    # get key/value states\n",
    "    key_states = project(\n",
    "        hidden_states,\n",
    "        self.k,\n",
    "        key_value_states,\n",
    "        past_key_value[0] if past_key_value is not None else None,\n",
    "    )\n",
    "    value_states = project(\n",
    "        hidden_states,\n",
    "        self.v,\n",
    "        key_value_states,\n",
    "        past_key_value[1] if past_key_value is not None else None,\n",
    "    )\n",
    "\n",
    "    # compute scores\n",
    "    scores = torch.matmul(\n",
    "        query_states, key_states.transpose(3, 2)\n",
    "    )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n",
    "\n",
    "    if position_bias is None:\n",
    "        if not self.has_relative_attention_bias:\n",
    "            position_bias = torch.zeros(\n",
    "                (1, self.n_heads, real_seq_length, key_length),\n",
    "                device=scores.device,\n",
    "                dtype=scores.dtype,\n",
    "            )\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                position_bias.requires_grad = True\n",
    "        else:\n",
    "            position_bias = self.compute_bias(\n",
    "                real_seq_length, key_length, device=scores.device\n",
    "            )\n",
    "\n",
    "        # if key and values are already calculated\n",
    "        # we want only the last query position bias\n",
    "        if past_key_value is not None:\n",
    "            position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n",
    "\n",
    "        if mask is not None:\n",
    "            position_bias = (\n",
    "                position_bias + mask\n",
    "            )  # (batch_size, n_heads, seq_length, key_length)\n",
    "\n",
    "    if self.pruned_heads:\n",
    "        mask = torch.ones(position_bias.shape[1])\n",
    "        mask[list(self.pruned_heads)] = 0\n",
    "        position_bias_masked = position_bias[:, mask.bool()]\n",
    "    else:\n",
    "        position_bias_masked = position_bias\n",
    "\n",
    "    scores += position_bias_masked\n",
    "    attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n",
    "        scores\n",
    "    )  # (batch_size, n_heads, seq_length, key_length)\n",
    "    attn_weights = nn.functional.dropout(\n",
    "        attn_weights, p=self.dropout, training=self.training\n",
    "    )  # (batch_size, n_heads, seq_length, key_length)\n",
    "\n",
    "    # Mask heads if we want to\n",
    "    if layer_head_mask is not None:\n",
    "        attn_weights = attn_weights * layer_head_mask\n",
    "\n",
    "    attn_output = unshape(\n",
    "        torch.matmul(attn_weights, value_states)\n",
    "    )  # (batch_size, seq_length, dim)\n",
    "    attn_output = self.o(attn_output)\n",
    "\n",
    "    present_key_value_state = (\n",
    "        (key_states, value_states) if (self.is_decoder and use_cache) else None\n",
    "    )\n",
    "    outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n",
    "\n",
    "    if output_attentions:\n",
    "        outputs = outputs + (attn_weights,)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def ANNUL_FORWARD(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    mask=None,\n",
    "    key_value_states=None,\n",
    "    position_bias=None,\n",
    "    past_key_value=None,\n",
    "    layer_head_mask=None,\n",
    "    query_length=None,\n",
    "    use_cache=False,\n",
    "    output_attentions=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n",
    "    \"\"\"\n",
    "    # Input is (batch_size, seq_length, dim)\n",
    "    # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n",
    "    # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n",
    "    batch_size, seq_length = hidden_states.shape[:2]\n",
    "\n",
    "    real_seq_length = seq_length\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        if len(past_key_value) != 2:\n",
    "            raise ValueError(\n",
    "                f\"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states\"\n",
    "            )\n",
    "        real_seq_length += (\n",
    "            past_key_value[0].shape[2] if query_length is None else query_length\n",
    "        )\n",
    "\n",
    "    key_length = (\n",
    "        real_seq_length if key_value_states is None else key_value_states.shape[1]\n",
    "    )\n",
    "\n",
    "    def shape(states):\n",
    "        \"\"\"projection\"\"\"\n",
    "        return states.view(\n",
    "            batch_size, -1, self.n_heads, self.key_value_proj_dim\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "    def unshape(states):\n",
    "        \"\"\"reshape\"\"\"\n",
    "        return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n",
    "\n",
    "    def project(hidden_states, proj_layer, key_value_states, past_key_value):\n",
    "        \"\"\"projects hidden states correctly to key/query states\"\"\"\n",
    "        if key_value_states is None:\n",
    "            # self-attn\n",
    "            # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "            hidden_states = shape(proj_layer(hidden_states))\n",
    "        elif past_key_value is None:\n",
    "            # cross-attn\n",
    "            # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "            hidden_states = shape(proj_layer(key_value_states))\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            if key_value_states is None:\n",
    "                # self-attn\n",
    "                # (batch_size, n_heads, key_length, dim_per_head)\n",
    "                hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n",
    "            elif past_key_value.shape[2] != key_value_states.shape[1]:\n",
    "                # checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "                # the provided `key_value_states` to support prefix tuning\n",
    "                # cross-attn\n",
    "                # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "                hidden_states = shape(proj_layer(key_value_states))\n",
    "            else:\n",
    "                # cross-attn\n",
    "                hidden_states = past_key_value\n",
    "        return hidden_states\n",
    "\n",
    "    # get query states\n",
    "    query_states = shape(\n",
    "        self.q(hidden_states)\n",
    "    )  # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "\n",
    "    # get key/value states\n",
    "    key_states = project(\n",
    "        hidden_states,\n",
    "        self.k,\n",
    "        key_value_states,\n",
    "        past_key_value[0] if past_key_value is not None else None,\n",
    "    )\n",
    "    value_states = project(\n",
    "        hidden_states,\n",
    "        self.v,\n",
    "        key_value_states,\n",
    "        past_key_value[1] if past_key_value is not None else None,\n",
    "    )\n",
    "\n",
    "    # compute scores\n",
    "    scores = torch.matmul(\n",
    "        query_states, key_states.transpose(3, 2)\n",
    "    )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n",
    "\n",
    "    if position_bias is None:\n",
    "        if not self.has_relative_attention_bias:\n",
    "            position_bias = torch.zeros(\n",
    "                (1, self.n_heads, real_seq_length, key_length),\n",
    "                device=scores.device,\n",
    "                dtype=scores.dtype,\n",
    "            )\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                position_bias.requires_grad = True\n",
    "        else:\n",
    "            position_bias = self.compute_bias(\n",
    "                real_seq_length, key_length, device=scores.device\n",
    "            )\n",
    "\n",
    "        # if key and values are already calculated\n",
    "        # we want only the last query position bias\n",
    "        if past_key_value is not None:\n",
    "            position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n",
    "\n",
    "        if mask is not None:\n",
    "            position_bias = (\n",
    "                position_bias + mask\n",
    "            )  # (batch_size, n_heads, seq_length, key_length)\n",
    "\n",
    "    if self.pruned_heads:\n",
    "        mask = torch.ones(position_bias.shape[1])\n",
    "        mask[list(self.pruned_heads)] = 0\n",
    "        position_bias_masked = position_bias[:, mask.bool()]\n",
    "    else:\n",
    "        position_bias_masked = position_bias\n",
    "\n",
    "    scores += position_bias_masked\n",
    "    for i, x in enumerate(mot):\n",
    "        for j, y in enumerate(mot):\n",
    "            if i != j:\n",
    "                scores[:,:,x][:,:,:, y] = 0\n",
    "    for i, x in enumerate(mot):\n",
    "        scores[:,:,question_portion][:,:,:,x]=0\n",
    "        scores[:,:,x][:,:,:,question_portion]=0\n",
    "        scores[:,:,x][:,:,:,question_ending]=0\n",
    "        scores[:,:,question_ending][:,:,x]=0\n",
    "    attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n",
    "        scores\n",
    "    )  # (batch_size, n_heads, seq_length, key_length)\n",
    "    attn_weights = nn.functional.dropout(\n",
    "        attn_weights, p=self.dropout, training=self.training\n",
    "    )  # (batch_size, n_heads, seq_length, key_length)\n",
    "\n",
    "    # Mask heads if we want to\n",
    "    if layer_head_mask is not None:\n",
    "        attn_weights = attn_weights * layer_head_mask\n",
    "\n",
    "    attn_output = unshape(\n",
    "        torch.matmul(attn_weights, value_states)\n",
    "    )  # (batch_size, seq_length, dim)\n",
    "    attn_output = self.o(attn_output)\n",
    "\n",
    "    present_key_value_state = (\n",
    "        (key_states, value_states) if (self.is_decoder and use_cache) else None\n",
    "    )\n",
    "    outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n",
    "\n",
    "    if output_attentions:\n",
    "        outputs = outputs + (attn_weights,)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "extra_dim_learning = []\n",
    "\n",
    "\n",
    "def set_mode(MODE):\n",
    "    for part in [\"encoder\"]:  # , 'decoder']:\n",
    "        for block in getattr(model, part).block:\n",
    "            for layer in block.layer:\n",
    "                if hasattr(layer, \"SelfAttention\"):\n",
    "                    # ALL block\n",
    "                    itself = layer.SelfAttention\n",
    "                    if MODE == \"new\":\n",
    "                        itself.forward = partial(ANNUL_FORWARD, layer.SelfAttention)\n",
    "                    else:\n",
    "                        itself.forward = partial(DEFAULT_FORWARD, layer.SelfAttention)\n",
    "                    if layer.SelfAttention.has_relative_attention_bias:\n",
    "                        # block 0 ONLY\n",
    "                        if MODE == \"new\":\n",
    "                            itself.compute_bias = partial(\n",
    "                                new_compute_bias, layer.SelfAttention\n",
    "                            )\n",
    "                        else:\n",
    "                            itself.compute_bias = partial(\n",
    "                                DEFAULT_COMPUTE_BIAS, layer.SelfAttention\n",
    "                            )\n",
    "\n",
    "\n",
    "print(textwrap.fill(dataset_train[0][0]))\n",
    "set_mode(\"old\")\n",
    "print(\"old \", run_tokens(check(dataset_train[0][0]).to(DEVICE)))\n",
    "set_mode(\"new\")\n",
    "print(\"new \", run_tokens(check(dataset_train[0][0]).to(DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = [(index, x, y) for index, (x, y) in enumerate(model.named_parameters())\n",
    "      if y.requires_grad == True]\n",
    "to_train_model = [y for index, x, y in kk]  # [:196]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def shape(input):\n",
    "    return input.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_forward(input_tokens):\n",
    "    encoder_attentions = None\n",
    "    last_hidden = None\n",
    "    with torch.no_grad():\n",
    "        start = [0]\n",
    "        for k in range(MAX_ANSWER_LENGTH):\n",
    "            result = model(\n",
    "                input_ids=input_tokens.to(DEVICE),\n",
    "                decoder_input_ids=torch.tensor([start]).to(DEVICE),\n",
    "                output_attentions=True,\n",
    "            )\n",
    "            encoder_attentions = result.encoder_attentions\n",
    "            last_hidden = result.encoder_last_hidden_state\n",
    "            item = result.logits.argmax(dim=2)[0][-1].item()\n",
    "            start.append(item)\n",
    "            if item == 1:\n",
    "                break\n",
    "            # print(start)\n",
    "    return (\n",
    "        tokenizer.decode(start, skip_special_tokens=True),\n",
    "        tokenizer.convert_ids_to_tokens(start),\n",
    "        last_hidden,\n",
    "        encoder_attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "\n",
    "def get_loss(logits, labels):\n",
    "    loss = torch.tensor(0)\n",
    "    found = False\n",
    "    for i in range(len(labels[0])):\n",
    "        current_loss = loss_fn(logits[0][i], labels[0][i].to(DEVICE))\n",
    "\n",
    "        current_certainly = torch.exp(-current_loss)\n",
    "        if current_certainly < 0.9:\n",
    "            loss = current_loss\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        loss = loss_fn(logits[0], labels[0].to(DEVICE))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # pbar = trange(0, len(dataset_train), 24)\n",
    "    # loss_score = 0\n",
    "    # count = 0\n",
    "    # extra_info = \"\"\n",
    "    # step=0\n",
    "    # # if count>20:\n",
    "    # #     break\n",
    "    # # print(textwrap.fill(dataset_train[0][0]))\n",
    "    step = 0\n",
    "    pbar = trange(200)\n",
    "    for re in pbar:\n",
    "        input_tokens = check(dataset_train[step][0])\n",
    "        labels = tokenizer.encode(dataset_train[step][1], return_tensors=\"pt\")\n",
    "        result = model(input_ids=input_tokens.to(DEVICE),\n",
    "                       labels=shape(labels).to(DEVICE))\n",
    "        loss = get_loss(result.logits, labels)\n",
    "        # print(result.logits.argmax(dim=2), labels)\n",
    "        optimizer.zero_grad()\n",
    "        # loss = result.loss\n",
    "        # print(result.logits, labels, loss)\n",
    "        if loss.item() != 0:\n",
    "            loss_score = loss.item()  # loss_score * 0.9 + loss.item() * 0.1\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # with torch.no_grad():\n",
    "        #     mong= model(input_ids=check(dataset_train[0][0]).to(DEVICE), decoder_input_ids=torch.tensor([[0]]).to(DEVICE))\n",
    "        #     print(mong.logits.argmax(dim=2).shape)\n",
    "        # print(tokenizer.decode())\n",
    "\n",
    "        extra_info = get_model_forward(\n",
    "            check(dataset_train[step][0]).to(DEVICE))\n",
    "        pbar.set_postfix_str(f\"Loss: {loss_score:.10f}:{extra_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong size QUESTION: 88\n"
     ]
    }
   ],
   "source": [
    "data_array = [(k, v, l.split(\" ( ) \")[1:])\n",
    "              for l, k, v in [(dataset_train[x][0], check(dataset_train[x][0]),\n",
    "                               dataset_train[x][1])\n",
    "                              for x in range(0, len(dataset_train), 24)]\n",
    "              if k is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CheckTransform(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # print(f\"'{sample[1]}'\")\n",
    "        return {\n",
    "            \"input_ids\": sample[0][0],\n",
    "            \"label_index\": sample[2].index(sample[1]),\n",
    "            \"all_labels\": sample[2],\n",
    "        }\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_array, transform=None):\n",
    "        self.dataset = dataset_array\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.dataset[idx])\n",
    "\n",
    "\n",
    "def collate(datas):\n",
    "    label_ids = tokenizer(sum([x[\"all_labels\"] for x in datas], []),\n",
    "                          padding=True)\n",
    "    wrapper = label_ids\n",
    "    wrapper[\"all_label_ids\"] = torch.tensor(wrapper.pop(\"input_ids\"))\n",
    "    # wrapper[\"label_index\"] = torch.tensor([x[\"label_index\"] for x in datas])\n",
    "    for k in wrapper[\"all_label_ids\"]:\n",
    "        k[k == tokenizer.pad_token_id] = -100\n",
    "    wrapper[\"all_decoder_attention_masks\"] = torch.tensor(\n",
    "        wrapper.pop(\"attention_mask\"))\n",
    "    wrapper[\"input_ids\"] = torch.stack([x[\"input_ids\"] for x in datas])\n",
    "    wrapper[\"label_index\"] = torch.tensor([x[\"label_index\"] for x in datas])\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "loi_dataloader = DataLoader(\n",
    "    CustomDataset(\n",
    "        data_array,\n",
    "        CheckTransform(),\n",
    "    ),\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "# for k in loi_dataloader:\n",
    "#     print(k[\"all_label_ids\"])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 898704\n",
    "# hidden state 242688\n",
    "# classification_layer = nn.Linear(242688, 4).to(DEVICE)\n",
    "optimizer = Adafactor(\n",
    "    to_train_model,  # + [x for x in classification_layer.parameters()],\n",
    "    relative_step=True,\n",
    "    warmup_init=True,\n",
    "    lr=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.572: 100%|██████████| 496/496 [09:38<00:00,  1.17s/it, quit eating lunch out]                                                \n",
      "Loss: 0.426: 100%|██████████| 496/496 [09:20<00:00,  1.13s/it, have lunch with friends]             \n",
      "Loss: 0.177: 100%|██████████| 496/496 [09:20<00:00,  1.13s/it, buy less lunch out]            \n",
      "Loss: 0.085:  27%|██▋       | 134/496 [02:31<06:59,  1.16s/it, buy less lunches]              "
     ]
    }
   ],
   "source": [
    "def turn_position_learning(on):\n",
    "    for x in all_position_weight:\n",
    "        x.requires_grad = on\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random_utils import set_seed\n",
    "\n",
    "loss_running_score = 0\n",
    "correct_running_score = 0\n",
    "conform_running_score = 0\n",
    "count = 0\n",
    "extra_info = \"\"\n",
    "res_tokens = []\n",
    "accumulate = 10\n",
    "optimizer.zero_grad()\n",
    "set_seed(42)\n",
    "for learn_pos in range(10):\n",
    "    pbar = tqdm(loi_dataloader)\n",
    "    for wrapper in pbar:\n",
    "        count += 1\n",
    "        only_correct_label_ids = torch.stack([\n",
    "            wrapper[\"all_label_ids\"][batch_index * 4 + x]\n",
    "            for batch_index, x in enumerate(wrapper[\"label_index\"])\n",
    "        ])\n",
    "        only_correct_decoder_attention_mask = torch.stack([\n",
    "            wrapper[\"all_decoder_attention_masks\"][batch_index * 4 + x]\n",
    "            for batch_index, x in enumerate(wrapper[\"label_index\"])\n",
    "        ])\n",
    "        result = model(\n",
    "            input_ids=wrapper[\"input_ids\"].to(DEVICE),\n",
    "            labels=only_correct_label_ids.to(DEVICE),\n",
    "            decoder_attention_mask=only_correct_decoder_attention_mask.to(\n",
    "                DEVICE),  # output_attentions=True\n",
    "        )\n",
    "        loss = result.loss\n",
    "        loss_running_score = loss_running_score * 0.9 + loss.item() * 0.1\n",
    "        if loss != 0:\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # scheduler.step()\n",
    "        with torch.no_grad():\n",
    "            if count % 10 == 0:\n",
    "                extra_info, res_tokens, _, _ = get_model_forward(\n",
    "                    check(dataset_test[0][0]).to(DEVICE))\n",
    "            pbar.set_description_str(f\"Loss: {loss_running_score:.3f}\")\n",
    "            pbar.set_postfix_str(extra_info)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_to_find=len(wrapper[\"label_ids\"][0])-1\n",
    "#         for batch in range(wrapper[\"input_ids\"].shape[0]):\n",
    "#             keys= result.logits[batch].argmax(dim=1)\n",
    "#             found=False\n",
    "#             tui_batch=wrapper[\"label_ids\"][batch]\n",
    "#             for m in range(len(tui_batch)):\n",
    "#                 if keys[m]!=tui_batch[m]:\n",
    "#                     if len_to_find>m:\n",
    "#                         len_to_find=m\n",
    "#                     break\n",
    "#         for batch in range(wrapper[\"input_ids\"].shape[0]):\n",
    "#             loss+=loss_fn(result.logits[batch][:len_to_find+1],tui_batch[:len_to_find+1].to(DEVICE))\n",
    "#         loss=loss/wrapper[\"input_ids\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:56<00:00,  4.30it/s, 152, 302, 287, 312]\n"
     ]
    }
   ],
   "source": [
    "data = dataset_test\n",
    "count = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "count10 = 0\n",
    "pbar1 = trange(500)\n",
    "for ques in pbar1:\n",
    "    question = data[24 * ques][0]\n",
    "    key = data[24 * ques][1]\n",
    "    answer = get_model_forward(check(data[24 * ques][0]).to(DEVICE))[0]\n",
    "    if key == answer:\n",
    "        count += 1\n",
    "    if key[0] == answer[0]:\n",
    "        count1 += 1\n",
    "    if key[:2] == answer[:2]:\n",
    "        count2 += 1\n",
    "    if answer in question:\n",
    "        count10 += 1\n",
    "    pbar1.set_postfix_str(f\"{count}, {count1}, {count2}, {count10}\")\n",
    "    # else:\n",
    "    #     pass\n",
    "    # print(ques,':****',textwrap.fill(question))\n",
    "    # print('Answer key',':****',key)\n",
    "    # print('Answer ',answer)\n",
    "\n",
    "# print(\"Count \", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbar = trange(0, len(dataset_train), 24)\n",
    "# loss_score = 0\n",
    "# count = 0\n",
    "# extra_info = \"\"\n",
    "# set_seed(42)\n",
    "# res_tokens=[]\n",
    "# for learn_pos in range(10):\n",
    "#     for step in pbar:\n",
    "#         count += 1\n",
    "#         # if count>20:\n",
    "#         #     break\n",
    "#         # print(textwrap.fill(dataset_train[0][0]))\n",
    "#         input_tokens = check(dataset_train[step][0])\n",
    "#         if input_tokens is None:\n",
    "#             continue\n",
    "#         labels = tokenizer.encode(dataset_train[step][1], return_tensors=\"pt\")\n",
    "#         result = model(input_ids=input_tokens.to(DEVICE), labels=shape(labels).to(DEVICE))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss =loss_fn(result.logits[0][learn_pos],labels[0][learn_pos].to(DEVICE))\n",
    "#         loss_score = loss_score * 0.9 + loss.item() * 0.1\n",
    "#         if loss.item()!=0:\n",
    "#             loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # scheduler.step()\n",
    "#         with torch.no_grad():\n",
    "#             if count % 10 == 0:\n",
    "#                 extra_info, res_tokens = get_model_forward(check(dataset_test[0][0]).to(DEVICE))\n",
    "#             pbar.set_description_str(f\"Loss: {loss_score:.2f}\")\n",
    "#             pbar.set_postfix_str(res_tokens[:learn_pos+2])\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCUMUTATE gradient\n",
    "# total_params = to_train_model\n",
    "# optimizer = SGD(total_params, lr=1e-2)\n",
    "# pbar = trange(0, len(dataset_train), 24)\n",
    "# loss_score = 0\n",
    "# for step in pbar:\n",
    "#     # count+=1\n",
    "#     # if count>20:\n",
    "#     #     break\n",
    "#     # print(textwrap.fill(dataset_train[0][0]))\n",
    "#     input_tokens = check(dataset_train[step][0])\n",
    "#     if input_tokens is None:\n",
    "#         continue\n",
    "#     labels = tokenizer.encode(dataset_train[step][1], return_tensors=\"pt\")\n",
    "#     result = model(input_ids=input_tokens.to(DEVICE), labels=shape(labels).to(DEVICE))\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = result.loss\n",
    "#     loss_score = loss_score * 0.9 + loss.item() * 0.1\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     with torch.no_grad():\n",
    "#         pbar.set_postfix_str(\n",
    "#             f\"Loss: {loss_score:.2f}:'{run_tokens(check(dataset_test[0][0]).to(DEVICE))}'\"\n",
    "#         )\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldh0033@auburn.edu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
