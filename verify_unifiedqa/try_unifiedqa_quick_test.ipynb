{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# model_name = \"allenai/unifiedqa-t5-large\"  # you can specify the model size here\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"allenai/unifiedqa-v2-t5-large-1363200\"  # you can specify the model size here\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name,\n",
    "                                                   device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_PAD = 45\n",
    "MAX_ANSWER_LENGTH = 40\n",
    "\n",
    "import itertools\n",
    "\n",
    "perm_order = list(itertools.permutations([0, 1, 2, 3], 4))\n",
    "\n",
    "import textwrap\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def measure_unalike(arr, print_arr=False):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    if print_arr:\n",
    "        print(arr)\n",
    "    return 1 - ((arr / n)**2).sum()\n",
    "\n",
    "\n",
    "question_to_do = 5\n",
    "per_question = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to \\\\n ( ) make more phone calls ( ) quit eating lunch out ( ) buy less with monopoly money ( ) have lunch with friends',\n",
       " 'quit eating lunch out')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "dataset_test = pickle.load(open('test_without_abcd.pkl', 'rb'))\n",
    "dataset_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   71,   568,  2746,    12,   456,  4380,   540,    78,    24,    79,\n",
      "            54,  5293,     3,     9,  1245,  4257,    44,     8,   414,    13,\n",
      "             8,   215,     5,   621,   479,   147,    70,  1487,    11,  5159,\n",
      "             6,    79,  2204,     8,   200,   194,    12,  1097,   540,    19,\n",
      "            12,     3,     2,    29,    41,     3,    61,   143,    72,   951,\n",
      "          3088,    41,     3,    61, 10399,  3182,  3074,    91,    41,     3,\n",
      "            61,   805,   705,    28,     3, 23507,    63,   540,    41,     3,\n",
      "            61,    43,  3074,    28,   803,     1]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     res \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m      6\u001b[0m                          \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerator_args,\n\u001b[1;32m      7\u001b[0m                          max_new_tokens\u001b[38;5;241m=\u001b[39mMAX_ANSWER_LENGTH)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(res, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(input_string, **generator_args)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(torch.argwhere(input_ids[0]==2)[0,0]+2)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m res \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      6\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerator_args,\n\u001b[1;32m      7\u001b[0m                      max_new_tokens\u001b[38;5;241m=\u001b[39mMAX_ANSWER_LENGTH)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(res, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def run_model(input_string, **generator_args):\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    print(input_ids)\n",
    "    # print(torch.argwhere(input_ids[0]==2)[0,0]+2)\n",
    "    res = model.generate(input_ids.to(0),\n",
    "                         **generator_args,\n",
    "                         max_new_tokens=MAX_ANSWER_LENGTH)\n",
    "    return tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "run_model(dataset_test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(index, perm):\n",
    "    json_line = dataset[\"test\"][index]\n",
    "    question = json_line[\"question_stem\"]\n",
    "    choices = json_line[\"choices\"]\n",
    "    choice_texts = choices[\"text\"]\n",
    "    choice_texts = [choice_texts[perm[i]] for i in range(len(choice_texts))]\n",
    "\n",
    "    def change(text, leng=20, is_question=False):\n",
    "        pad_x = [0] * leng\n",
    "        encoded = tokenizer.encode(text)[:-1]\n",
    "        if len(encoded) > leng:\n",
    "            print('too long ', len(encoded), leng, is_question)\n",
    "        pad_x[:len(encoded)] = encoded\n",
    "        return tokenizer.decode(pad_x)\n",
    "\n",
    "    candidates = \" \".join([\n",
    "        change(f'( ) {text}', ANSWER_PAD)\n",
    "        for text, label in zip(choice_texts, choices[\"label\"])\n",
    "    ]).replace(\"\\n\", \" \")\n",
    "    # print(json_line)\n",
    "    answer_key = json_line[\"answerKey\"]\n",
    "    answer_key_idx = ord(answer_key[0]) - ord(\"A\")\n",
    "    # answer_text = choice_texts[answer_key_idx]\n",
    "    # id = \"OBQA_\" + json_line['id']\n",
    "    question_pad = f'{question} \\\\n '\n",
    "    prompt = f\"{change(question_pad,QUESTION_PAD, is_question=True)}{candidates}\"\n",
    "    return prompt, choice_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quit eating lunch with friends    20\n",
      "Name: count, dtype: int64\n",
      "question  0 ['make more phone calls', 'quit eating lunch out', 'buy less with monopoly money', 'have lunch with friends'] 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a deserth    20\n",
      "Name: count, dtype: int64\n",
      "question  1 ['a marsh', 'a tundra', 'the plains', 'a desert'] 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grasslions    20\n",
      "Name: count, dtype: int64\n",
      "question  2 ['lions', 'humans', 'bunnies', 'grass'] 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts may fall apart    20\n",
      "Name: count, dtype: int64\n",
      "question  3 ['roots may be split', 'roots may begin to die', 'parts may break the concrete', 'roots may fall apart'] 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuel conductors    20\n",
      "Name: count, dtype: int64\n",
      "question  4 ['gasoline', 'a power station', 'electrical conductors', 'fuel'] 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# answer=[[]]*question_to_do\n",
    "\n",
    "for i in range(question_to_do):\n",
    "    choice0 = None\n",
    "    answer = []\n",
    "    for k in trange(per_question):\n",
    "        prompt, choice = get_prompt(i, perm_order[k % 24])\n",
    "        if choice0 is None:\n",
    "            choice0 = choice\n",
    "        # print(perm, textwrap.fill(prompt))\n",
    "        answer.append(run_model(prompt)[0])\n",
    "    print('question ', i, choice0, measure_unalike(answer, print_arr=True))\n",
    "\n",
    "# for index,ans in enumerate(answer):\n",
    "\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('wm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69f52fabb15766d39c6bf90ba53c555c905cb082f5a671ecb5c4487727b3f015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
