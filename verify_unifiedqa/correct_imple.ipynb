{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldh0033@auburn.edu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Adafactor\n",
    "from functools import wraps, partial\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "\n",
    "model_name = \"google-t5/t5-large\"\n",
    "# model_name = (\n",
    "#     \"allenai/unifiedqa-v2-t5-large-1363200\"  # you can specify the model size here\n",
    "# )\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "DEVICE = 0\n",
    "model_original = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name, device_map=f\"cuda:{DEVICE}\")  #'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# del model\n",
    "model = model_original  # copy.deepcopy(model_original)\n",
    "\n",
    "\n",
    "# %%\n",
    "def DEFAULT_COMPUTE_BIAS(self, query_length, key_length, device=None):\n",
    "    \"\"\"Compute binned relative position bias\"\"\"\n",
    "    if device is None:\n",
    "        device = self.relative_attention_bias.weight.device\n",
    "    context_position = torch.arange(query_length, dtype=torch.long, device=device)[\n",
    "        :, None\n",
    "    ]\n",
    "    memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n",
    "    relative_position = (\n",
    "        memory_position - context_position\n",
    "    )  # shape (query_length, key_length)\n",
    "    relative_position_bucket = self._relative_position_bucket(\n",
    "        relative_position,  # shape (query_length, key_length)\n",
    "        bidirectional=(not self.is_decoder),\n",
    "        num_buckets=self.relative_attention_num_buckets,\n",
    "        max_distance=self.relative_attention_max_distance,\n",
    "    )\n",
    "    values = self.relative_attention_bias(\n",
    "        relative_position_bucket\n",
    "    )  # shape (query_length, key_length, num_heads)\n",
    "    values = values.permute([2, 0, 1]).unsqueeze(\n",
    "        0\n",
    "    )  # shape (1, num_heads, query_length, key_length)\n",
    "    return values\n",
    "\n",
    "\n",
    "# %%\n",
    "import pickle\n",
    "\n",
    "dataset_test = pickle.load(open(\"test_without_abcd.pkl\", \"rb\"))\n",
    "dataset_train = pickle.load(open(\"train_without_abcd.pkl\", \"rb\"))\n",
    "\n",
    "# %%\n",
    "MODE = \"new\"  #'old'\n",
    "\n",
    "# if hasattr(layer, 'EncDecAttention'):\n",
    "#     layer.EncDecAttention.compute_bias = partial(\n",
    "#         new_compute_bias, layer.EncDecAttention)\n",
    "\n",
    "# %%\n",
    "model.hf_device_map\n",
    "\n",
    "# %%\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def measure_unalike(arr, print_arr=False):\n",
    "    n = len(arr)\n",
    "    arr = pd.Series(arr).value_counts()\n",
    "    if print_arr:\n",
    "        print(arr)\n",
    "    return 1 - ((arr / n) ** 2).sum()\n",
    "\n",
    "\n",
    "question_to_do = 5\n",
    "per_question = 20\n",
    "\n",
    "\n",
    "def run_tokens(tokens):\n",
    "    res = model.generate(tokens, max_new_tokens=MAX_ANSWER_LENGTH)\n",
    "    return tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def run_model(input_string, **generator_args):\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    # print(torch.argwhere(input_ids[0]==2)[0,0]+2)\n",
    "    res = model.generate(\n",
    "        input_ids.to(DEVICE), **generator_args, max_new_tokens=MAX_ANSWER_LENGTH\n",
    "    )\n",
    "    return tokenizer.batch_decode(res, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is responsible for \\n ( ) puppies learning new tricks ( )\n",
      "children growing up and getting old ( ) flowers wilting in a vase ( )\n",
      "plants sprouting, blooming and wilting\n",
      "x:y tensor([ 76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,\n",
      "         90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115]) tensor([116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
      "        144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155])\n",
      "after: a ->b:  tensor([[  0,   1,   2,  ...,  37,  38,  39],\n",
      "        [ -1,   0,   1,  ...,  36,  37,  38],\n",
      "        [ -2,  -1,   0,  ...,  35,  36,  37],\n",
      "        ...,\n",
      "        [-37, -36, -35,  ...,   0,   1,   2],\n",
      "        [-38, -37, -36,  ...,  -1,   0,   1],\n",
      "        [-39, -38, -37,  ...,  -2,  -1,   0]], device='cuda:2') 40\n",
      "after: a ->b:  tensor([[  0,   1,   2,  ...,  37,  38,  39],\n",
      "        [ -1,   0,   1,  ...,  36,  37,  38],\n",
      "        [ -2,  -1,   0,  ...,  35,  36,  37],\n",
      "        ...,\n",
      "        [-37, -36, -35,  ...,   0,   1,   2],\n",
      "        [-38, -37, -36,  ...,  -1,   0,   1],\n",
      "        [-39, -38, -37,  ...,  -2,  -1,   0]], device='cuda:2') 40\n",
      "new  ['sprouting, blooming in and a vaseilting in ( ) for for ( ) for)n n n']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# %%\n",
    "QUESTION_MAX_LENGTH = 76\n",
    "MAX_ANSWER_LENGTH = 40\n",
    "\n",
    "\n",
    "# %%\n",
    "def check(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")[0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    original = input_ids.tolist()\n",
    "    anchor = []\n",
    "    for i in range(len(tokens)):\n",
    "        if (i < len(tokens) - 2 and tokens[i] == \"▁(\" and tokens[i + 1] == \"▁\"\n",
    "                and tokens[i + 2] == \")\") or original[i] == 1:\n",
    "            anchor.append(i)\n",
    "    # 0 1 2 3 4\n",
    "    for x in reversed(range(1, 5)):\n",
    "        if anchor[x] - anchor[x - 1] < MAX_ANSWER_LENGTH:\n",
    "            [\n",
    "                original.insert(anchor[x], 0)\n",
    "                for _ in range(MAX_ANSWER_LENGTH - (anchor[x] - anchor[x - 1]))\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"Wrong size ANSWER: {anchor[x] - anchor[x - 1] }\")\n",
    "            return None\n",
    "    if anchor[0] < QUESTION_MAX_LENGTH:\n",
    "        [\n",
    "            original.insert(anchor[0], 0)\n",
    "            for _ in range(QUESTION_MAX_LENGTH - anchor[0])\n",
    "        ]\n",
    "    else:\n",
    "        print(f\"Wrong size QUESTION: {anchor[0]}\")\n",
    "        return None\n",
    "    return torch.tensor(original).view(1, -1)\n",
    "\n",
    "\n",
    "start_pos = QUESTION_MAX_LENGTH\n",
    "leng = MAX_ANSWER_LENGTH\n",
    "a = [start_pos + leng * 0, start_pos + leng * 1]\n",
    "b = [start_pos + leng * 1, start_pos + leng * 2]\n",
    "c = [start_pos + leng * 2, start_pos + leng * 3]\n",
    "d = [start_pos + leng * 3, start_pos + leng * 4]\n",
    "DEC = {\"01\": 0, \"02\": 1, \"03\": 2, \"12\": 3, \"13\": 4, \"23\": 5}\n",
    "mot = [a, b, c, d]\n",
    "six_mask_turn_off = torch.ones((237, 237, 16))\n",
    "six_mask_turn_on = torch.zeros((6, 237, 237, 16))\n",
    "\n",
    "for i in range(len(mot) - 1):\n",
    "    for j in range(i + 1, len(mot)):\n",
    "        x = mot[i]\n",
    "        y = mot[j]\n",
    "        # print(mask_turn_off_hyper_dimension[x][:, y][:].shape)\n",
    "        # cal index in 6\n",
    "        comb_index = DEC[f\"{i}{j}\"]\n",
    "        # no distance, a very special distance\n",
    "        six_mask_turn_on[comb_index,x[0]:x[1],y[0]:y[1],:] = 1\n",
    "        six_mask_turn_on[comb_index,y[0]:y[1],x[0]:x[1],:] = 1\n",
    "        six_mask_turn_off[x[0]:x[1], y[0]:y[1],:] = 0\n",
    "        six_mask_turn_off[x[0]:x[1],y[0]:y[1],:] = 0\n",
    "\n",
    "\n",
    "# %%\n",
    "def new_compute_bias(self, query_length, key_length, device=None):\n",
    "    \"\"\"Compute binned relative position bias\"\"\"\n",
    "    if device is None:\n",
    "        device = self.relative_attention_bias.weight.device\n",
    "    context_position = torch.arange(query_length,\n",
    "                                    dtype=torch.long,\n",
    "                                    device=device)[:, None]\n",
    "    memory_position = torch.arange(key_length, dtype=torch.long,\n",
    "                                   device=device)[None, :]\n",
    "\n",
    "    relative_position = (memory_position - context_position\n",
    "                         )  # shape (query_length, key_length)\n",
    "    # implementation='simple'\n",
    "    if self.is_decoder:\n",
    "        pass\n",
    "    else:\n",
    "        context_position_new = context_position.clone()\n",
    "        context_position_new[b] = context_position_new[a]\n",
    "        context_position_new[c] = context_position_new[a]\n",
    "        context_position_new[d] = context_position_new[a]\n",
    "        context_position_new[-1] = context_position_new[a[0]] + leng\n",
    "        memory_position_new = context_position_new.clone().view(1, -1)\n",
    "        relative_position = (memory_position_new - context_position_new\n",
    "                             )  # shape (query_length, key_length)\n",
    "        for i in range(len(mot)):\n",
    "            for j in range(len(mot)):\n",
    "                if i!=j:\n",
    "                    x = mot[i]\n",
    "                    y = mot[j]\n",
    "                    print('x:y',x,y)\n",
    "                    print('after: a ->b: ',relative_position[a][:,b], MAX_ANSWER_LENGTH)\n",
    "                    relative_position[a][:,b]=0\n",
    "                    # relative_position[x][:,y]=relative_position[x][:,y]+1#MAX_ANSWER_LENGTH\n",
    "                    print('after: a ->b: ',relative_position[a][:,b], MAX_ANSWER_LENGTH)\n",
    "                    break\n",
    "            break\n",
    "    relative_position_bucket = self._relative_position_bucket(\n",
    "        relative_position,  # shape (query_length, key_length)\n",
    "        bidirectional=(not self.is_decoder),\n",
    "        num_buckets=self.relative_attention_num_buckets,\n",
    "        max_distance=self.relative_attention_max_distance,\n",
    "    )\n",
    "    implementation = \"complicated1\"  # \"change_32\"  # \"complicated\"\n",
    "    if self.is_decoder:\n",
    "        values = self.relative_attention_bias(relative_position_bucket)\n",
    "    else:  # special algo\n",
    "        if implementation == \"complicated\":\n",
    "            values = self.relative_attention_bias(relative_position_bucket)\n",
    "            device_of_values = values.device\n",
    "            # six_extra_embedding_forward = [\n",
    "            #     self.extra_dimension_embedding_forward[i](torch.tensor(\n",
    "            #         [0])).view(1, 1, 16).to(device_of_values) for i in range(6)\n",
    "            # ]\n",
    "            # six_extra_embedding_backward = [\n",
    "            #     self.extra_dimension_embedding_backward[i](torch.tensor(\n",
    "            #         [0])).view(1, 1, 16).to(device_of_values) for i in range(6)\n",
    "            # ]\n",
    "            # print(six_extra_embedding_backward[0].shape)\n",
    "            # const_hyper=torch.tensor([[2]*16]).view(1,1,16).to(device_of_values)\n",
    "            # # print(const_hyper)\n",
    "            # tmp = torch.zeros_like(values).to(device_of_values)\n",
    "            # for i in range(6):\n",
    "            #     tmp += (six_mask_turn_on[i].to(device_of_values) *\n",
    "            #             const_hyper +\n",
    "            #             six_mask_turn_on_backward[i].to(device_of_values) *\n",
    "            #             const_hyper)\n",
    "            # values = values * six_mask_turn_off.to(device_of_values) + tmp\n",
    "        elif implementation == \"change_32\":\n",
    "            for i, x in enumerate(mot):\n",
    "                for j, y in enumerate(mot):\n",
    "                    if i != j:\n",
    "                        relative_position_bucket[x, y] = 31  # furthest\n",
    "            values = self.relative_attention_bias(relative_position_bucket)\n",
    "        else:\n",
    "            values = self.relative_attention_bias(relative_position_bucket)\n",
    "\n",
    "    values = values.permute([2, 0, 1]).unsqueeze(\n",
    "        0)  # shape (1, num_heads, query_length, key_length)\n",
    "    return values\n",
    "\n",
    "\n",
    "extra_dim_learning = []\n",
    "\n",
    "\n",
    "def set_mode(MODE):\n",
    "    for part in [\"encoder\"]:  # , 'decoder']:\n",
    "        for block in getattr(model, part).block:\n",
    "            for layer in block.layer:\n",
    "                # only need to deal in the Encoder level\n",
    "                if (hasattr(layer, \"SelfAttention\")\n",
    "                        and layer.SelfAttention.has_relative_attention_bias):\n",
    "                    itself = layer.SelfAttention\n",
    "                    if MODE == \"new\":\n",
    "                        itself.compute_bias = partial(new_compute_bias,\n",
    "                                                      layer.SelfAttention)\n",
    "                        tmp_extra_dim_learning = [[], []]\n",
    "                        for i in range(2):\n",
    "                            for j in range(6):\n",
    "                                new_emb = Embedding(1, 16)\n",
    "                                new_emb.weight.data.normal_(mean=0.0,\n",
    "                                                            std=1024**-0.5)\n",
    "                                tmp_extra_dim_learning[i].append(new_emb)\n",
    "\n",
    "                        itself.extra_dimension_embedding_forward = nn.ModuleList(\n",
    "                            tmp_extra_dim_learning[0])\n",
    "                        itself.extra_dimension_embedding_backward = nn.ModuleList(\n",
    "                            tmp_extra_dim_learning[1])\n",
    "                    else:\n",
    "                        itself.compute_bias = partial(DEFAULT_COMPUTE_BIAS,\n",
    "                                                      layer.SelfAttention)\n",
    "\n",
    "\n",
    "print(textwrap.fill(dataset_train[0][0]))\n",
    "# set_mode(\"old\")\n",
    "# print(\"old \", run_tokens(check(dataset_train[0][0]).to(DEVICE)))\n",
    "set_mode(\"new\")\n",
    "print(\"new \", run_tokens(check(dataset_train[0][0]).to(DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = [(index, x, y) for index, (x, y) in enumerate(model.named_parameters())\n",
    "      if y.requires_grad == True]\n",
    "[(index, x) for index, x, y in kk if \"decoder\" in x]\n",
    "len(kk)\n",
    "all_position_weight=[y for index, x, y in kk if ('extra_dimension_embedding' in x) or (('encoder' in x) and ('relative_attention_bias' in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shared.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.0.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.1.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.2.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.3.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.4.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_forward.5.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.0.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.1.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.2.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.3.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.4.weight',\n",
       " 'encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding_backward.5.weight',\n",
       " 'encoder.block.0.layer.0.layer_norm.weight',\n",
       " 'encoder.block.0.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.0.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.0.layer.1.layer_norm.weight',\n",
       " 'encoder.block.1.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.1.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.1.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.1.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.1.layer.0.layer_norm.weight',\n",
       " 'encoder.block.1.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.1.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.1.layer.1.layer_norm.weight',\n",
       " 'encoder.block.2.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.2.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.2.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.2.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.2.layer.0.layer_norm.weight',\n",
       " 'encoder.block.2.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.2.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.2.layer.1.layer_norm.weight',\n",
       " 'encoder.block.3.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.3.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.3.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.3.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.3.layer.0.layer_norm.weight',\n",
       " 'encoder.block.3.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.3.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.3.layer.1.layer_norm.weight',\n",
       " 'encoder.block.4.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.4.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.4.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.4.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.4.layer.0.layer_norm.weight',\n",
       " 'encoder.block.4.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.4.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.4.layer.1.layer_norm.weight',\n",
       " 'encoder.block.5.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.5.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.5.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.5.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.5.layer.0.layer_norm.weight',\n",
       " 'encoder.block.5.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.5.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.5.layer.1.layer_norm.weight',\n",
       " 'encoder.block.6.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.6.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.6.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.6.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.6.layer.0.layer_norm.weight',\n",
       " 'encoder.block.6.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.6.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.6.layer.1.layer_norm.weight',\n",
       " 'encoder.block.7.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.7.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.7.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.7.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.7.layer.0.layer_norm.weight',\n",
       " 'encoder.block.7.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.7.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.7.layer.1.layer_norm.weight',\n",
       " 'encoder.block.8.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.8.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.8.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.8.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.8.layer.0.layer_norm.weight',\n",
       " 'encoder.block.8.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.8.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.8.layer.1.layer_norm.weight',\n",
       " 'encoder.block.9.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.9.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.9.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.9.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.9.layer.0.layer_norm.weight',\n",
       " 'encoder.block.9.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.9.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.9.layer.1.layer_norm.weight',\n",
       " 'encoder.block.10.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.10.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.10.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.10.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.10.layer.0.layer_norm.weight',\n",
       " 'encoder.block.10.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.10.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.10.layer.1.layer_norm.weight',\n",
       " 'encoder.block.11.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.11.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.11.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.11.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.11.layer.0.layer_norm.weight',\n",
       " 'encoder.block.11.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.11.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.11.layer.1.layer_norm.weight',\n",
       " 'encoder.block.12.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.12.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.12.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.12.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.12.layer.0.layer_norm.weight',\n",
       " 'encoder.block.12.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.12.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.12.layer.1.layer_norm.weight',\n",
       " 'encoder.block.13.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.13.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.13.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.13.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.13.layer.0.layer_norm.weight',\n",
       " 'encoder.block.13.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.13.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.13.layer.1.layer_norm.weight',\n",
       " 'encoder.block.14.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.14.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.14.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.14.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.14.layer.0.layer_norm.weight',\n",
       " 'encoder.block.14.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.14.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.14.layer.1.layer_norm.weight',\n",
       " 'encoder.block.15.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.15.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.15.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.15.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.15.layer.0.layer_norm.weight',\n",
       " 'encoder.block.15.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.15.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.15.layer.1.layer_norm.weight',\n",
       " 'encoder.block.16.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.16.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.16.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.16.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.16.layer.0.layer_norm.weight',\n",
       " 'encoder.block.16.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.16.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.16.layer.1.layer_norm.weight',\n",
       " 'encoder.block.17.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.17.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.17.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.17.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.17.layer.0.layer_norm.weight',\n",
       " 'encoder.block.17.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.17.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.17.layer.1.layer_norm.weight',\n",
       " 'encoder.block.18.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.18.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.18.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.18.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.18.layer.0.layer_norm.weight',\n",
       " 'encoder.block.18.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.18.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.18.layer.1.layer_norm.weight',\n",
       " 'encoder.block.19.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.19.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.19.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.19.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.19.layer.0.layer_norm.weight',\n",
       " 'encoder.block.19.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.19.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.19.layer.1.layer_norm.weight',\n",
       " 'encoder.block.20.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.20.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.20.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.20.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.20.layer.0.layer_norm.weight',\n",
       " 'encoder.block.20.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.20.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.20.layer.1.layer_norm.weight',\n",
       " 'encoder.block.21.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.21.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.21.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.21.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.21.layer.0.layer_norm.weight',\n",
       " 'encoder.block.21.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.21.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.21.layer.1.layer_norm.weight',\n",
       " 'encoder.block.22.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.22.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.22.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.22.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.22.layer.0.layer_norm.weight',\n",
       " 'encoder.block.22.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.22.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.22.layer.1.layer_norm.weight',\n",
       " 'encoder.block.23.layer.0.SelfAttention.q.weight',\n",
       " 'encoder.block.23.layer.0.SelfAttention.k.weight',\n",
       " 'encoder.block.23.layer.0.SelfAttention.v.weight',\n",
       " 'encoder.block.23.layer.0.SelfAttention.o.weight',\n",
       " 'encoder.block.23.layer.0.layer_norm.weight',\n",
       " 'encoder.block.23.layer.1.DenseReluDense.wi.weight',\n",
       " 'encoder.block.23.layer.1.DenseReluDense.wo.weight',\n",
       " 'encoder.block.23.layer.1.layer_norm.weight',\n",
       " 'encoder.final_layer_norm.weight',\n",
       " 'decoder.block.0.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.0.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.0.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.0.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
       " 'decoder.block.0.layer.0.layer_norm.weight',\n",
       " 'decoder.block.0.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.0.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.0.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.0.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.0.layer.1.layer_norm.weight',\n",
       " 'decoder.block.0.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.0.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.0.layer.2.layer_norm.weight',\n",
       " 'decoder.block.1.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.1.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.1.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.1.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.1.layer.0.layer_norm.weight',\n",
       " 'decoder.block.1.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.1.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.1.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.1.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.1.layer.1.layer_norm.weight',\n",
       " 'decoder.block.1.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.1.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.1.layer.2.layer_norm.weight',\n",
       " 'decoder.block.2.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.2.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.2.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.2.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.2.layer.0.layer_norm.weight',\n",
       " 'decoder.block.2.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.2.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.2.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.2.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.2.layer.1.layer_norm.weight',\n",
       " 'decoder.block.2.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.2.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.2.layer.2.layer_norm.weight',\n",
       " 'decoder.block.3.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.3.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.3.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.3.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.3.layer.0.layer_norm.weight',\n",
       " 'decoder.block.3.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.3.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.3.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.3.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.3.layer.1.layer_norm.weight',\n",
       " 'decoder.block.3.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.3.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.3.layer.2.layer_norm.weight',\n",
       " 'decoder.block.4.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.4.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.4.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.4.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.4.layer.0.layer_norm.weight',\n",
       " 'decoder.block.4.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.4.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.4.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.4.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.4.layer.1.layer_norm.weight',\n",
       " 'decoder.block.4.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.4.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.4.layer.2.layer_norm.weight',\n",
       " 'decoder.block.5.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.5.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.5.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.5.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.5.layer.0.layer_norm.weight',\n",
       " 'decoder.block.5.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.5.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.5.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.5.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.5.layer.1.layer_norm.weight',\n",
       " 'decoder.block.5.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.5.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.5.layer.2.layer_norm.weight',\n",
       " 'decoder.block.6.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.6.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.6.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.6.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.6.layer.0.layer_norm.weight',\n",
       " 'decoder.block.6.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.6.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.6.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.6.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.6.layer.1.layer_norm.weight',\n",
       " 'decoder.block.6.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.6.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.6.layer.2.layer_norm.weight',\n",
       " 'decoder.block.7.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.7.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.7.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.7.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.7.layer.0.layer_norm.weight',\n",
       " 'decoder.block.7.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.7.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.7.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.7.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.7.layer.1.layer_norm.weight',\n",
       " 'decoder.block.7.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.7.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.7.layer.2.layer_norm.weight',\n",
       " 'decoder.block.8.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.8.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.8.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.8.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.8.layer.0.layer_norm.weight',\n",
       " 'decoder.block.8.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.8.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.8.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.8.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.8.layer.1.layer_norm.weight',\n",
       " 'decoder.block.8.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.8.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.8.layer.2.layer_norm.weight',\n",
       " 'decoder.block.9.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.9.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.9.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.9.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.9.layer.0.layer_norm.weight',\n",
       " 'decoder.block.9.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.9.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.9.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.9.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.9.layer.1.layer_norm.weight',\n",
       " 'decoder.block.9.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.9.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.9.layer.2.layer_norm.weight',\n",
       " 'decoder.block.10.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.10.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.10.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.10.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.10.layer.0.layer_norm.weight',\n",
       " 'decoder.block.10.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.10.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.10.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.10.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.10.layer.1.layer_norm.weight',\n",
       " 'decoder.block.10.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.10.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.10.layer.2.layer_norm.weight',\n",
       " 'decoder.block.11.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.11.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.11.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.11.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.11.layer.0.layer_norm.weight',\n",
       " 'decoder.block.11.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.11.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.11.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.11.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.11.layer.1.layer_norm.weight',\n",
       " 'decoder.block.11.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.11.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.11.layer.2.layer_norm.weight',\n",
       " 'decoder.block.12.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.12.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.12.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.12.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.12.layer.0.layer_norm.weight',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.12.layer.1.layer_norm.weight',\n",
       " 'decoder.block.12.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.12.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.12.layer.2.layer_norm.weight',\n",
       " 'decoder.block.13.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.13.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.13.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.13.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.13.layer.0.layer_norm.weight',\n",
       " 'decoder.block.13.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.13.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.13.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.13.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.13.layer.1.layer_norm.weight',\n",
       " 'decoder.block.13.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.13.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.13.layer.2.layer_norm.weight',\n",
       " 'decoder.block.14.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.14.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.14.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.14.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.14.layer.0.layer_norm.weight',\n",
       " 'decoder.block.14.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.14.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.14.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.14.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.14.layer.1.layer_norm.weight',\n",
       " 'decoder.block.14.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.14.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.14.layer.2.layer_norm.weight',\n",
       " 'decoder.block.15.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.15.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.15.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.15.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.15.layer.0.layer_norm.weight',\n",
       " 'decoder.block.15.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.15.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.15.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.15.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.15.layer.1.layer_norm.weight',\n",
       " 'decoder.block.15.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.15.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.15.layer.2.layer_norm.weight',\n",
       " 'decoder.block.16.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.16.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.16.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.16.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.16.layer.0.layer_norm.weight',\n",
       " 'decoder.block.16.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.16.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.16.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.16.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.16.layer.1.layer_norm.weight',\n",
       " 'decoder.block.16.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.16.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.16.layer.2.layer_norm.weight',\n",
       " 'decoder.block.17.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.17.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.17.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.17.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.17.layer.0.layer_norm.weight',\n",
       " 'decoder.block.17.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.17.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.17.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.17.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.17.layer.1.layer_norm.weight',\n",
       " 'decoder.block.17.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.17.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.17.layer.2.layer_norm.weight',\n",
       " 'decoder.block.18.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.18.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.18.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.18.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.18.layer.0.layer_norm.weight',\n",
       " 'decoder.block.18.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.18.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.18.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.18.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.18.layer.1.layer_norm.weight',\n",
       " 'decoder.block.18.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.18.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.18.layer.2.layer_norm.weight',\n",
       " 'decoder.block.19.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.19.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.19.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.19.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.19.layer.0.layer_norm.weight',\n",
       " 'decoder.block.19.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.19.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.19.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.19.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.19.layer.1.layer_norm.weight',\n",
       " 'decoder.block.19.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.19.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.19.layer.2.layer_norm.weight',\n",
       " 'decoder.block.20.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.20.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.20.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.20.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.20.layer.0.layer_norm.weight',\n",
       " 'decoder.block.20.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.20.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.20.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.20.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.20.layer.1.layer_norm.weight',\n",
       " 'decoder.block.20.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.20.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.20.layer.2.layer_norm.weight',\n",
       " 'decoder.block.21.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.21.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.21.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.21.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.21.layer.0.layer_norm.weight',\n",
       " 'decoder.block.21.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.21.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.21.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.21.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.21.layer.1.layer_norm.weight',\n",
       " 'decoder.block.21.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.21.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.21.layer.2.layer_norm.weight',\n",
       " 'decoder.block.22.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.22.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.22.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.22.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.22.layer.0.layer_norm.weight',\n",
       " 'decoder.block.22.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.22.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.22.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.22.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.22.layer.1.layer_norm.weight',\n",
       " 'decoder.block.22.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.22.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.22.layer.2.layer_norm.weight',\n",
       " 'decoder.block.23.layer.0.SelfAttention.q.weight',\n",
       " 'decoder.block.23.layer.0.SelfAttention.k.weight',\n",
       " 'decoder.block.23.layer.0.SelfAttention.v.weight',\n",
       " 'decoder.block.23.layer.0.SelfAttention.o.weight',\n",
       " 'decoder.block.23.layer.0.layer_norm.weight',\n",
       " 'decoder.block.23.layer.1.EncDecAttention.q.weight',\n",
       " 'decoder.block.23.layer.1.EncDecAttention.k.weight',\n",
       " 'decoder.block.23.layer.1.EncDecAttention.v.weight',\n",
       " 'decoder.block.23.layer.1.EncDecAttention.o.weight',\n",
       " 'decoder.block.23.layer.1.layer_norm.weight',\n",
       " 'decoder.block.23.layer.2.DenseReluDense.wi.weight',\n",
       " 'decoder.block.23.layer.2.DenseReluDense.wo.weight',\n",
       " 'decoder.block.23.layer.2.layer_norm.weight',\n",
       " 'decoder.final_layer_norm.weight']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_train = [\n",
    "    \"shared.weight\",\n",
    "    \"encoder.block.0.layer.0.SelfAttention.q.weight\",\n",
    "    \"encoder.block.0.layer.0.SelfAttention.k.weight\",\n",
    "    \"encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\",\n",
    "]\n",
    "to_train_model = [(x, y) for index, x, y in kk]  # [:196]]\n",
    "# to_train_model=to_train_model+\n",
    "# to_train_model=[(x, y) for x, y in model.named_parameters() if x==\"encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding.weight\"]\n",
    "# to_train=[]\n",
    "to_train = [\n",
    "    \"encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\",\n",
    "    \"encoder.block.0.layer.0.layer_norm.weight\",\n",
    "    \"encoder.block.0.layer.1.DenseReluDense.wi.weight\",\n",
    "    \"encoder.block.0.layer.1.DenseReluDense.wo.weight\",\n",
    "    \"encoder.block.0.layer.1.layer_norm.weight\",\n",
    "    \"encoder.block.0.layer.0.SelfAttention.extra_dimension_embedding.weight\",\n",
    "]\n",
    "# to_train_model=[(x,y) for x, y in model.named_parameters() if x in to_train]\n",
    "# to_train_model = [(x, y) for x, y in model.named_parameters()\n",
    "#                   if not x in not_train]\n",
    "\n",
    "for y in model.parameters():\n",
    "    y.requires_grad = False\n",
    "for x, y in to_train_model:\n",
    "    y.requires_grad = True\n",
    "[x for x, y in to_train_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# to_train_model=[y for x, y in model.named_parameters() if x in train_name_list ]\n",
    "# for y in to_train_model:\n",
    "#     y.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def shape(input):\n",
    "    return input.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# no_decay = ['layer_norm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in to_train_model], 'weight_decay': 0.0},\n",
    "#     ]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, eps=1e-8)\n",
    "# scheduler =  get_linear_schedule_with_warmup(optimizer,\n",
    "#                                 num_warmup_steps=0,\n",
    "#                                 num_training_steps=100000)\n",
    "to_train_model = [y for x, y in to_train_model]\n",
    "# optimizer = SGD(to_train_model, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_forward(input_tokens):\n",
    "    encoder_attentions = None\n",
    "    last_hidden = None\n",
    "    with torch.no_grad():\n",
    "        start = [0]\n",
    "        for k in range(MAX_ANSWER_LENGTH):\n",
    "            result = model(\n",
    "                input_ids=input_tokens.to(DEVICE),\n",
    "                decoder_input_ids=torch.tensor([start]).to(DEVICE),\n",
    "                output_attentions=True,\n",
    "            )\n",
    "            encoder_attentions = result.encoder_attentions\n",
    "            last_hidden = result.encoder_last_hidden_state\n",
    "            item = result.logits.argmax(dim=2)[0][-1].item()\n",
    "            start.append(item)\n",
    "            if item == 1:\n",
    "                break\n",
    "            # print(start)\n",
    "    return (\n",
    "        tokenizer.decode(start, skip_special_tokens=True),\n",
    "        tokenizer.convert_ids_to_tokens(start),\n",
    "        last_hidden,\n",
    "        encoder_attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "\n",
    "def get_loss(logits, labels):\n",
    "    loss = torch.tensor(0)\n",
    "    found = False\n",
    "    for i in range(len(labels[0])):\n",
    "        current_loss = loss_fn(logits[0][i], labels[0][i].to(DEVICE))\n",
    "\n",
    "        current_certainly = torch.exp(-current_loss)\n",
    "        if current_certainly < 0.9:\n",
    "            loss = current_loss\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        loss = loss_fn(logits[0], labels[0].to(DEVICE))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # pbar = trange(0, len(dataset_train), 24)\n",
    "    # loss_score = 0\n",
    "    # count = 0\n",
    "    # extra_info = \"\"\n",
    "    # step=0\n",
    "    # # if count>20:\n",
    "    # #     break\n",
    "    # # print(textwrap.fill(dataset_train[0][0]))\n",
    "    step = 0\n",
    "    pbar = trange(200)\n",
    "    for re in pbar:\n",
    "        input_tokens = check(dataset_train[step][0])\n",
    "        labels = tokenizer.encode(dataset_train[step][1], return_tensors=\"pt\")\n",
    "        result = model(input_ids=input_tokens.to(DEVICE),\n",
    "                       labels=shape(labels).to(DEVICE))\n",
    "        loss = get_loss(result.logits, labels)\n",
    "        # print(result.logits.argmax(dim=2), labels)\n",
    "        optimizer.zero_grad()\n",
    "        # loss = result.loss\n",
    "        # print(result.logits, labels, loss)\n",
    "        if loss.item() != 0:\n",
    "            loss_score = loss.item()  # loss_score * 0.9 + loss.item() * 0.1\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # with torch.no_grad():\n",
    "        #     mong= model(input_ids=check(dataset_train[0][0]).to(DEVICE), decoder_input_ids=torch.tensor([[0]]).to(DEVICE))\n",
    "        #     print(mong.logits.argmax(dim=2).shape)\n",
    "        # print(tokenizer.decode())\n",
    "\n",
    "        extra_info = get_model_forward(\n",
    "            check(dataset_train[step][0]).to(DEVICE))\n",
    "        pbar.set_postfix_str(f\"Loss: {loss_score:.10f}:{extra_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong size QUESTION: 88\n"
     ]
    }
   ],
   "source": [
    "data_array = [(k, v, l.split(\" ( ) \")[1:])\n",
    "              for l, k, v in [(dataset_train[x][0], check(dataset_train[x][0]),\n",
    "                               dataset_train[x][1])\n",
    "                              for x in range(0, len(dataset_train), 24)]\n",
    "              if k is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   37,  1997,    19,  1966,    21,     3,     2,    29,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,    41,     3,    61, 26675,\n",
       "           1036,   126, 13258,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,    41,     3,    61,   502,\n",
       "           1710,    95,    11,   652,   625,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,    41,     3,    61,  3652,\n",
       "              3,   210,   173,  1222,    16,     3,     9, 16997,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,    41,     3,    61,  2677,\n",
       "          23032,    53,     6, 13081,    53,    11,     3,   210,   173,  1222,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     1]]),\n",
       " 'plants sprouting, blooming and wilting',\n",
       " ['puppies learning new tricks',\n",
       "  'children growing up and getting old',\n",
       "  'flowers wilting in a vase',\n",
       "  'plants sprouting, blooming and wilting'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  394,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2714,    49,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [10540,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [20394,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   71, 16092,   358,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  389,   625,     3,     7, 20895,  8735,    49,     1,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  389,  1906, 16867,  1016,     1,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   71,     3,    75,  3623,    31,     7,  9190,     1,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [    8, 22583,    44,     8,   420,    13,     8,  9956,    65,  2681,\n",
      "             3,     9, 10783,    30,     8,  1511,     1],\n",
      "        [    3,     9,  1978,   851,    65,  2301, 15998,     1,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [    3,     9, 12195,    19,    81,    12,   456,    44,     8,  2078,\n",
      "           640,     8,  2815,     1,  -100,  -100,  -100],\n",
      "        [    8,  3596,    19,  1735,   190,     3,     9,  2164,   616,    13,\n",
      "           628,     1,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  234,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 1021,   697,     7,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 3127,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 8636,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 7479,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 1042,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 5958,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 6269,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [26806,     7,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 7944,     7,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 7065,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2328,  7677,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [19791,    23,    26,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2515,  3481,    63,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [19275,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [16544,    63,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [11667,   625,  1905,    12,   143,     3,     9,   629,     1,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [13049,  2343,    16,     8,  5431,     1,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [    3, 28004,  4954,    12,     3,     9, 30542,  1814,     1,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [    3,  3626,   625,     3,  9955,    12,   143,     3,     9, 10934,\n",
      "             1,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 4335,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 3050,    15,    26,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 4816,  4632,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [21815,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2158, 24180,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2709, 11971,   588,    51,     7,     1,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [30946,     7,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 1546,    51,    32, 16333,  8065,     1,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CheckTransform(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # print(f\"'{sample[1]}'\")\n",
    "        return {\n",
    "            \"input_ids\": sample[0][0],\n",
    "            \"label_index\": sample[2].index(sample[1]),\n",
    "            \"all_labels\": sample[2],\n",
    "        }\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_array, transform=None):\n",
    "        self.dataset = dataset_array\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.dataset[idx])\n",
    "\n",
    "\n",
    "def collate(datas):\n",
    "    label_ids = tokenizer(sum([x[\"all_labels\"] for x in datas], []),\n",
    "                          padding=True)\n",
    "    wrapper = label_ids\n",
    "    wrapper[\"all_label_ids\"] = torch.tensor(wrapper.pop(\"input_ids\"))\n",
    "    # wrapper[\"label_index\"] = torch.tensor([x[\"label_index\"] for x in datas])\n",
    "    for k in wrapper[\"all_label_ids\"]:\n",
    "        k[k == tokenizer.pad_token_id] = -100\n",
    "    wrapper[\"all_decoder_attention_masks\"] = torch.tensor(\n",
    "        wrapper.pop(\"attention_mask\"))\n",
    "    wrapper[\"input_ids\"] = torch.stack([x[\"input_ids\"] for x in datas])\n",
    "    wrapper[\"label_index\"] = torch.tensor([x[\"label_index\"] for x in datas])\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "loi_dataloader = DataLoader(\n",
    "    CustomDataset(\n",
    "        data_array,\n",
    "        CheckTransform(),\n",
    "    ),\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "for k in loi_dataloader:\n",
    "    print(k[\"all_label_ids\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 898704\n",
    "# hidden state 242688\n",
    "# classification_layer = nn.Linear(242688, 4).to(DEVICE)\n",
    "optimizer = Adafactor(\n",
    "    to_train_model,  # + [x for x in classification_layer.parameters()],\n",
    "    relative_step=True,\n",
    "    warmup_init=True,\n",
    "    lr=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.557: 100%|██████████| 496/496 [08:31<00:00,  1.03s/it, quit eating lunch out]                                                                                            \n",
      "Loss: 0.399: 100%|██████████| 496/496 [08:28<00:00,  1.02s/it, have lunch with friends]       \n",
      "Loss: 0.180: 100%|██████████| 496/496 [08:39<00:00,  1.05s/it, buy less lunch]                      \n",
      "Loss: 0.125:   1%|          | 5/496 [00:05<09:37,  1.18s/it, buy less lunch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4156096/1228910850.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                             )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    738\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"RMS\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_data_fp32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                 \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                 \u001b[0mbeta2t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"decay_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36m_get_lr\u001b[0;34m(param_group, param_state)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mparam_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_parameter\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             \u001b[0mparam_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"RMS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparam_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrel_step_sz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def turn_position_learning(on):\n",
    "    for x in all_position_weight:\n",
    "        x.requires_grad=on\n",
    "from tqdm import tqdm\n",
    "from random_utils import set_seed\n",
    "\n",
    "loss_running_score = 0\n",
    "correct_running_score = 0\n",
    "conform_running_score = 0\n",
    "count = 0\n",
    "extra_info = \"\"\n",
    "res_tokens = []\n",
    "accumulate = 10\n",
    "optimizer.zero_grad()\n",
    "set_seed(42)\n",
    "turn_position=False\n",
    "turn_position_learning(False)\n",
    "for learn_pos in range(10):\n",
    "    pbar = tqdm(loi_dataloader)\n",
    "    for wrapper in pbar:\n",
    "        count += 1\n",
    "        # if count%20==0:\n",
    "        #     turn_position=not turn_position\n",
    "        #     turn_position_learning(turn_position)\n",
    "        # if count>20:\n",
    "        #     break\n",
    "        # print(textwrap.fill(dataset_train[0][0]))\n",
    "        only_correct_label_ids = torch.stack(\n",
    "            [\n",
    "                wrapper[\"all_label_ids\"][batch_index * 4 + x]\n",
    "                for batch_index, x in enumerate(wrapper[\"label_index\"])\n",
    "            ]\n",
    "        )\n",
    "        only_correct_decoder_attention_mask = torch.stack(\n",
    "            [\n",
    "                wrapper[\"all_decoder_attention_masks\"][batch_index * 4 + x]\n",
    "                for batch_index, x in enumerate(wrapper[\"label_index\"])\n",
    "            ]\n",
    "        )\n",
    "        result = model(\n",
    "            input_ids=wrapper[\"input_ids\"].to(DEVICE),\n",
    "            labels=only_correct_label_ids.to(DEVICE),\n",
    "            decoder_attention_mask=only_correct_decoder_attention_mask.to(\n",
    "                DEVICE\n",
    "            ),  # output_attentions=True\n",
    "        )\n",
    "        # conform_loss = 0\n",
    "        # for batch in range(wrapper[\"input_ids\"].shape[0]):\n",
    "        #     selected_answer = result.logits[batch].argmax(dim=1)\n",
    "        #     found = False\n",
    "        #     conform_losses = [0, 0, 0, 0]\n",
    "        #     for each_answer in range(4):\n",
    "        #         tui_batch = wrapper[\"all_label_ids\"][batch * 4 + each_answer]\n",
    "        #         conform_losses[each_answer] += loss_fn(\n",
    "        #                     result.logits[batch], tui_batch.to(DEVICE)\n",
    "        #                 )\n",
    "        #         # for m in range(len(tui_batch)):\n",
    "        #         #     if selected_answer[m] != tui_batch[m] and tui_batch[m] != -100:\n",
    "        #         #         conform_losses[each_answer] += loss_fn(\n",
    "        #         #             result.logits[batch][m], tui_batch[m].to(DEVICE)\n",
    "        #         #         )\n",
    "        #         # conform_min_index = torch.argmin(conform_losses)\n",
    "        #         # print(conform_min_index)\n",
    "        #     conform_loss += min(conform_losses)  # conform_losses[conform_min_index]\n",
    "        # conform_loss = conform_loss / wrapper[\"input_ids\"].shape[0]\n",
    "        # kk1=result.encoder_attentions\n",
    "        # break\n",
    "        # final_logits = classification_layer(\n",
    "        #     torch.flatten(result.encoder_last_hidden_state, start_dim=1)\n",
    "        # )\n",
    "        # loss = loss_fn(final_logits, wrapper[\"label_index\"].to(DEVICE))\n",
    "        loss = result.loss\n",
    "        loss_running_score = loss_running_score * 0.9 + loss.item() * 0.1\n",
    "        if loss != 0:\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # scheduler.step()\n",
    "        with torch.no_grad():\n",
    "            if count % 10 == 0:\n",
    "                extra_info, res_tokens, _, _ = get_model_forward(\n",
    "                    check(dataset_test[0][0]).to(DEVICE)\n",
    "                )\n",
    "                # final_logits = classification_layer(torch.flatten(hidden, start_dim=1))\n",
    "                # extra_info = str(final_logits.argmax())\n",
    "            pbar.set_description_str(\n",
    "                f\"Loss: {loss_running_score:.3f}\"\n",
    "            )\n",
    "            pbar.set_postfix_str(extra_info)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_to_find=len(wrapper[\"label_ids\"][0])-1\n",
    "#         for batch in range(wrapper[\"input_ids\"].shape[0]):\n",
    "#             keys= result.logits[batch].argmax(dim=1)\n",
    "#             found=False\n",
    "#             tui_batch=wrapper[\"label_ids\"][batch]\n",
    "#             for m in range(len(tui_batch)):\n",
    "#                 if keys[m]!=tui_batch[m]:\n",
    "#                     if len_to_find>m:\n",
    "#                         len_to_find=m\n",
    "#                     break\n",
    "#         for batch in range(wrapper[\"input_ids\"].shape[0]):\n",
    "#             loss+=loss_fn(result.logits[batch][:len_to_find+1],tui_batch[:len_to_find+1].to(DEVICE))\n",
    "#         loss=loss/wrapper[\"input_ids\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:56<00:00,  4.30it/s, 152, 302, 287, 312]\n"
     ]
    }
   ],
   "source": [
    "data = dataset_test\n",
    "count = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "count10 = 0\n",
    "pbar1 = trange(500)\n",
    "for ques in pbar1:\n",
    "    question = data[24 * ques][0]\n",
    "    key = data[24 * ques][1]\n",
    "    answer = get_model_forward(check(data[24 * ques][0]).to(DEVICE))[0]\n",
    "    if key == answer:\n",
    "        count += 1\n",
    "    if key[0] == answer[0]:\n",
    "        count1 += 1\n",
    "    if key[:2] == answer[:2]:\n",
    "        count2 += 1\n",
    "    if answer in question:\n",
    "        count10 += 1\n",
    "    pbar1.set_postfix_str(f\"{count}, {count1}, {count2}, {count10}\")\n",
    "    # else:\n",
    "    #     pass\n",
    "    # print(ques,':****',textwrap.fill(question))\n",
    "    # print('Answer key',':****',key)\n",
    "    # print('Answer ',answer)\n",
    "\n",
    "# print(\"Count \", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbar = trange(0, len(dataset_train), 24)\n",
    "# loss_score = 0\n",
    "# count = 0\n",
    "# extra_info = \"\"\n",
    "# set_seed(42)\n",
    "# res_tokens=[]\n",
    "# for learn_pos in range(10):\n",
    "#     for step in pbar:\n",
    "#         count += 1\n",
    "#         # if count>20:\n",
    "#         #     break\n",
    "#         # print(textwrap.fill(dataset_train[0][0]))\n",
    "#         input_tokens = check(dataset_train[step][0])\n",
    "#         if input_tokens is None:\n",
    "#             continue\n",
    "#         labels = tokenizer.encode(dataset_train[step][1], return_tensors=\"pt\")\n",
    "#         result = model(input_ids=input_tokens.to(DEVICE), labels=shape(labels).to(DEVICE))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss =loss_fn(result.logits[0][learn_pos],labels[0][learn_pos].to(DEVICE))\n",
    "#         loss_score = loss_score * 0.9 + loss.item() * 0.1\n",
    "#         if loss.item()!=0:\n",
    "#             loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # scheduler.step()\n",
    "#         with torch.no_grad():\n",
    "#             if count % 10 == 0:\n",
    "#                 extra_info, res_tokens = get_model_forward(check(dataset_test[0][0]).to(DEVICE))\n",
    "#             pbar.set_description_str(f\"Loss: {loss_score:.2f}\")\n",
    "#             pbar.set_postfix_str(res_tokens[:learn_pos+2])\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCUMUTATE gradient\n",
    "# total_params = to_train_model\n",
    "# optimizer = SGD(total_params, lr=1e-2)\n",
    "# pbar = trange(0, len(dataset_train), 24)\n",
    "# loss_score = 0\n",
    "# for step in pbar:\n",
    "#     # count+=1\n",
    "#     # if count>20:\n",
    "#     #     break\n",
    "#     # print(textwrap.fill(dataset_train[0][0]))\n",
    "#     input_tokens = check(dataset_train[step][0])\n",
    "#     if input_tokens is None:\n",
    "#         continue\n",
    "#     labels = tokenizer.encode(dataset_train[step][1], return_tensors=\"pt\")\n",
    "#     result = model(input_ids=input_tokens.to(DEVICE), labels=shape(labels).to(DEVICE))\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = result.loss\n",
    "#     loss_score = loss_score * 0.9 + loss.item() * 0.1\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     with torch.no_grad():\n",
    "#         pbar.set_postfix_str(\n",
    "#             f\"Loss: {loss_score:.2f}:'{run_tokens(check(dataset_test[0][0]).to(DEVICE))}'\"\n",
    "#         )\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldh0033@auburn.edu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
